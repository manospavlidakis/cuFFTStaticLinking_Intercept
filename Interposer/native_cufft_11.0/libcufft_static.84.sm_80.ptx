







.version 7.0
.target sm_80
.address_size 64


.global .align 4 .u32 _ZZN75_INTERNAL_53_half_32bit_vector_c2r_RT_SM70_plus_compute_80_cpp1_ii_72ac17fe18cooperative_groups4__v17details17_binary_partitionINS1_15coalesced_groupEEES4_RKT_bE8fullMask = -1;
.extern .shared .align 4 .b8 smem_full[];

.weak .entry _Z14vector_fft_c2rILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<8>;
.reg .b16 %rs<10>;
.reg .f32 %f<2>;
.reg .b32 %r<124>;
.reg .f64 %fd<2>;
.reg .b64 %rd<17>;


ld.param.u64 %rd8, [_Z14vector_fft_c2rILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_c2rILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r55, %r2, 5;
mov.u32 %r3, %tid.y;
add.s32 %r56, %r55, %r3;
shl.b32 %r4, %r56, 1;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB0_6;

shl.b32 %r5, %r3, 1;
mov.u32 %r57, 1;
shl.b32 %r58, %r2, 6;
add.s32 %r59, %r5, %r58;
shl.b32 %r60, %r59, 1;
add.s32 %r61, %r60, 2;
mov.u32 %r6, %nctaid.x;
add.s32 %r62, %r6, -1;
setp.lt.u32	%p2, %r2, %r62;
mov.u32 %r63, %tid.x;
add.s32 %r64, %r63, %r60;
sub.s32 %r65, %r57, %r63;
add.s32 %r66, %r65, %r60;
cvta.to.global.u64 %rd9, %rd7;
mul.wide.u32 %rd10, %r64, 4;
add.s64 %rd1, %rd9, %rd10;
mul.wide.u32 %rd11, %r66, 4;
add.s64 %rd2, %rd9, %rd11;
add.s32 %r67, %r63, %r61;
mul.wide.u32 %rd12, %r67, 4;
add.s64 %rd3, %rd9, %rd12;
add.s32 %r68, %r65, %r61;
mul.wide.u32 %rd13, %r68, 4;
add.s64 %rd4, %rd9, %rd13;
@%p2 bra BB0_4;
bra.uni BB0_2;

BB0_4:
ld.global.u32 %r121, [%rd1];
ld.global.u32 %r75, [%rd2];
xor.b32 %r119, %r75, -2147483648;
ld.global.u32 %r120, [%rd3];
ld.global.u32 %r76, [%rd4];
xor.b32 %r118, %r76, -2147483648;
bra.uni BB0_5;

BB0_2:
shl.b32 %r70, %r6, 6;
add.s32 %r71, %r5, %r70;
add.s32 %r72, %r71, -63;
ld.global.u32 %r121, [%rd1];
ld.global.u32 %r73, [%rd2];
xor.b32 %r119, %r73, -2147483648;
setp.ge.u32	%p3, %r72, %r1;
@%p3 bra BB0_5;

ld.global.u32 %r120, [%rd3];
ld.global.u32 %r74, [%rd4];
xor.b32 %r118, %r74, -2147483648;

BB0_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r121;
mov.b32 {blow,bhigh}, %r120;
mov.b32 %r123, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r121;
mov.b32 {blow,bhigh}, %r120;
mov.b32 %r80, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r119;
mov.b32 {blow,bhigh}, %r118;
mov.b32 %r122, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r119;
mov.b32 {blow,bhigh}, %r118;
mov.b32 %r86, {ahigh,bhigh};}



BB0_6:

	{add.f16x2 %r89,%r123,%r122;
}

	
	{sub.f16x2 %r95,%r123,%r122;
}

	@%p1 bra BB0_11;

mov.u32 %r107, %tid.x;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r89;
mov.b32 {blow,bhigh}, %r95;
mov.b32 %r101, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r89;
mov.b32 {blow,bhigh}, %r95;
mov.b32 %r104, {ahigh,bhigh};}


	setp.eq.s16	%p5, %rs1, 0;
selp.b32	%r108, 1, 2, %p5;
shl.b32 %r31, %r3, 1;
shl.b32 %r109, %r2, 6;
add.s32 %r110, %r31, %r109;
add.s32 %r111, %r110, 1;
mov.u32 %r32, %nctaid.x;
add.s32 %r112, %r32, -1;
setp.lt.u32	%p6, %r2, %r112;
mad.lo.s32 %r113, %r110, %r108, %r107;
cvta.to.global.u64 %rd14, %rd8;
mul.wide.u32 %rd15, %r113, 4;
add.s64 %rd5, %rd14, %rd15;
mad.lo.s32 %r114, %r111, %r108, %r107;
mul.wide.u32 %rd16, %r114, 4;
add.s64 %rd6, %rd14, %rd16;
@%p6 bra BB0_9;
bra.uni BB0_8;

BB0_9:
st.global.u32 [%rd5], %r101;
bra.uni BB0_10;

BB0_8:
shl.b32 %r115, %r32, 6;
add.s32 %r116, %r31, %r115;
add.s32 %r117, %r116, -63;
st.global.u32 [%rd5], %r101;
setp.ge.u32	%p7, %r117, %r1;
@%p7 bra BB0_11;

BB0_10:
st.global.u32 [%rd6], %r104;

BB0_11:
ret;
}


.weak .entry _Z14vector_fft_c2rILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<8>;
.reg .b16 %rs<10>;
.reg .f32 %f<11>;
.reg .b32 %r<206>;
.reg .f64 %fd<2>;
.reg .b64 %rd<17>;


ld.param.u64 %rd8, [_Z14vector_fft_c2rILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_c2rILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r67, %r2, 4;
mov.u32 %r3, %tid.y;
add.s32 %r68, %r67, %r3;
shl.b32 %r4, %r68, 1;
mov.u32 %r5, %tid.x;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB1_6;

shl.b32 %r6, %r3, 1;
shl.b32 %r69, %r2, 5;
add.s32 %r70, %r6, %r69;
mul.lo.s32 %r71, %r70, 3;
add.s32 %r72, %r71, 3;
mov.u32 %r7, %nctaid.x;
add.s32 %r73, %r7, -1;
setp.lt.u32	%p2, %r2, %r73;
add.s32 %r74, %r71, %r5;
mov.u32 %r75, 2;
sub.s32 %r76, %r75, %r5;
add.s32 %r77, %r71, %r76;
cvta.to.global.u64 %rd9, %rd7;
mul.wide.u32 %rd10, %r74, 4;
add.s64 %rd1, %rd9, %rd10;
mul.wide.u32 %rd11, %r77, 4;
add.s64 %rd2, %rd9, %rd11;
add.s32 %r78, %r72, %r5;
mul.wide.u32 %rd12, %r78, 4;
add.s64 %rd3, %rd9, %rd12;
add.s32 %r79, %r72, %r76;
mul.wide.u32 %rd13, %r79, 4;
add.s64 %rd4, %rd9, %rd13;
@%p2 bra BB1_4;
bra.uni BB1_2;

BB1_4:
ld.global.u32 %r201, [%rd1];
ld.global.u32 %r86, [%rd2];
xor.b32 %r199, %r86, -2147483648;
ld.global.u32 %r200, [%rd3];
ld.global.u32 %r87, [%rd4];
xor.b32 %r198, %r87, -2147483648;
bra.uni BB1_5;

BB1_2:
shl.b32 %r81, %r7, 5;
add.s32 %r82, %r6, %r81;
add.s32 %r83, %r82, -31;
ld.global.u32 %r201, [%rd1];
ld.global.u32 %r84, [%rd2];
xor.b32 %r199, %r84, -2147483648;
setp.ge.u32	%p3, %r83, %r1;
@%p3 bra BB1_5;

ld.global.u32 %r200, [%rd3];
ld.global.u32 %r85, [%rd4];
xor.b32 %r198, %r85, -2147483648;

BB1_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r201;
mov.b32 {blow,bhigh}, %r200;
mov.b32 %r205, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r201;
mov.b32 {blow,bhigh}, %r200;
mov.b32 %r204, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r199;
mov.b32 {blow,bhigh}, %r198;
mov.b32 %r203, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r199;
mov.b32 {blow,bhigh}, %r198;
mov.b32 %r202, {ahigh,bhigh};}



BB1_6:

	{add.f16x2 %r100,%r205,%r203;
}

	
	{add.f16x2 %r103,%r204,%r202;
}

	
	{sub.f16x2 %r106,%r205,%r203;
}

	
	{sub.f16x2 %r109,%r204,%r202;
}

	and.b32 %r30, %r5, 1;
cvt.rn.f32.u32	%f8, %r30;
mul.f32 %f9, %f8, 0f3FC90FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r112, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r112;
mov.b32 %r115, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r112;
mov.b32 %r117, {high,high};}


	
	{mul.f16x2 %r119,%r109,%r117;
}

	
	{fma.rn.f16x2 %r122,%r106,%r115,%r119;
}

	
	{mul.f16x2 %r126,%r106,%r117;
}

	
	{xor.b32 %r129,%r126,0x80008000;
}

	
	{fma.rn.f16x2 %r131,%r109,%r115,%r129;
}

	shl.b32 %r152, %r5, 1;
and.b32 %r153, %r152, -4;
shl.b32 %r33, %r3, 2;
add.s32 %r34, %r153, %r33;
barrier.sync 0;
shl.b32 %r154, %r30, 1;
add.s32 %r155, %r154, %r34;
shl.b32 %r156, %r155, 2;
mov.u32 %r157, smem_full;
add.s32 %r35, %r157, %r156;
st.shared.u32 [%r35], %r100;
st.shared.u32 [%r35+4], %r122;
barrier.sync 0;
add.s32 %r158, %r30, %r34;
shl.b32 %r159, %r158, 2;
add.s32 %r36, %r157, %r159;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+8];
barrier.sync 0;
st.shared.u32 [%r35], %r103;
st.shared.u32 [%r35+4], %r131;
barrier.sync 0;

	{add.f16x2 %r161,%r37,%r38;
}

	
	{sub.f16x2 %r167,%r37,%r38;
}

	@%p1 bra BB1_11;

barrier.sync 0;
add.s32 %r173, %r5, %r33;
shl.b32 %r174, %r173, 2;
add.s32 %r176, %r157, %r174;
st.shared.u32 [%r176], %r161;
st.shared.u32 [%r176+8], %r167;
barrier.sync 0;
add.s32 %r184, %r152, %r33;
shl.b32 %r185, %r184, 2;
add.s32 %r187, %r157, %r185;
ld.shared.u32 %r178, [%r187];
ld.shared.u32 %r179, [%r187+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r178;
mov.b32 {blow,bhigh}, %r179;
mov.b32 %r177, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r178;
mov.b32 {blow,bhigh}, %r179;
mov.b32 %r180, {ahigh,bhigh};}


	setp.eq.s16	%p5, %rs1, 0;
selp.b32	%r188, 2, 3, %p5;
shl.b32 %r43, %r3, 1;
shl.b32 %r189, %r2, 5;
add.s32 %r190, %r43, %r189;
add.s32 %r191, %r190, 1;
mov.u32 %r44, %nctaid.x;
add.s32 %r192, %r44, -1;
setp.lt.u32	%p6, %r2, %r192;
mad.lo.s32 %r193, %r190, %r188, %r5;
cvta.to.global.u64 %rd14, %rd8;
mul.wide.u32 %rd15, %r193, 4;
add.s64 %rd5, %rd14, %rd15;
mad.lo.s32 %r194, %r191, %r188, %r5;
mul.wide.u32 %rd16, %r194, 4;
add.s64 %rd6, %rd14, %rd16;
@%p6 bra BB1_9;
bra.uni BB1_8;

BB1_9:
st.global.u32 [%rd5], %r177;
bra.uni BB1_10;

BB1_8:
shl.b32 %r195, %r44, 5;
add.s32 %r196, %r43, %r195;
add.s32 %r197, %r196, -31;
st.global.u32 [%rd5], %r177;
setp.ge.u32	%p7, %r197, %r1;
@%p7 bra BB1_11;

BB1_10:
st.global.u32 [%rd6], %r180;

BB1_11:
ret;
}


.weak .entry _Z14vector_fft_c2rILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<8>;
.reg .b16 %rs<10>;
.reg .f32 %f<20>;
.reg .b32 %r<277>;
.reg .f64 %fd<2>;
.reg .b64 %rd<17>;


ld.param.u64 %rd8, [_Z14vector_fft_c2rILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_c2rILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r77, %r2, 3;
mov.u32 %r3, %tid.y;
add.s32 %r78, %r77, %r3;
shl.b32 %r4, %r78, 1;
mov.u32 %r5, %tid.x;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB2_6;

shl.b32 %r6, %r3, 1;
shl.b32 %r79, %r2, 4;
mov.u32 %r80, 4;
add.s32 %r81, %r6, %r79;
mul.lo.s32 %r82, %r81, 5;
add.s32 %r83, %r82, 5;
mov.u32 %r7, %nctaid.x;
add.s32 %r84, %r7, -1;
setp.lt.u32	%p2, %r2, %r84;
add.s32 %r85, %r82, %r5;
sub.s32 %r86, %r80, %r5;
add.s32 %r87, %r82, %r86;
cvta.to.global.u64 %rd9, %rd7;
mul.wide.u32 %rd10, %r85, 4;
add.s64 %rd1, %rd9, %rd10;
mul.wide.u32 %rd11, %r87, 4;
add.s64 %rd2, %rd9, %rd11;
add.s32 %r88, %r83, %r5;
mul.wide.u32 %rd12, %r88, 4;
add.s64 %rd3, %rd9, %rd12;
add.s32 %r89, %r83, %r86;
mul.wide.u32 %rd13, %r89, 4;
add.s64 %rd4, %rd9, %rd13;
@%p2 bra BB2_4;
bra.uni BB2_2;

BB2_4:
ld.global.u32 %r272, [%rd1];
ld.global.u32 %r96, [%rd2];
xor.b32 %r270, %r96, -2147483648;
ld.global.u32 %r271, [%rd3];
ld.global.u32 %r97, [%rd4];
xor.b32 %r269, %r97, -2147483648;
bra.uni BB2_5;

BB2_2:
shl.b32 %r91, %r7, 4;
add.s32 %r92, %r6, %r91;
add.s32 %r93, %r92, -15;
ld.global.u32 %r272, [%rd1];
ld.global.u32 %r94, [%rd2];
xor.b32 %r270, %r94, -2147483648;
setp.ge.u32	%p3, %r93, %r1;
@%p3 bra BB2_5;

ld.global.u32 %r271, [%rd3];
ld.global.u32 %r95, [%rd4];
xor.b32 %r269, %r95, -2147483648;

BB2_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r272;
mov.b32 {blow,bhigh}, %r271;
mov.b32 %r276, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r272;
mov.b32 {blow,bhigh}, %r271;
mov.b32 %r275, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r270;
mov.b32 {blow,bhigh}, %r269;
mov.b32 %r274, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r270;
mov.b32 {blow,bhigh}, %r269;
mov.b32 %r273, {ahigh,bhigh};}



BB2_6:

	{add.f16x2 %r110,%r276,%r274;
}

	
	{add.f16x2 %r113,%r275,%r273;
}

	
	{sub.f16x2 %r116,%r276,%r274;
}

	
	{sub.f16x2 %r119,%r275,%r273;
}

	and.b32 %r30, %r5, 3;
cvt.rn.f32.u32	%f8, %r30;
mul.f32 %f9, %f8, 0f3F490FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r122, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r122;
mov.b32 %r125, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r122;
mov.b32 %r127, {high,high};}


	
	{mul.f16x2 %r129,%r119,%r127;
}

	
	{fma.rn.f16x2 %r132,%r116,%r125,%r129;
}

	
	{mul.f16x2 %r136,%r116,%r127;
}

	
	{xor.b32 %r139,%r136,0x80008000;
}

	
	{fma.rn.f16x2 %r141,%r119,%r125,%r139;
}

	shl.b32 %r162, %r5, 1;
and.b32 %r163, %r162, -8;
shl.b32 %r33, %r3, 3;
add.s32 %r34, %r163, %r33;
barrier.sync 0;
shl.b32 %r164, %r30, 1;
add.s32 %r165, %r164, %r34;
shl.b32 %r166, %r165, 2;
mov.u32 %r167, smem_full;
add.s32 %r35, %r167, %r166;
st.shared.u32 [%r35], %r110;
st.shared.u32 [%r35+4], %r132;
barrier.sync 0;
add.s32 %r168, %r30, %r34;
shl.b32 %r169, %r168, 2;
add.s32 %r36, %r167, %r169;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+16];
barrier.sync 0;
st.shared.u32 [%r35], %r113;
st.shared.u32 [%r35+4], %r141;
barrier.sync 0;
ld.shared.u32 %r175, [%r36];
ld.shared.u32 %r176, [%r36+16];

	{add.f16x2 %r171,%r37,%r38;
}

	
	{add.f16x2 %r174,%r175,%r176;
}

	
	{sub.f16x2 %r177,%r37,%r38;
}

	
	{sub.f16x2 %r180,%r175,%r176;
}

	and.b32 %r41, %r5, 2;
bfe.u32 %r223, %r5, 1, 1;
cvt.rn.f32.u32	%f17, %r223;
mul.f32 %f18, %f17, 0f3FC90FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r183, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r183;
mov.b32 %r186, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r183;
mov.b32 %r188, {high,high};}


	
	{mul.f16x2 %r190,%r180,%r188;
}

	
	{fma.rn.f16x2 %r193,%r177,%r186,%r190;
}

	
	{mul.f16x2 %r197,%r177,%r188;
}

	
	{xor.b32 %r200,%r197,0x80008000;
}

	
	{fma.rn.f16x2 %r202,%r180,%r186,%r200;
}

	and.b32 %r224, %r5, 1;
add.s32 %r44, %r34, %r224;
barrier.sync 0;
shl.b32 %r225, %r41, 1;
add.s32 %r226, %r225, %r44;
shl.b32 %r227, %r226, 2;
add.s32 %r45, %r167, %r227;
st.shared.u32 [%r45], %r171;
st.shared.u32 [%r45+8], %r193;
barrier.sync 0;
add.s32 %r229, %r41, %r44;
shl.b32 %r230, %r229, 2;
add.s32 %r46, %r167, %r230;
ld.shared.u32 %r47, [%r46];
ld.shared.u32 %r48, [%r46+16];
barrier.sync 0;
st.shared.u32 [%r45], %r174;
st.shared.u32 [%r45+8], %r202;
barrier.sync 0;

	{add.f16x2 %r232,%r47,%r48;
}

	
	{sub.f16x2 %r238,%r47,%r48;
}

	@%p1 bra BB2_11;

barrier.sync 0;
add.s32 %r244, %r5, %r33;
shl.b32 %r245, %r244, 2;
add.s32 %r247, %r167, %r245;
st.shared.u32 [%r247], %r232;
st.shared.u32 [%r247+16], %r238;
barrier.sync 0;
add.s32 %r255, %r162, %r33;
shl.b32 %r256, %r255, 2;
add.s32 %r258, %r167, %r256;
ld.shared.u32 %r249, [%r258];
ld.shared.u32 %r250, [%r258+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r249;
mov.b32 {blow,bhigh}, %r250;
mov.b32 %r248, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r249;
mov.b32 {blow,bhigh}, %r250;
mov.b32 %r251, {ahigh,bhigh};}


	setp.eq.s16	%p5, %rs1, 0;
selp.b32	%r259, 4, 5, %p5;
shl.b32 %r53, %r3, 1;
shl.b32 %r260, %r2, 4;
add.s32 %r261, %r53, %r260;
add.s32 %r262, %r261, 1;
mov.u32 %r54, %nctaid.x;
add.s32 %r263, %r54, -1;
setp.lt.u32	%p6, %r2, %r263;
mad.lo.s32 %r264, %r261, %r259, %r5;
cvta.to.global.u64 %rd14, %rd8;
mul.wide.u32 %rd15, %r264, 4;
add.s64 %rd5, %rd14, %rd15;
mad.lo.s32 %r265, %r262, %r259, %r5;
mul.wide.u32 %rd16, %r265, 4;
add.s64 %rd6, %rd14, %rd16;
@%p6 bra BB2_9;
bra.uni BB2_8;

BB2_9:
st.global.u32 [%rd5], %r248;
bra.uni BB2_10;

BB2_8:
shl.b32 %r266, %r54, 4;
add.s32 %r267, %r53, %r266;
add.s32 %r268, %r267, -15;
st.global.u32 [%rd5], %r248;
setp.ge.u32	%p7, %r268, %r1;
@%p7 bra BB2_11;

BB2_10:
st.global.u32 [%rd6], %r251;

BB2_11:
ret;
}


.weak .entry _Z14vector_fft_c2rILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<8>;
.reg .b16 %rs<10>;
.reg .f32 %f<29>;
.reg .b32 %r<348>;
.reg .f64 %fd<2>;
.reg .b64 %rd<17>;


ld.param.u64 %rd8, [_Z14vector_fft_c2rILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_c2rILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r87, %r2, 2;
mov.u32 %r3, %tid.y;
add.s32 %r88, %r87, %r3;
shl.b32 %r4, %r88, 1;
mov.u32 %r5, %tid.x;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB3_6;

shl.b32 %r6, %r3, 1;
shl.b32 %r89, %r2, 3;
add.s32 %r90, %r6, %r89;
mul.lo.s32 %r91, %r90, 9;
add.s32 %r92, %r91, 9;
mov.u32 %r7, %nctaid.x;
add.s32 %r93, %r7, -1;
setp.lt.u32	%p2, %r2, %r93;
add.s32 %r94, %r91, %r5;
mov.u32 %r95, 8;
sub.s32 %r96, %r95, %r5;
add.s32 %r97, %r91, %r96;
cvta.to.global.u64 %rd9, %rd7;
mul.wide.u32 %rd10, %r94, 4;
add.s64 %rd1, %rd9, %rd10;
mul.wide.u32 %rd11, %r97, 4;
add.s64 %rd2, %rd9, %rd11;
add.s32 %r98, %r92, %r5;
mul.wide.u32 %rd12, %r98, 4;
add.s64 %rd3, %rd9, %rd12;
add.s32 %r99, %r92, %r96;
mul.wide.u32 %rd13, %r99, 4;
add.s64 %rd4, %rd9, %rd13;
@%p2 bra BB3_4;
bra.uni BB3_2;

BB3_4:
ld.global.u32 %r343, [%rd1];
ld.global.u32 %r106, [%rd2];
xor.b32 %r341, %r106, -2147483648;
ld.global.u32 %r342, [%rd3];
ld.global.u32 %r107, [%rd4];
xor.b32 %r340, %r107, -2147483648;
bra.uni BB3_5;

BB3_2:
shl.b32 %r101, %r7, 3;
add.s32 %r102, %r6, %r101;
add.s32 %r103, %r102, -7;
ld.global.u32 %r343, [%rd1];
ld.global.u32 %r104, [%rd2];
xor.b32 %r341, %r104, -2147483648;
setp.ge.u32	%p3, %r103, %r1;
@%p3 bra BB3_5;

ld.global.u32 %r342, [%rd3];
ld.global.u32 %r105, [%rd4];
xor.b32 %r340, %r105, -2147483648;

BB3_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r343;
mov.b32 {blow,bhigh}, %r342;
mov.b32 %r347, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r343;
mov.b32 {blow,bhigh}, %r342;
mov.b32 %r346, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r341;
mov.b32 {blow,bhigh}, %r340;
mov.b32 %r345, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r341;
mov.b32 {blow,bhigh}, %r340;
mov.b32 %r344, {ahigh,bhigh};}



BB3_6:

	{add.f16x2 %r120,%r347,%r345;
}

	
	{add.f16x2 %r123,%r346,%r344;
}

	
	{sub.f16x2 %r126,%r347,%r345;
}

	
	{sub.f16x2 %r129,%r346,%r344;
}

	and.b32 %r30, %r5, 7;
cvt.rn.f32.u32	%f8, %r30;
mul.f32 %f9, %f8, 0f3EC90FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r132, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r132;
mov.b32 %r135, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r132;
mov.b32 %r137, {high,high};}


	
	{mul.f16x2 %r139,%r129,%r137;
}

	
	{fma.rn.f16x2 %r142,%r126,%r135,%r139;
}

	
	{mul.f16x2 %r146,%r126,%r137;
}

	
	{xor.b32 %r149,%r146,0x80008000;
}

	
	{fma.rn.f16x2 %r151,%r129,%r135,%r149;
}

	shl.b32 %r172, %r5, 1;
and.b32 %r173, %r172, -16;
shl.b32 %r33, %r3, 4;
add.s32 %r34, %r173, %r33;
barrier.sync 0;
shl.b32 %r174, %r30, 1;
add.s32 %r175, %r174, %r34;
shl.b32 %r176, %r175, 2;
mov.u32 %r177, smem_full;
add.s32 %r35, %r177, %r176;
st.shared.u32 [%r35], %r120;
st.shared.u32 [%r35+4], %r142;
barrier.sync 0;
add.s32 %r178, %r30, %r34;
shl.b32 %r179, %r178, 2;
add.s32 %r36, %r177, %r179;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+32];
barrier.sync 0;
st.shared.u32 [%r35], %r123;
st.shared.u32 [%r35+4], %r151;
barrier.sync 0;
ld.shared.u32 %r185, [%r36];
ld.shared.u32 %r186, [%r36+32];

	{add.f16x2 %r181,%r37,%r38;
}

	
	{add.f16x2 %r184,%r185,%r186;
}

	
	{sub.f16x2 %r187,%r37,%r38;
}

	
	{sub.f16x2 %r190,%r185,%r186;
}

	and.b32 %r41, %r5, 6;
bfe.u32 %r233, %r5, 1, 2;
cvt.rn.f32.u32	%f17, %r233;
mul.f32 %f18, %f17, 0f3F490FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r193, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r193;
mov.b32 %r196, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r193;
mov.b32 %r198, {high,high};}


	
	{mul.f16x2 %r200,%r190,%r198;
}

	
	{fma.rn.f16x2 %r203,%r187,%r196,%r200;
}

	
	{mul.f16x2 %r207,%r187,%r198;
}

	
	{xor.b32 %r210,%r207,0x80008000;
}

	
	{fma.rn.f16x2 %r212,%r190,%r196,%r210;
}

	and.b32 %r234, %r5, 1;
add.s32 %r44, %r34, %r234;
barrier.sync 0;
shl.b32 %r235, %r41, 1;
add.s32 %r236, %r235, %r44;
shl.b32 %r237, %r236, 2;
add.s32 %r45, %r177, %r237;
st.shared.u32 [%r45], %r181;
st.shared.u32 [%r45+8], %r203;
barrier.sync 0;
add.s32 %r239, %r41, %r44;
shl.b32 %r240, %r239, 2;
add.s32 %r46, %r177, %r240;
ld.shared.u32 %r47, [%r46];
ld.shared.u32 %r48, [%r46+32];
barrier.sync 0;
st.shared.u32 [%r45], %r184;
st.shared.u32 [%r45+8], %r212;
barrier.sync 0;
ld.shared.u32 %r246, [%r46];
ld.shared.u32 %r247, [%r46+32];

	{add.f16x2 %r242,%r47,%r48;
}

	
	{add.f16x2 %r245,%r246,%r247;
}

	
	{sub.f16x2 %r248,%r47,%r48;
}

	
	{sub.f16x2 %r251,%r246,%r247;
}

	and.b32 %r51, %r5, 4;
bfe.u32 %r294, %r5, 2, 1;
cvt.rn.f32.u32	%f26, %r294;
mul.f32 %f27, %f26, 0f3FC90FDB;
cos.approx.f32 %f20, %f27;
sin.approx.f32 %f28, %f27;
neg.f32 %f21, %f28;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f20;
cvt.rn.f16.f32 high, %f21;
mov.b32 %r254, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r254;
mov.b32 %r257, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r254;
mov.b32 %r259, {high,high};}


	
	{mul.f16x2 %r261,%r251,%r259;
}

	
	{fma.rn.f16x2 %r264,%r248,%r257,%r261;
}

	
	{mul.f16x2 %r268,%r248,%r259;
}

	
	{xor.b32 %r271,%r268,0x80008000;
}

	
	{fma.rn.f16x2 %r273,%r251,%r257,%r271;
}

	and.b32 %r295, %r5, 3;
add.s32 %r54, %r34, %r295;
barrier.sync 0;
shl.b32 %r296, %r51, 1;
add.s32 %r297, %r296, %r54;
shl.b32 %r298, %r297, 2;
add.s32 %r55, %r177, %r298;
st.shared.u32 [%r55], %r242;
st.shared.u32 [%r55+16], %r264;
barrier.sync 0;
add.s32 %r300, %r51, %r54;
shl.b32 %r301, %r300, 2;
add.s32 %r56, %r177, %r301;
ld.shared.u32 %r57, [%r56];
ld.shared.u32 %r58, [%r56+32];
barrier.sync 0;
st.shared.u32 [%r55], %r245;
st.shared.u32 [%r55+16], %r273;
barrier.sync 0;

	{add.f16x2 %r303,%r57,%r58;
}

	
	{sub.f16x2 %r309,%r57,%r58;
}

	@%p1 bra BB3_11;

barrier.sync 0;
add.s32 %r315, %r5, %r33;
shl.b32 %r316, %r315, 2;
add.s32 %r318, %r177, %r316;
st.shared.u32 [%r318], %r303;
st.shared.u32 [%r318+32], %r309;
barrier.sync 0;
add.s32 %r326, %r172, %r33;
shl.b32 %r327, %r326, 2;
add.s32 %r329, %r177, %r327;
ld.shared.u32 %r320, [%r329];
ld.shared.u32 %r321, [%r329+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r320;
mov.b32 {blow,bhigh}, %r321;
mov.b32 %r319, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r320;
mov.b32 {blow,bhigh}, %r321;
mov.b32 %r322, {ahigh,bhigh};}


	setp.eq.s16	%p5, %rs1, 0;
selp.b32	%r330, 8, 9, %p5;
shl.b32 %r63, %r3, 1;
shl.b32 %r331, %r2, 3;
add.s32 %r332, %r63, %r331;
add.s32 %r333, %r332, 1;
mov.u32 %r64, %nctaid.x;
add.s32 %r334, %r64, -1;
setp.lt.u32	%p6, %r2, %r334;
mad.lo.s32 %r335, %r332, %r330, %r5;
cvta.to.global.u64 %rd14, %rd8;
mul.wide.u32 %rd15, %r335, 4;
add.s64 %rd5, %rd14, %rd15;
mad.lo.s32 %r336, %r333, %r330, %r5;
mul.wide.u32 %rd16, %r336, 4;
add.s64 %rd6, %rd14, %rd16;
@%p6 bra BB3_9;
bra.uni BB3_8;

BB3_9:
st.global.u32 [%rd5], %r319;
bra.uni BB3_10;

BB3_8:
shl.b32 %r337, %r64, 3;
add.s32 %r338, %r63, %r337;
add.s32 %r339, %r338, -7;
st.global.u32 [%rd5], %r319;
setp.ge.u32	%p7, %r339, %r1;
@%p7 bra BB3_11;

BB3_10:
st.global.u32 [%rd6], %r322;

BB3_11:
ret;
}


.weak .entry _Z14vector_fft_c2rILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 128, 1, 1
{
.reg .pred %p<8>;
.reg .b16 %rs<10>;
.reg .f32 %f<38>;
.reg .b32 %r<420>;
.reg .f64 %fd<2>;
.reg .b64 %rd<17>;


ld.param.u64 %rd8, [_Z14vector_fft_c2rILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_c2rILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r97, %r2, 3;
mov.u32 %r3, %tid.y;
add.s32 %r98, %r97, %r3;
shl.b32 %r4, %r98, 1;
mov.u32 %r5, %tid.x;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB4_6;

shl.b32 %r6, %r3, 1;
shl.b32 %r99, %r2, 4;
add.s32 %r100, %r6, %r99;
mul.lo.s32 %r101, %r100, 17;
add.s32 %r102, %r101, 17;
mov.u32 %r7, %nctaid.x;
add.s32 %r103, %r7, -1;
setp.lt.u32	%p2, %r2, %r103;
add.s32 %r104, %r101, %r5;
mov.u32 %r105, 16;
sub.s32 %r106, %r105, %r5;
add.s32 %r107, %r101, %r106;
cvta.to.global.u64 %rd9, %rd7;
mul.wide.u32 %rd10, %r104, 4;
add.s64 %rd1, %rd9, %rd10;
mul.wide.u32 %rd11, %r107, 4;
add.s64 %rd2, %rd9, %rd11;
add.s32 %r108, %r102, %r5;
mul.wide.u32 %rd12, %r108, 4;
add.s64 %rd3, %rd9, %rd12;
add.s32 %r109, %r102, %r106;
mul.wide.u32 %rd13, %r109, 4;
add.s64 %rd4, %rd9, %rd13;
@%p2 bra BB4_4;
bra.uni BB4_2;

BB4_4:
ld.global.u32 %r415, [%rd1];
ld.global.u32 %r116, [%rd2];
xor.b32 %r413, %r116, -2147483648;
ld.global.u32 %r414, [%rd3];
ld.global.u32 %r117, [%rd4];
xor.b32 %r412, %r117, -2147483648;
bra.uni BB4_5;

BB4_2:
shl.b32 %r111, %r7, 4;
add.s32 %r112, %r6, %r111;
add.s32 %r113, %r112, -15;
ld.global.u32 %r415, [%rd1];
ld.global.u32 %r114, [%rd2];
xor.b32 %r413, %r114, -2147483648;
setp.ge.u32	%p3, %r113, %r1;
@%p3 bra BB4_5;

ld.global.u32 %r414, [%rd3];
ld.global.u32 %r115, [%rd4];
xor.b32 %r412, %r115, -2147483648;

BB4_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r415;
mov.b32 {blow,bhigh}, %r414;
mov.b32 %r419, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r415;
mov.b32 {blow,bhigh}, %r414;
mov.b32 %r418, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r413;
mov.b32 {blow,bhigh}, %r412;
mov.b32 %r417, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r413;
mov.b32 {blow,bhigh}, %r412;
mov.b32 %r416, {ahigh,bhigh};}



BB4_6:

	{add.f16x2 %r130,%r419,%r417;
}

	
	{add.f16x2 %r133,%r418,%r416;
}

	
	{sub.f16x2 %r136,%r419,%r417;
}

	
	{sub.f16x2 %r139,%r418,%r416;
}

	and.b32 %r30, %r5, 15;
cvt.rn.f32.u32	%f8, %r30;
mul.f32 %f9, %f8, 0f3E490FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r142, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r142;
mov.b32 %r145, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r142;
mov.b32 %r147, {high,high};}


	
	{mul.f16x2 %r149,%r139,%r147;
}

	
	{fma.rn.f16x2 %r152,%r136,%r145,%r149;
}

	
	{mul.f16x2 %r156,%r136,%r147;
}

	
	{xor.b32 %r159,%r156,0x80008000;
}

	
	{fma.rn.f16x2 %r161,%r139,%r145,%r159;
}

	shl.b32 %r182, %r5, 1;
and.b32 %r183, %r182, -32;
shl.b32 %r33, %r3, 5;
add.s32 %r34, %r183, %r33;
barrier.sync 0;
shl.b32 %r184, %r30, 1;
add.s32 %r185, %r184, %r34;
shl.b32 %r186, %r185, 2;
mov.u32 %r187, smem_full;
add.s32 %r35, %r187, %r186;
st.shared.u32 [%r35], %r130;
st.shared.u32 [%r35+4], %r152;
barrier.sync 0;
add.s32 %r188, %r30, %r34;
shl.b32 %r189, %r188, 2;
add.s32 %r36, %r187, %r189;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+64];
barrier.sync 0;
st.shared.u32 [%r35], %r133;
st.shared.u32 [%r35+4], %r161;
barrier.sync 0;
ld.shared.u32 %r195, [%r36];
ld.shared.u32 %r196, [%r36+64];

	{add.f16x2 %r191,%r37,%r38;
}

	
	{add.f16x2 %r194,%r195,%r196;
}

	
	{sub.f16x2 %r197,%r37,%r38;
}

	
	{sub.f16x2 %r200,%r195,%r196;
}

	and.b32 %r41, %r5, 14;
bfe.u32 %r243, %r5, 1, 3;
cvt.rn.f32.u32	%f17, %r243;
mul.f32 %f18, %f17, 0f3EC90FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r203, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r203;
mov.b32 %r206, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r203;
mov.b32 %r208, {high,high};}


	
	{mul.f16x2 %r210,%r200,%r208;
}

	
	{fma.rn.f16x2 %r213,%r197,%r206,%r210;
}

	
	{mul.f16x2 %r217,%r197,%r208;
}

	
	{xor.b32 %r220,%r217,0x80008000;
}

	
	{fma.rn.f16x2 %r222,%r200,%r206,%r220;
}

	neg.s32 %r244, %r5;
and.b32 %r245, %r244, 1;
add.s32 %r44, %r34, %r245;
barrier.sync 0;
shl.b32 %r246, %r41, 1;
add.s32 %r247, %r246, %r44;
shl.b32 %r248, %r247, 2;
add.s32 %r45, %r187, %r248;
st.shared.u32 [%r45], %r191;
st.shared.u32 [%r45+8], %r213;
barrier.sync 0;
add.s32 %r250, %r41, %r44;
shl.b32 %r251, %r250, 2;
add.s32 %r46, %r187, %r251;
ld.shared.u32 %r47, [%r46];
ld.shared.u32 %r48, [%r46+64];
barrier.sync 0;
st.shared.u32 [%r45], %r194;
st.shared.u32 [%r45+8], %r222;
barrier.sync 0;
ld.shared.u32 %r257, [%r46];
ld.shared.u32 %r258, [%r46+64];

	{add.f16x2 %r253,%r47,%r48;
}

	
	{add.f16x2 %r256,%r257,%r258;
}

	
	{sub.f16x2 %r259,%r47,%r48;
}

	
	{sub.f16x2 %r262,%r257,%r258;
}

	and.b32 %r51, %r5, 12;
bfe.u32 %r305, %r5, 2, 2;
cvt.rn.f32.u32	%f26, %r305;
mul.f32 %f27, %f26, 0f3F490FDB;
cos.approx.f32 %f20, %f27;
sin.approx.f32 %f28, %f27;
neg.f32 %f21, %f28;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f20;
cvt.rn.f16.f32 high, %f21;
mov.b32 %r265, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r265;
mov.b32 %r268, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r265;
mov.b32 %r270, {high,high};}


	
	{mul.f16x2 %r272,%r262,%r270;
}

	
	{fma.rn.f16x2 %r275,%r259,%r268,%r272;
}

	
	{mul.f16x2 %r279,%r259,%r270;
}

	
	{xor.b32 %r282,%r279,0x80008000;
}

	
	{fma.rn.f16x2 %r284,%r262,%r268,%r282;
}

	and.b32 %r306, %r5, 3;
add.s32 %r54, %r34, %r306;
barrier.sync 0;
shl.b32 %r307, %r51, 1;
add.s32 %r308, %r307, %r54;
shl.b32 %r309, %r308, 2;
add.s32 %r55, %r187, %r309;
st.shared.u32 [%r55], %r253;
st.shared.u32 [%r55+16], %r275;
barrier.sync 0;
add.s32 %r311, %r51, %r54;
shl.b32 %r312, %r311, 2;
add.s32 %r56, %r187, %r312;
ld.shared.u32 %r57, [%r56];
ld.shared.u32 %r58, [%r56+64];
barrier.sync 0;
st.shared.u32 [%r55], %r256;
st.shared.u32 [%r55+16], %r284;
barrier.sync 0;
ld.shared.u32 %r318, [%r56];
ld.shared.u32 %r319, [%r56+64];

	{add.f16x2 %r314,%r57,%r58;
}

	
	{add.f16x2 %r317,%r318,%r319;
}

	
	{sub.f16x2 %r320,%r57,%r58;
}

	
	{sub.f16x2 %r323,%r318,%r319;
}

	and.b32 %r61, %r5, 8;
bfe.u32 %r366, %r5, 3, 1;
cvt.rn.f32.u32	%f35, %r366;
mul.f32 %f36, %f35, 0f3FC90FDB;
cos.approx.f32 %f29, %f36;
sin.approx.f32 %f37, %f36;
neg.f32 %f30, %f37;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f29;
cvt.rn.f16.f32 high, %f30;
mov.b32 %r326, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r326;
mov.b32 %r329, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r326;
mov.b32 %r331, {high,high};}


	
	{mul.f16x2 %r333,%r323,%r331;
}

	
	{fma.rn.f16x2 %r336,%r320,%r329,%r333;
}

	
	{mul.f16x2 %r340,%r320,%r331;
}

	
	{xor.b32 %r343,%r340,0x80008000;
}

	
	{fma.rn.f16x2 %r345,%r323,%r329,%r343;
}

	and.b32 %r367, %r5, 7;
add.s32 %r64, %r34, %r367;
barrier.sync 0;
shl.b32 %r368, %r61, 1;
add.s32 %r369, %r368, %r64;
shl.b32 %r370, %r369, 2;
add.s32 %r65, %r187, %r370;
st.shared.u32 [%r65], %r314;
st.shared.u32 [%r65+32], %r336;
barrier.sync 0;
add.s32 %r372, %r61, %r64;
shl.b32 %r373, %r372, 2;
add.s32 %r66, %r187, %r373;
ld.shared.u32 %r67, [%r66];
ld.shared.u32 %r68, [%r66+64];
barrier.sync 0;
st.shared.u32 [%r65], %r317;
st.shared.u32 [%r65+32], %r345;
barrier.sync 0;

	{add.f16x2 %r375,%r67,%r68;
}

	
	{sub.f16x2 %r381,%r67,%r68;
}

	@%p1 bra BB4_11;

barrier.sync 0;
add.s32 %r387, %r5, %r33;
shl.b32 %r388, %r387, 2;
add.s32 %r390, %r187, %r388;
st.shared.u32 [%r390], %r375;
st.shared.u32 [%r390+64], %r381;
barrier.sync 0;
add.s32 %r398, %r182, %r33;
shl.b32 %r399, %r398, 2;
add.s32 %r401, %r187, %r399;
ld.shared.u32 %r392, [%r401];
ld.shared.u32 %r393, [%r401+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r392;
mov.b32 {blow,bhigh}, %r393;
mov.b32 %r391, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r392;
mov.b32 {blow,bhigh}, %r393;
mov.b32 %r394, {ahigh,bhigh};}


	setp.eq.s16	%p5, %rs1, 0;
selp.b32	%r402, 16, 17, %p5;
shl.b32 %r73, %r3, 1;
shl.b32 %r403, %r2, 4;
add.s32 %r404, %r73, %r403;
add.s32 %r405, %r404, 1;
mov.u32 %r74, %nctaid.x;
add.s32 %r406, %r74, -1;
setp.lt.u32	%p6, %r2, %r406;
mad.lo.s32 %r407, %r404, %r402, %r5;
cvta.to.global.u64 %rd14, %rd8;
mul.wide.u32 %rd15, %r407, 4;
add.s64 %rd5, %rd14, %rd15;
mad.lo.s32 %r408, %r405, %r402, %r5;
mul.wide.u32 %rd16, %r408, 4;
add.s64 %rd6, %rd14, %rd16;
@%p6 bra BB4_9;
bra.uni BB4_8;

BB4_9:
st.global.u32 [%rd5], %r391;
bra.uni BB4_10;

BB4_8:
shl.b32 %r409, %r74, 4;
add.s32 %r410, %r73, %r409;
add.s32 %r411, %r410, -15;
st.global.u32 [%rd5], %r391;
setp.ge.u32	%p7, %r411, %r1;
@%p7 bra BB4_11;

BB4_10:
st.global.u32 [%rd6], %r394;

BB4_11:
ret;
}


.weak .entry _Z14vector_fft_c2rILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 16, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<10>;
.reg .f32 %f<28>;
.reg .b32 %r<560>;
.reg .f64 %fd<2>;
.reg .b64 %rd<15>;


ld.param.u32 %r1, [_Z14vector_fft_c2rILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd6, [_Z14vector_fft_c2rILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
mul.lo.s32 %r91, %r2, 66;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p1, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r92, %r91, %r5;
mov.u32 %r93, 16;
sub.s32 %r94, %r93, %r5;
add.s32 %r95, %r91, %r94;
cvta.to.global.u64 %rd8, %rd6;
mul.wide.u32 %rd9, %r92, 4;
add.s64 %rd1, %rd8, %rd9;
mul.wide.u32 %rd10, %r95, 4;
add.s64 %rd2, %rd8, %rd10;
add.s32 %r96, %r95, 33;
mul.wide.u32 %rd11, %r96, 4;
add.s64 %rd3, %rd8, %rd11;
@%p1 bra BB5_3;
bra.uni BB5_1;

BB5_3:
ld.global.u32 %r559, [%rd1];
ld.global.u32 %r104, [%rd2];
xor.b32 %r553, %r104, -2147483648;
ld.global.u32 %r557, [%rd1+64];
ld.global.u32 %r105, [%rd2+64];
xor.b32 %r555, %r105, -2147483648;
ld.global.u32 %r558, [%rd1+132];
ld.global.u32 %r106, [%rd3];
xor.b32 %r552, %r106, -2147483648;
ld.global.u32 %r556, [%rd1+196];
ld.global.u32 %r107, [%rd3+64];
xor.b32 %r554, %r107, -2147483648;
bra.uni BB5_4;

BB5_1:
shl.b32 %r98, %r3, 1;
add.s32 %r99, %r98, -1;
ld.global.u32 %r559, [%rd1];
ld.global.u32 %r100, [%rd2];
xor.b32 %r553, %r100, -2147483648;
ld.global.u32 %r557, [%rd1+64];
ld.global.u32 %r101, [%rd2+64];
xor.b32 %r555, %r101, -2147483648;
setp.ge.u32	%p2, %r99, %r1;
@%p2 bra BB5_4;

ld.global.u32 %r558, [%rd1+132];
ld.global.u32 %r102, [%rd3];
xor.b32 %r552, %r102, -2147483648;
ld.global.u32 %r556, [%rd1+196];
ld.global.u32 %r103, [%rd3+64];
xor.b32 %r554, %r103, -2147483648;

BB5_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r559;
mov.b32 {blow,bhigh}, %r558;
mov.b32 %r108, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r559;
mov.b32 {blow,bhigh}, %r558;
mov.b32 %r111, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r557;
mov.b32 {blow,bhigh}, %r556;
mov.b32 %r114, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r557;
mov.b32 {blow,bhigh}, %r556;
mov.b32 %r117, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r555;
mov.b32 {blow,bhigh}, %r554;
mov.b32 %r120, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r555;
mov.b32 {blow,bhigh}, %r554;
mov.b32 %r123, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r553;
mov.b32 {blow,bhigh}, %r552;
mov.b32 %r126, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r553;
mov.b32 {blow,bhigh}, %r552;
mov.b32 %r129, {ahigh,bhigh};}


	
	{add.f16x2 %r132,%r108,%r120;
}

	
	{add.f16x2 %r135,%r111,%r123;
}

	
	{sub.f16x2 %r138,%r108,%r120;
}

	
	{sub.f16x2 %r141,%r111,%r123;
}

	
	{add.f16x2 %r144,%r114,%r126;
}

	
	{add.f16x2 %r147,%r117,%r129;
}

	
	{sub.f16x2 %r150,%r114,%r126;
}

	
	{sub.f16x2 %r153,%r117,%r129;
}

	
	{xor.b32 %r156,%r153,0x80008000;
}

	
	{add.f16x2 %r158,%r132,%r144;
}

	
	{add.f16x2 %r161,%r135,%r147;
}

	
	{sub.f16x2 %r164,%r132,%r144;
}

	
	{sub.f16x2 %r167,%r135,%r147;
}

	
	{add.f16x2 %r170,%r138,%r156;
}

	
	{add.f16x2 %r173,%r141,%r150;
}

	
	{sub.f16x2 %r176,%r138,%r156;
}

	
	{sub.f16x2 %r179,%r141,%r150;
}

	and.b32 %r32, %r5, 15;
cvt.rn.f32.u32	%f12, %r32;
mul.f32 %f13, %f12, 0f3DC90FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r182, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r182;
mov.b32 %r185, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r182;
mov.b32 %r187, {high,high};}


	
	{mul.f16x2 %r189,%r173,%r187;
}

	
	{fma.rn.f16x2 %r192,%r170,%r185,%r189;
}

	
	{mul.f16x2 %r196,%r170,%r187;
}

	
	{xor.b32 %r199,%r196,0x80008000;
}

	
	{fma.rn.f16x2 %r201,%r173,%r185,%r199;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r182;
mov.b32 %r205, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r182;
mov.b32 %r207, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r209, {low,high};}


	
	{mul.f16x2 %r210,%r207,%r209;
}

	
	{mul.f16x2 %r213,%r182,%r205;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r182;
mov.b32 %r216, {high,low};}


	
	{fma.rn.f16x2 %r218,%r210,%r216,%r213;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r218;
mov.b32 %r222, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r218;
mov.b32 %r224, {high,high};}


	
	{mul.f16x2 %r226,%r167,%r224;
}

	
	{fma.rn.f16x2 %r229,%r164,%r222,%r226;
}

	
	{mul.f16x2 %r233,%r164,%r224;
}

	
	{xor.b32 %r236,%r233,0x80008000;
}

	
	{fma.rn.f16x2 %r238,%r167,%r222,%r236;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r182;
mov.b32 %r242, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r182;
mov.b32 %r244, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r246, {low,high};}


	
	{mul.f16x2 %r247,%r244,%r246;
}

	
	{mul.f16x2 %r250,%r218,%r242;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r218;
mov.b32 %r253, {high,low};}


	
	{fma.rn.f16x2 %r255,%r247,%r253,%r250;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r255;
mov.b32 %r259, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r255;
mov.b32 %r261, {high,high};}


	
	{mul.f16x2 %r263,%r179,%r261;
}

	
	{fma.rn.f16x2 %r266,%r176,%r259,%r263;
}

	
	{mul.f16x2 %r270,%r176,%r261;
}

	
	{xor.b32 %r273,%r270,0x80008000;
}

	
	{fma.rn.f16x2 %r275,%r179,%r259,%r273;
}

	shl.b32 %r296, %r5, 2;
and.b32 %r39, %r296, -64;
barrier.sync 0;
shl.b32 %r297, %r32, 2;
add.s32 %r298, %r297, %r39;
shl.b32 %r299, %r298, 2;
mov.u32 %r300, smem_full;
add.s32 %r40, %r300, %r299;
st.shared.u32 [%r40], %r158;
st.shared.u32 [%r40+4], %r192;
st.shared.u32 [%r40+8], %r229;
st.shared.u32 [%r40+12], %r266;
barrier.sync 0;
add.s32 %r301, %r32, %r39;
shl.b32 %r302, %r301, 2;
add.s32 %r41, %r300, %r302;
ld.shared.u32 %r42, [%r41];
ld.shared.u32 %r43, [%r41+64];
ld.shared.u32 %r44, [%r41+128];
ld.shared.u32 %r45, [%r41+192];
barrier.sync 0;
st.shared.u32 [%r40], %r161;
st.shared.u32 [%r40+4], %r201;
st.shared.u32 [%r40+8], %r238;
st.shared.u32 [%r40+12], %r275;
barrier.sync 0;
ld.shared.u32 %r308, [%r41];
ld.shared.u32 %r320, [%r41+64];
ld.shared.u32 %r309, [%r41+128];
ld.shared.u32 %r321, [%r41+192];

	{add.f16x2 %r304,%r42,%r44;
}

	
	{add.f16x2 %r307,%r308,%r309;
}

	
	{sub.f16x2 %r310,%r42,%r44;
}

	
	{sub.f16x2 %r313,%r308,%r309;
}

	
	{add.f16x2 %r316,%r43,%r45;
}

	
	{add.f16x2 %r319,%r320,%r321;
}

	
	{sub.f16x2 %r322,%r43,%r45;
}

	
	{sub.f16x2 %r325,%r320,%r321;
}

	
	{xor.b32 %r328,%r325,0x80008000;
}

	
	{add.f16x2 %r330,%r304,%r316;
}

	
	{add.f16x2 %r333,%r307,%r319;
}

	
	{sub.f16x2 %r336,%r304,%r316;
}

	
	{sub.f16x2 %r339,%r307,%r319;
}

	
	{add.f16x2 %r342,%r310,%r328;
}

	
	{add.f16x2 %r345,%r313,%r322;
}

	
	{sub.f16x2 %r348,%r310,%r328;
}

	
	{sub.f16x2 %r351,%r313,%r322;
}

	and.b32 %r48, %r5, 12;
bfe.u32 %r468, %r5, 2, 2;
cvt.rn.f32.u32	%f25, %r468;
mul.f32 %f26, %f25, 0f3EC90FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r354, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r354;
mov.b32 %r357, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r354;
mov.b32 %r359, {high,high};}


	
	{mul.f16x2 %r361,%r345,%r359;
}

	
	{fma.rn.f16x2 %r364,%r342,%r357,%r361;
}

	
	{mul.f16x2 %r368,%r342,%r359;
}

	
	{xor.b32 %r371,%r368,0x80008000;
}

	
	{fma.rn.f16x2 %r373,%r345,%r357,%r371;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r354;
mov.b32 %r377, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r354;
mov.b32 %r379, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r381, {low,high};}


	
	{mul.f16x2 %r382,%r379,%r381;
}

	
	{mul.f16x2 %r385,%r354,%r377;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r354;
mov.b32 %r388, {high,low};}


	
	{fma.rn.f16x2 %r390,%r382,%r388,%r385;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r390;
mov.b32 %r394, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r390;
mov.b32 %r396, {high,high};}


	
	{mul.f16x2 %r398,%r339,%r396;
}

	
	{fma.rn.f16x2 %r401,%r336,%r394,%r398;
}

	
	{mul.f16x2 %r405,%r336,%r396;
}

	
	{xor.b32 %r408,%r405,0x80008000;
}

	
	{fma.rn.f16x2 %r410,%r339,%r394,%r408;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r354;
mov.b32 %r414, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r354;
mov.b32 %r416, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r418, {low,high};}


	
	{mul.f16x2 %r419,%r416,%r418;
}

	
	{mul.f16x2 %r422,%r390,%r414;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r390;
mov.b32 %r425, {high,low};}


	
	{fma.rn.f16x2 %r427,%r419,%r425,%r422;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r427;
mov.b32 %r431, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r427;
mov.b32 %r433, {high,high};}


	
	{mul.f16x2 %r435,%r351,%r433;
}

	
	{fma.rn.f16x2 %r438,%r348,%r431,%r435;
}

	
	{mul.f16x2 %r442,%r348,%r433;
}

	
	{xor.b32 %r445,%r442,0x80008000;
}

	
	{fma.rn.f16x2 %r447,%r351,%r431,%r445;
}

	and.b32 %r469, %r5, 3;
add.s32 %r55, %r39, %r469;
barrier.sync 0;
shl.b32 %r470, %r48, 2;
add.s32 %r471, %r470, %r55;
shl.b32 %r472, %r471, 2;
add.s32 %r56, %r300, %r472;
st.shared.u32 [%r56], %r330;
st.shared.u32 [%r56+16], %r364;
st.shared.u32 [%r56+32], %r401;
st.shared.u32 [%r56+48], %r438;
barrier.sync 0;
add.s32 %r474, %r48, %r55;
shl.b32 %r475, %r474, 2;
add.s32 %r57, %r300, %r475;
ld.shared.u32 %r58, [%r57];
ld.shared.u32 %r59, [%r57+64];
ld.shared.u32 %r60, [%r57+128];
ld.shared.u32 %r61, [%r57+192];
barrier.sync 0;
st.shared.u32 [%r56], %r333;
st.shared.u32 [%r56+16], %r373;
st.shared.u32 [%r56+32], %r410;
st.shared.u32 [%r56+48], %r447;
barrier.sync 0;
ld.shared.u32 %r499, [%r57+64];
ld.shared.u32 %r500, [%r57+192];

	{add.f16x2 %r477,%r58,%r60;
}

	
	{sub.f16x2 %r483,%r58,%r60;
}

	
	{add.f16x2 %r489,%r59,%r61;
}

	
	{sub.f16x2 %r498,%r499,%r500;
}

	
	{xor.b32 %r501,%r498,0x80008000;
}

	
	{add.f16x2 %r503,%r477,%r489;
}

	
	{sub.f16x2 %r509,%r477,%r489;
}

	
	{add.f16x2 %r515,%r483,%r501;
}

	
	{sub.f16x2 %r521,%r483,%r501;
}

	barrier.sync 0;
add.s32 %r529, %r300, %r296;
st.shared.u32 [%r529], %r503;
st.shared.u32 [%r529+64], %r515;
st.shared.u32 [%r529+128], %r509;
st.shared.u32 [%r529+192], %r521;
barrier.sync 0;
shl.b32 %r542, %r5, 3;
add.s32 %r544, %r300, %r542;
ld.shared.u32 %r531, [%r544];
ld.shared.u32 %r532, [%r544+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r531;
mov.b32 {blow,bhigh}, %r532;
mov.b32 %r530, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r531;
mov.b32 {blow,bhigh}, %r532;
mov.b32 %r533, {ahigh,bhigh};}


	ld.shared.u32 %r537, [%r544+128];
ld.shared.u32 %r538, [%r544+132];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r537;
mov.b32 {blow,bhigh}, %r538;
mov.b32 %r536, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r537;
mov.b32 {blow,bhigh}, %r538;
mov.b32 %r539, {ahigh,bhigh};}


	setp.eq.s16	%p3, %rs1, 0;
selp.b32	%r545, 32, 33, %p3;
shl.b32 %r546, %r2, 1;
add.s32 %r547, %r546, 1;
mad.lo.s32 %r548, %r546, %r545, %r5;
cvta.to.global.u64 %rd12, %rd7;
mul.wide.u32 %rd13, %r548, 4;
add.s64 %rd4, %rd12, %rd13;
mad.lo.s32 %r549, %r547, %r545, %r5;
mul.wide.u32 %rd14, %r549, 4;
add.s64 %rd5, %rd12, %rd14;
@%p1 bra BB5_6;
bra.uni BB5_5;

BB5_6:
st.global.u32 [%rd4], %r530;
st.global.u32 [%rd4+64], %r536;
bra.uni BB5_7;

BB5_5:
shl.b32 %r550, %r3, 1;
add.s32 %r551, %r550, -1;
st.global.u32 [%rd4], %r530;
st.global.u32 [%rd4+64], %r536;
setp.ge.u32	%p5, %r551, %r1;
@%p5 bra BB5_8;

BB5_7:
st.global.u32 [%rd5], %r533;
st.global.u32 [%rd5+64], %r539;

BB5_8:
ret;
}


.weak .entry _Z14vector_fft_c2rILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<10>;
.reg .f32 %f<41>;
.reg .b32 %r<723>;
.reg .f64 %fd<2>;
.reg .b64 %rd<15>;


ld.param.u32 %r1, [_Z14vector_fft_c2rILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd6, [_Z14vector_fft_c2rILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
mul.lo.s32 %r107, %r2, 130;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p1, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r108, %r107, %r5;
mov.u32 %r109, 32;
sub.s32 %r110, %r109, %r5;
add.s32 %r111, %r107, %r110;
cvta.to.global.u64 %rd8, %rd6;
mul.wide.u32 %rd9, %r108, 4;
add.s64 %rd1, %rd8, %rd9;
mul.wide.u32 %rd10, %r111, 4;
add.s64 %rd2, %rd8, %rd10;
add.s32 %r112, %r111, 65;
mul.wide.u32 %rd11, %r112, 4;
add.s64 %rd3, %rd8, %rd11;
@%p1 bra BB6_3;
bra.uni BB6_1;

BB6_3:
ld.global.u32 %r722, [%rd1];
ld.global.u32 %r120, [%rd2];
xor.b32 %r716, %r120, -2147483648;
ld.global.u32 %r720, [%rd1+128];
ld.global.u32 %r121, [%rd2+128];
xor.b32 %r718, %r121, -2147483648;
ld.global.u32 %r721, [%rd1+260];
ld.global.u32 %r122, [%rd3];
xor.b32 %r715, %r122, -2147483648;
ld.global.u32 %r719, [%rd1+388];
ld.global.u32 %r123, [%rd3+128];
xor.b32 %r717, %r123, -2147483648;
bra.uni BB6_4;

BB6_1:
shl.b32 %r114, %r3, 1;
add.s32 %r115, %r114, -1;
ld.global.u32 %r722, [%rd1];
ld.global.u32 %r116, [%rd2];
xor.b32 %r716, %r116, -2147483648;
ld.global.u32 %r720, [%rd1+128];
ld.global.u32 %r117, [%rd2+128];
xor.b32 %r718, %r117, -2147483648;
setp.ge.u32	%p2, %r115, %r1;
@%p2 bra BB6_4;

ld.global.u32 %r721, [%rd1+260];
ld.global.u32 %r118, [%rd3];
xor.b32 %r715, %r118, -2147483648;
ld.global.u32 %r719, [%rd1+388];
ld.global.u32 %r119, [%rd3+128];
xor.b32 %r717, %r119, -2147483648;

BB6_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r722;
mov.b32 {blow,bhigh}, %r721;
mov.b32 %r124, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r722;
mov.b32 {blow,bhigh}, %r721;
mov.b32 %r127, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r720;
mov.b32 {blow,bhigh}, %r719;
mov.b32 %r130, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r720;
mov.b32 {blow,bhigh}, %r719;
mov.b32 %r133, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r718;
mov.b32 {blow,bhigh}, %r717;
mov.b32 %r136, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r718;
mov.b32 {blow,bhigh}, %r717;
mov.b32 %r139, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r716;
mov.b32 {blow,bhigh}, %r715;
mov.b32 %r142, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r716;
mov.b32 {blow,bhigh}, %r715;
mov.b32 %r145, {ahigh,bhigh};}


	
	{add.f16x2 %r148,%r124,%r136;
}

	
	{add.f16x2 %r151,%r127,%r139;
}

	
	{sub.f16x2 %r154,%r124,%r136;
}

	
	{sub.f16x2 %r157,%r127,%r139;
}

	
	{add.f16x2 %r160,%r130,%r142;
}

	
	{add.f16x2 %r163,%r133,%r145;
}

	
	{sub.f16x2 %r166,%r130,%r142;
}

	
	{sub.f16x2 %r169,%r133,%r145;
}

	
	{xor.b32 %r172,%r169,0x80008000;
}

	
	{add.f16x2 %r174,%r148,%r160;
}

	
	{add.f16x2 %r177,%r151,%r163;
}

	
	{sub.f16x2 %r180,%r148,%r160;
}

	
	{sub.f16x2 %r183,%r151,%r163;
}

	
	{add.f16x2 %r186,%r154,%r172;
}

	
	{add.f16x2 %r189,%r157,%r166;
}

	
	{sub.f16x2 %r192,%r154,%r172;
}

	
	{sub.f16x2 %r195,%r157,%r166;
}

	and.b32 %r32, %r5, 31;
cvt.rn.f32.u32	%f12, %r32;
mul.f32 %f13, %f12, 0f3D490FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r198, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r201, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r203, {high,high};}


	
	{mul.f16x2 %r205,%r189,%r203;
}

	
	{fma.rn.f16x2 %r208,%r186,%r201,%r205;
}

	
	{mul.f16x2 %r212,%r186,%r203;
}

	
	{xor.b32 %r215,%r212,0x80008000;
}

	
	{fma.rn.f16x2 %r217,%r189,%r201,%r215;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r221, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r223, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r225, {low,high};}


	
	{mul.f16x2 %r226,%r223,%r225;
}

	
	{mul.f16x2 %r229,%r198,%r221;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r232, {high,low};}


	
	{fma.rn.f16x2 %r234,%r226,%r232,%r229;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r234;
mov.b32 %r238, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r234;
mov.b32 %r240, {high,high};}


	
	{mul.f16x2 %r242,%r183,%r240;
}

	
	{fma.rn.f16x2 %r245,%r180,%r238,%r242;
}

	
	{mul.f16x2 %r249,%r180,%r240;
}

	
	{xor.b32 %r252,%r249,0x80008000;
}

	
	{fma.rn.f16x2 %r254,%r183,%r238,%r252;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r258, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r260, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r262, {low,high};}


	
	{mul.f16x2 %r263,%r260,%r262;
}

	
	{mul.f16x2 %r266,%r234,%r258;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r234;
mov.b32 %r269, {high,low};}


	
	{fma.rn.f16x2 %r271,%r263,%r269,%r266;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r271;
mov.b32 %r275, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r271;
mov.b32 %r277, {high,high};}


	
	{mul.f16x2 %r279,%r195,%r277;
}

	
	{fma.rn.f16x2 %r282,%r192,%r275,%r279;
}

	
	{mul.f16x2 %r286,%r192,%r277;
}

	
	{xor.b32 %r289,%r286,0x80008000;
}

	
	{fma.rn.f16x2 %r291,%r195,%r275,%r289;
}

	shl.b32 %r312, %r5, 2;
and.b32 %r39, %r312, -128;
barrier.sync 0;
shl.b32 %r313, %r32, 2;
add.s32 %r314, %r313, %r39;
shl.b32 %r315, %r314, 2;
mov.u32 %r316, smem_full;
add.s32 %r40, %r316, %r315;
st.shared.u32 [%r40], %r174;
st.shared.u32 [%r40+4], %r208;
st.shared.u32 [%r40+8], %r245;
st.shared.u32 [%r40+12], %r282;
barrier.sync 0;
add.s32 %r317, %r32, %r39;
shl.b32 %r318, %r317, 2;
add.s32 %r41, %r316, %r318;
ld.shared.u32 %r42, [%r41];
ld.shared.u32 %r43, [%r41+128];
ld.shared.u32 %r44, [%r41+256];
ld.shared.u32 %r45, [%r41+384];
barrier.sync 0;
st.shared.u32 [%r40], %r177;
st.shared.u32 [%r40+4], %r217;
st.shared.u32 [%r40+8], %r254;
st.shared.u32 [%r40+12], %r291;
barrier.sync 0;
ld.shared.u32 %r324, [%r41];
ld.shared.u32 %r336, [%r41+128];
ld.shared.u32 %r325, [%r41+256];
ld.shared.u32 %r337, [%r41+384];

	{add.f16x2 %r320,%r42,%r44;
}

	
	{add.f16x2 %r323,%r324,%r325;
}

	
	{sub.f16x2 %r326,%r42,%r44;
}

	
	{sub.f16x2 %r329,%r324,%r325;
}

	
	{add.f16x2 %r332,%r43,%r45;
}

	
	{add.f16x2 %r335,%r336,%r337;
}

	
	{sub.f16x2 %r338,%r43,%r45;
}

	
	{sub.f16x2 %r341,%r336,%r337;
}

	
	{xor.b32 %r344,%r341,0x80008000;
}

	
	{add.f16x2 %r346,%r320,%r332;
}

	
	{add.f16x2 %r349,%r323,%r335;
}

	
	{sub.f16x2 %r352,%r320,%r332;
}

	
	{sub.f16x2 %r355,%r323,%r335;
}

	
	{add.f16x2 %r358,%r326,%r344;
}

	
	{add.f16x2 %r361,%r329,%r338;
}

	
	{sub.f16x2 %r364,%r326,%r344;
}

	
	{sub.f16x2 %r367,%r329,%r338;
}

	shr.u32 %r48, %r32, 2;
cvt.rn.f32.u32	%f25, %r48;
mul.f32 %f26, %f25, 0f3E490FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r370, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r373, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r375, {high,high};}


	
	{mul.f16x2 %r377,%r361,%r375;
}

	
	{fma.rn.f16x2 %r380,%r358,%r373,%r377;
}

	
	{mul.f16x2 %r384,%r358,%r375;
}

	
	{xor.b32 %r387,%r384,0x80008000;
}

	
	{fma.rn.f16x2 %r389,%r361,%r373,%r387;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r393, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r395, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r397, {low,high};}


	
	{mul.f16x2 %r398,%r395,%r397;
}

	
	{mul.f16x2 %r401,%r370,%r393;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r404, {high,low};}


	
	{fma.rn.f16x2 %r406,%r398,%r404,%r401;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r406;
mov.b32 %r410, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r406;
mov.b32 %r412, {high,high};}


	
	{mul.f16x2 %r414,%r355,%r412;
}

	
	{fma.rn.f16x2 %r417,%r352,%r410,%r414;
}

	
	{mul.f16x2 %r421,%r352,%r412;
}

	
	{xor.b32 %r424,%r421,0x80008000;
}

	
	{fma.rn.f16x2 %r426,%r355,%r410,%r424;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r430, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r432, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r434, {low,high};}


	
	{mul.f16x2 %r435,%r432,%r434;
}

	
	{mul.f16x2 %r438,%r406,%r430;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r406;
mov.b32 %r441, {high,low};}


	
	{fma.rn.f16x2 %r443,%r435,%r441,%r438;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r443;
mov.b32 %r447, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r443;
mov.b32 %r449, {high,high};}


	
	{mul.f16x2 %r451,%r367,%r449;
}

	
	{fma.rn.f16x2 %r454,%r364,%r447,%r451;
}

	
	{mul.f16x2 %r458,%r364,%r449;
}

	
	{xor.b32 %r461,%r458,0x80008000;
}

	
	{fma.rn.f16x2 %r463,%r367,%r447,%r461;
}

	and.b32 %r484, %r5, 3;
add.s32 %r55, %r39, %r484;
barrier.sync 0;
shl.b32 %r485, %r48, 4;
add.s32 %r486, %r485, %r55;
shl.b32 %r487, %r486, 2;
add.s32 %r56, %r316, %r487;
st.shared.u32 [%r56], %r346;
st.shared.u32 [%r56+16], %r380;
st.shared.u32 [%r56+32], %r417;
st.shared.u32 [%r56+48], %r454;
barrier.sync 0;
shl.b32 %r489, %r48, 2;
add.s32 %r490, %r489, %r55;
shl.b32 %r491, %r490, 2;
add.s32 %r57, %r316, %r491;
ld.shared.u32 %r58, [%r57];
ld.shared.u32 %r59, [%r57+128];
ld.shared.u32 %r60, [%r57+256];
ld.shared.u32 %r61, [%r57+384];
barrier.sync 0;
st.shared.u32 [%r56], %r349;
st.shared.u32 [%r56+16], %r389;
st.shared.u32 [%r56+32], %r426;
st.shared.u32 [%r56+48], %r463;
barrier.sync 0;
ld.shared.u32 %r497, [%r57];
ld.shared.u32 %r509, [%r57+128];
ld.shared.u32 %r498, [%r57+256];
ld.shared.u32 %r510, [%r57+384];

	{add.f16x2 %r493,%r58,%r60;
}

	
	{add.f16x2 %r496,%r497,%r498;
}

	
	{sub.f16x2 %r499,%r58,%r60;
}

	
	{sub.f16x2 %r502,%r497,%r498;
}

	
	{add.f16x2 %r505,%r59,%r61;
}

	
	{add.f16x2 %r508,%r509,%r510;
}

	
	{sub.f16x2 %r511,%r59,%r61;
}

	
	{sub.f16x2 %r514,%r509,%r510;
}

	
	{xor.b32 %r517,%r514,0x80008000;
}

	
	{add.f16x2 %r519,%r493,%r505;
}

	
	{add.f16x2 %r522,%r496,%r508;
}

	
	{sub.f16x2 %r525,%r493,%r505;
}

	
	{sub.f16x2 %r528,%r496,%r508;
}

	
	{add.f16x2 %r531,%r499,%r517;
}

	
	{add.f16x2 %r534,%r502,%r511;
}

	
	{sub.f16x2 %r537,%r499,%r517;
}

	
	{sub.f16x2 %r540,%r502,%r511;
}

	shr.u32 %r64, %r32, 4;
cvt.rn.f32.u32	%f38, %r64;
mul.f32 %f39, %f38, 0f3F490FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r543, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r546, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r548, {high,high};}


	
	{mul.f16x2 %r550,%r534,%r548;
}

	
	{fma.rn.f16x2 %r553,%r531,%r546,%r550;
}

	
	{mul.f16x2 %r557,%r531,%r548;
}

	
	{xor.b32 %r560,%r557,0x80008000;
}

	
	{fma.rn.f16x2 %r562,%r534,%r546,%r560;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r566, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r568, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r570, {low,high};}


	
	{mul.f16x2 %r571,%r568,%r570;
}

	
	{mul.f16x2 %r574,%r543,%r566;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r577, {high,low};}


	
	{fma.rn.f16x2 %r579,%r571,%r577,%r574;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r579;
mov.b32 %r583, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r579;
mov.b32 %r585, {high,high};}


	
	{mul.f16x2 %r587,%r528,%r585;
}

	
	{fma.rn.f16x2 %r590,%r525,%r583,%r587;
}

	
	{mul.f16x2 %r594,%r525,%r585;
}

	
	{xor.b32 %r597,%r594,0x80008000;
}

	
	{fma.rn.f16x2 %r599,%r528,%r583,%r597;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r603, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r605, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r607, {low,high};}


	
	{mul.f16x2 %r608,%r605,%r607;
}

	
	{mul.f16x2 %r611,%r579,%r603;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r579;
mov.b32 %r614, {high,low};}


	
	{fma.rn.f16x2 %r616,%r608,%r614,%r611;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r616;
mov.b32 %r620, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r616;
mov.b32 %r622, {high,high};}


	
	{mul.f16x2 %r624,%r540,%r622;
}

	
	{fma.rn.f16x2 %r627,%r537,%r620,%r624;
}

	
	{mul.f16x2 %r631,%r537,%r622;
}

	
	{xor.b32 %r634,%r631,0x80008000;
}

	
	{fma.rn.f16x2 %r636,%r540,%r620,%r634;
}

	and.b32 %r657, %r5, 15;
add.s32 %r71, %r39, %r657;
barrier.sync 0;
shl.b32 %r658, %r64, 6;
add.s32 %r659, %r658, %r71;
shl.b32 %r660, %r659, 2;
add.s32 %r72, %r316, %r660;
st.shared.u32 [%r72], %r519;
st.shared.u32 [%r72+64], %r553;
st.shared.u32 [%r72+128], %r590;
st.shared.u32 [%r72+192], %r627;
barrier.sync 0;
shl.b32 %r662, %r64, 4;
add.s32 %r663, %r662, %r71;
shl.b32 %r664, %r663, 2;
add.s32 %r73, %r316, %r664;
ld.shared.u32 %r74, [%r73];
ld.shared.u32 %r75, [%r73+128];
ld.shared.u32 %r76, [%r73+256];
ld.shared.u32 %r77, [%r73+384];
barrier.sync 0;
st.shared.u32 [%r72], %r522;
st.shared.u32 [%r72+64], %r562;
st.shared.u32 [%r72+128], %r599;
st.shared.u32 [%r72+192], %r636;
barrier.sync 0;

	{add.f16x2 %r666,%r74,%r76;
}

	
	{sub.f16x2 %r672,%r74,%r76;
}

	
	{add.f16x2 %r678,%r75,%r77;
}

	
	{sub.f16x2 %r684,%r75,%r77;
}

	barrier.sync 0;
add.s32 %r692, %r316, %r312;
st.shared.u32 [%r692], %r666;
st.shared.u32 [%r692+128], %r678;
st.shared.u32 [%r692+256], %r672;
st.shared.u32 [%r692+384], %r684;
barrier.sync 0;
shl.b32 %r705, %r5, 3;
add.s32 %r707, %r316, %r705;
ld.shared.u32 %r694, [%r707];
ld.shared.u32 %r695, [%r707+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r694;
mov.b32 {blow,bhigh}, %r695;
mov.b32 %r693, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r694;
mov.b32 {blow,bhigh}, %r695;
mov.b32 %r696, {ahigh,bhigh};}


	ld.shared.u32 %r700, [%r707+256];
ld.shared.u32 %r701, [%r707+260];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r700;
mov.b32 {blow,bhigh}, %r701;
mov.b32 %r699, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r700;
mov.b32 {blow,bhigh}, %r701;
mov.b32 %r702, {ahigh,bhigh};}


	setp.eq.s16	%p3, %rs1, 0;
selp.b32	%r708, 64, 65, %p3;
shl.b32 %r709, %r2, 1;
add.s32 %r710, %r709, 1;
mad.lo.s32 %r711, %r709, %r708, %r5;
cvta.to.global.u64 %rd12, %rd7;
mul.wide.u32 %rd13, %r711, 4;
add.s64 %rd4, %rd12, %rd13;
mad.lo.s32 %r712, %r710, %r708, %r5;
mul.wide.u32 %rd14, %r712, 4;
add.s64 %rd5, %rd12, %rd14;
@%p1 bra BB6_6;
bra.uni BB6_5;

BB6_6:
st.global.u32 [%rd4], %r693;
st.global.u32 [%rd4+128], %r699;
bra.uni BB6_7;

BB6_5:
shl.b32 %r713, %r3, 1;
add.s32 %r714, %r713, -1;
st.global.u32 [%rd4], %r693;
st.global.u32 [%rd4+128], %r699;
setp.ge.u32	%p5, %r714, %r1;
@%p5 bra BB6_8;

BB6_7:
st.global.u32 [%rd5], %r696;
st.global.u32 [%rd5+128], %r702;

BB6_8:
ret;
}


.weak .entry _Z14vector_fft_c2rILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 64, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<10>;
.reg .f32 %f<41>;
.reg .b32 %r<749>;
.reg .f64 %fd<2>;
.reg .b64 %rd<15>;


ld.param.u32 %r1, [_Z14vector_fft_c2rILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd6, [_Z14vector_fft_c2rILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
mul.lo.s32 %r107, %r2, 258;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p1, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r108, %r107, %r5;
mov.u32 %r109, 64;
sub.s32 %r110, %r109, %r5;
add.s32 %r111, %r107, %r110;
cvta.to.global.u64 %rd8, %rd6;
mul.wide.u32 %rd9, %r108, 4;
add.s64 %rd1, %rd8, %rd9;
mul.wide.u32 %rd10, %r111, 4;
add.s64 %rd2, %rd8, %rd10;
add.s32 %r112, %r111, 129;
mul.wide.u32 %rd11, %r112, 4;
add.s64 %rd3, %rd8, %rd11;
@%p1 bra BB7_3;
bra.uni BB7_1;

BB7_3:
ld.global.u32 %r748, [%rd1];
ld.global.u32 %r120, [%rd2];
xor.b32 %r742, %r120, -2147483648;
ld.global.u32 %r746, [%rd1+256];
ld.global.u32 %r121, [%rd2+256];
xor.b32 %r744, %r121, -2147483648;
ld.global.u32 %r747, [%rd1+516];
ld.global.u32 %r122, [%rd3];
xor.b32 %r741, %r122, -2147483648;
ld.global.u32 %r745, [%rd1+772];
ld.global.u32 %r123, [%rd3+256];
xor.b32 %r743, %r123, -2147483648;
bra.uni BB7_4;

BB7_1:
shl.b32 %r114, %r3, 1;
add.s32 %r115, %r114, -1;
ld.global.u32 %r748, [%rd1];
ld.global.u32 %r116, [%rd2];
xor.b32 %r742, %r116, -2147483648;
ld.global.u32 %r746, [%rd1+256];
ld.global.u32 %r117, [%rd2+256];
xor.b32 %r744, %r117, -2147483648;
setp.ge.u32	%p2, %r115, %r1;
@%p2 bra BB7_4;

ld.global.u32 %r747, [%rd1+516];
ld.global.u32 %r118, [%rd3];
xor.b32 %r741, %r118, -2147483648;
ld.global.u32 %r745, [%rd1+772];
ld.global.u32 %r119, [%rd3+256];
xor.b32 %r743, %r119, -2147483648;

BB7_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r748;
mov.b32 {blow,bhigh}, %r747;
mov.b32 %r124, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r748;
mov.b32 {blow,bhigh}, %r747;
mov.b32 %r127, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r746;
mov.b32 {blow,bhigh}, %r745;
mov.b32 %r130, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r746;
mov.b32 {blow,bhigh}, %r745;
mov.b32 %r133, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r744;
mov.b32 {blow,bhigh}, %r743;
mov.b32 %r136, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r744;
mov.b32 {blow,bhigh}, %r743;
mov.b32 %r139, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r742;
mov.b32 {blow,bhigh}, %r741;
mov.b32 %r142, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r742;
mov.b32 {blow,bhigh}, %r741;
mov.b32 %r145, {ahigh,bhigh};}


	
	{add.f16x2 %r148,%r124,%r136;
}

	
	{add.f16x2 %r151,%r127,%r139;
}

	
	{sub.f16x2 %r154,%r124,%r136;
}

	
	{sub.f16x2 %r157,%r127,%r139;
}

	
	{add.f16x2 %r160,%r130,%r142;
}

	
	{add.f16x2 %r163,%r133,%r145;
}

	
	{sub.f16x2 %r166,%r130,%r142;
}

	
	{sub.f16x2 %r169,%r133,%r145;
}

	
	{xor.b32 %r172,%r169,0x80008000;
}

	
	{add.f16x2 %r174,%r148,%r160;
}

	
	{add.f16x2 %r177,%r151,%r163;
}

	
	{sub.f16x2 %r180,%r148,%r160;
}

	
	{sub.f16x2 %r183,%r151,%r163;
}

	
	{add.f16x2 %r186,%r154,%r172;
}

	
	{add.f16x2 %r189,%r157,%r166;
}

	
	{sub.f16x2 %r192,%r154,%r172;
}

	
	{sub.f16x2 %r195,%r157,%r166;
}

	and.b32 %r32, %r5, 63;
cvt.rn.f32.u32	%f12, %r32;
mul.f32 %f13, %f12, 0f3CC90FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r198, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r201, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r203, {high,high};}


	
	{mul.f16x2 %r205,%r189,%r203;
}

	
	{fma.rn.f16x2 %r208,%r186,%r201,%r205;
}

	
	{mul.f16x2 %r212,%r186,%r203;
}

	
	{xor.b32 %r215,%r212,0x80008000;
}

	
	{fma.rn.f16x2 %r217,%r189,%r201,%r215;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r221, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r223, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r225, {low,high};}


	
	{mul.f16x2 %r226,%r223,%r225;
}

	
	{mul.f16x2 %r229,%r198,%r221;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r232, {high,low};}


	
	{fma.rn.f16x2 %r234,%r226,%r232,%r229;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r234;
mov.b32 %r238, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r234;
mov.b32 %r240, {high,high};}


	
	{mul.f16x2 %r242,%r183,%r240;
}

	
	{fma.rn.f16x2 %r245,%r180,%r238,%r242;
}

	
	{mul.f16x2 %r249,%r180,%r240;
}

	
	{xor.b32 %r252,%r249,0x80008000;
}

	
	{fma.rn.f16x2 %r254,%r183,%r238,%r252;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r258, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r198;
mov.b32 %r260, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r262, {low,high};}


	
	{mul.f16x2 %r263,%r260,%r262;
}

	
	{mul.f16x2 %r266,%r234,%r258;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r234;
mov.b32 %r269, {high,low};}


	
	{fma.rn.f16x2 %r271,%r263,%r269,%r266;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r271;
mov.b32 %r275, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r271;
mov.b32 %r277, {high,high};}


	
	{mul.f16x2 %r279,%r195,%r277;
}

	
	{fma.rn.f16x2 %r282,%r192,%r275,%r279;
}

	
	{mul.f16x2 %r286,%r192,%r277;
}

	
	{xor.b32 %r289,%r286,0x80008000;
}

	
	{fma.rn.f16x2 %r291,%r195,%r275,%r289;
}

	shl.b32 %r312, %r5, 2;
and.b32 %r39, %r312, -256;
barrier.sync 0;
shl.b32 %r313, %r32, 2;
add.s32 %r314, %r313, %r39;
shl.b32 %r315, %r314, 2;
mov.u32 %r316, smem_full;
add.s32 %r40, %r316, %r315;
st.shared.u32 [%r40], %r174;
st.shared.u32 [%r40+4], %r208;
st.shared.u32 [%r40+8], %r245;
st.shared.u32 [%r40+12], %r282;
barrier.sync 0;
add.s32 %r317, %r32, %r39;
shl.b32 %r318, %r317, 2;
add.s32 %r41, %r316, %r318;
ld.shared.u32 %r42, [%r41];
ld.shared.u32 %r43, [%r41+256];
ld.shared.u32 %r44, [%r41+512];
ld.shared.u32 %r45, [%r41+768];
barrier.sync 0;
st.shared.u32 [%r40], %r177;
st.shared.u32 [%r40+4], %r217;
st.shared.u32 [%r40+8], %r254;
st.shared.u32 [%r40+12], %r291;
barrier.sync 0;
ld.shared.u32 %r324, [%r41];
ld.shared.u32 %r336, [%r41+256];
ld.shared.u32 %r325, [%r41+512];
ld.shared.u32 %r337, [%r41+768];

	{add.f16x2 %r320,%r42,%r44;
}

	
	{add.f16x2 %r323,%r324,%r325;
}

	
	{sub.f16x2 %r326,%r42,%r44;
}

	
	{sub.f16x2 %r329,%r324,%r325;
}

	
	{add.f16x2 %r332,%r43,%r45;
}

	
	{add.f16x2 %r335,%r336,%r337;
}

	
	{sub.f16x2 %r338,%r43,%r45;
}

	
	{sub.f16x2 %r341,%r336,%r337;
}

	
	{xor.b32 %r344,%r341,0x80008000;
}

	
	{add.f16x2 %r346,%r320,%r332;
}

	
	{add.f16x2 %r349,%r323,%r335;
}

	
	{sub.f16x2 %r352,%r320,%r332;
}

	
	{sub.f16x2 %r355,%r323,%r335;
}

	
	{add.f16x2 %r358,%r326,%r344;
}

	
	{add.f16x2 %r361,%r329,%r338;
}

	
	{sub.f16x2 %r364,%r326,%r344;
}

	
	{sub.f16x2 %r367,%r329,%r338;
}

	and.b32 %r48, %r5, 60;
bfe.u32 %r484, %r5, 2, 4;
cvt.rn.f32.u32	%f25, %r484;
mul.f32 %f26, %f25, 0f3DC90FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r370, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r373, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r375, {high,high};}


	
	{mul.f16x2 %r377,%r361,%r375;
}

	
	{fma.rn.f16x2 %r380,%r358,%r373,%r377;
}

	
	{mul.f16x2 %r384,%r358,%r375;
}

	
	{xor.b32 %r387,%r384,0x80008000;
}

	
	{fma.rn.f16x2 %r389,%r361,%r373,%r387;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r393, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r395, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r397, {low,high};}


	
	{mul.f16x2 %r398,%r395,%r397;
}

	
	{mul.f16x2 %r401,%r370,%r393;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r404, {high,low};}


	
	{fma.rn.f16x2 %r406,%r398,%r404,%r401;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r406;
mov.b32 %r410, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r406;
mov.b32 %r412, {high,high};}


	
	{mul.f16x2 %r414,%r355,%r412;
}

	
	{fma.rn.f16x2 %r417,%r352,%r410,%r414;
}

	
	{mul.f16x2 %r421,%r352,%r412;
}

	
	{xor.b32 %r424,%r421,0x80008000;
}

	
	{fma.rn.f16x2 %r426,%r355,%r410,%r424;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r430, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r370;
mov.b32 %r432, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r434, {low,high};}


	
	{mul.f16x2 %r435,%r432,%r434;
}

	
	{mul.f16x2 %r438,%r406,%r430;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r406;
mov.b32 %r441, {high,low};}


	
	{fma.rn.f16x2 %r443,%r435,%r441,%r438;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r443;
mov.b32 %r447, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r443;
mov.b32 %r449, {high,high};}


	
	{mul.f16x2 %r451,%r367,%r449;
}

	
	{fma.rn.f16x2 %r454,%r364,%r447,%r451;
}

	
	{mul.f16x2 %r458,%r364,%r449;
}

	
	{xor.b32 %r461,%r458,0x80008000;
}

	
	{fma.rn.f16x2 %r463,%r367,%r447,%r461;
}

	and.b32 %r485, %r5, 3;
add.s32 %r55, %r39, %r485;
barrier.sync 0;
shl.b32 %r486, %r48, 2;
add.s32 %r487, %r486, %r55;
shl.b32 %r488, %r487, 2;
add.s32 %r56, %r316, %r488;
st.shared.u32 [%r56], %r346;
st.shared.u32 [%r56+16], %r380;
st.shared.u32 [%r56+32], %r417;
st.shared.u32 [%r56+48], %r454;
barrier.sync 0;
add.s32 %r490, %r48, %r55;
shl.b32 %r491, %r490, 2;
add.s32 %r57, %r316, %r491;
ld.shared.u32 %r58, [%r57];
ld.shared.u32 %r59, [%r57+256];
ld.shared.u32 %r60, [%r57+512];
ld.shared.u32 %r61, [%r57+768];
barrier.sync 0;
st.shared.u32 [%r56], %r349;
st.shared.u32 [%r56+16], %r389;
st.shared.u32 [%r56+32], %r426;
st.shared.u32 [%r56+48], %r463;
barrier.sync 0;
ld.shared.u32 %r497, [%r57];
ld.shared.u32 %r509, [%r57+256];
ld.shared.u32 %r498, [%r57+512];
ld.shared.u32 %r510, [%r57+768];

	{add.f16x2 %r493,%r58,%r60;
}

	
	{add.f16x2 %r496,%r497,%r498;
}

	
	{sub.f16x2 %r499,%r58,%r60;
}

	
	{sub.f16x2 %r502,%r497,%r498;
}

	
	{add.f16x2 %r505,%r59,%r61;
}

	
	{add.f16x2 %r508,%r509,%r510;
}

	
	{sub.f16x2 %r511,%r59,%r61;
}

	
	{sub.f16x2 %r514,%r509,%r510;
}

	
	{xor.b32 %r517,%r514,0x80008000;
}

	
	{add.f16x2 %r519,%r493,%r505;
}

	
	{add.f16x2 %r522,%r496,%r508;
}

	
	{sub.f16x2 %r525,%r493,%r505;
}

	
	{sub.f16x2 %r528,%r496,%r508;
}

	
	{add.f16x2 %r531,%r499,%r517;
}

	
	{add.f16x2 %r534,%r502,%r511;
}

	
	{sub.f16x2 %r537,%r499,%r517;
}

	
	{sub.f16x2 %r540,%r502,%r511;
}

	and.b32 %r64, %r5, 48;
bfe.u32 %r657, %r5, 4, 2;
cvt.rn.f32.u32	%f38, %r657;
mul.f32 %f39, %f38, 0f3EC90FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r543, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r546, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r548, {high,high};}


	
	{mul.f16x2 %r550,%r534,%r548;
}

	
	{fma.rn.f16x2 %r553,%r531,%r546,%r550;
}

	
	{mul.f16x2 %r557,%r531,%r548;
}

	
	{xor.b32 %r560,%r557,0x80008000;
}

	
	{fma.rn.f16x2 %r562,%r534,%r546,%r560;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r566, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r568, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r570, {low,high};}


	
	{mul.f16x2 %r571,%r568,%r570;
}

	
	{mul.f16x2 %r574,%r543,%r566;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r577, {high,low};}


	
	{fma.rn.f16x2 %r579,%r571,%r577,%r574;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r579;
mov.b32 %r583, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r579;
mov.b32 %r585, {high,high};}


	
	{mul.f16x2 %r587,%r528,%r585;
}

	
	{fma.rn.f16x2 %r590,%r525,%r583,%r587;
}

	
	{mul.f16x2 %r594,%r525,%r585;
}

	
	{xor.b32 %r597,%r594,0x80008000;
}

	
	{fma.rn.f16x2 %r599,%r528,%r583,%r597;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r603, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r543;
mov.b32 %r605, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r607, {low,high};}


	
	{mul.f16x2 %r608,%r605,%r607;
}

	
	{mul.f16x2 %r611,%r579,%r603;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r579;
mov.b32 %r614, {high,low};}


	
	{fma.rn.f16x2 %r616,%r608,%r614,%r611;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r616;
mov.b32 %r620, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r616;
mov.b32 %r622, {high,high};}


	
	{mul.f16x2 %r624,%r540,%r622;
}

	
	{fma.rn.f16x2 %r627,%r537,%r620,%r624;
}

	
	{mul.f16x2 %r631,%r537,%r622;
}

	
	{xor.b32 %r634,%r631,0x80008000;
}

	
	{fma.rn.f16x2 %r636,%r540,%r620,%r634;
}

	and.b32 %r658, %r5, 15;
add.s32 %r71, %r39, %r658;
barrier.sync 0;
shl.b32 %r659, %r64, 2;
add.s32 %r660, %r659, %r71;
shl.b32 %r661, %r660, 2;
add.s32 %r72, %r316, %r661;
st.shared.u32 [%r72], %r519;
st.shared.u32 [%r72+64], %r553;
st.shared.u32 [%r72+128], %r590;
st.shared.u32 [%r72+192], %r627;
barrier.sync 0;
add.s32 %r663, %r64, %r71;
shl.b32 %r664, %r663, 2;
add.s32 %r73, %r316, %r664;
ld.shared.u32 %r74, [%r73];
ld.shared.u32 %r75, [%r73+256];
ld.shared.u32 %r76, [%r73+512];
ld.shared.u32 %r77, [%r73+768];
barrier.sync 0;
st.shared.u32 [%r72], %r522;
st.shared.u32 [%r72+64], %r562;
st.shared.u32 [%r72+128], %r599;
st.shared.u32 [%r72+192], %r636;
barrier.sync 0;
ld.shared.u32 %r688, [%r73+256];
ld.shared.u32 %r689, [%r73+768];

	{add.f16x2 %r666,%r74,%r76;
}

	
	{sub.f16x2 %r672,%r74,%r76;
}

	
	{add.f16x2 %r678,%r75,%r77;
}

	
	{sub.f16x2 %r687,%r688,%r689;
}

	
	{xor.b32 %r690,%r687,0x80008000;
}

	
	{add.f16x2 %r692,%r666,%r678;
}

	
	{sub.f16x2 %r698,%r666,%r678;
}

	
	{add.f16x2 %r704,%r672,%r690;
}

	
	{sub.f16x2 %r710,%r672,%r690;
}

	barrier.sync 0;
add.s32 %r718, %r316, %r312;
st.shared.u32 [%r718], %r692;
st.shared.u32 [%r718+256], %r704;
st.shared.u32 [%r718+512], %r698;
st.shared.u32 [%r718+768], %r710;
barrier.sync 0;
shl.b32 %r731, %r5, 3;
add.s32 %r733, %r316, %r731;
ld.shared.u32 %r720, [%r733];
ld.shared.u32 %r721, [%r733+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r720;
mov.b32 {blow,bhigh}, %r721;
mov.b32 %r719, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r720;
mov.b32 {blow,bhigh}, %r721;
mov.b32 %r722, {ahigh,bhigh};}


	ld.shared.u32 %r726, [%r733+512];
ld.shared.u32 %r727, [%r733+516];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r726;
mov.b32 {blow,bhigh}, %r727;
mov.b32 %r725, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r726;
mov.b32 {blow,bhigh}, %r727;
mov.b32 %r728, {ahigh,bhigh};}


	setp.eq.s16	%p3, %rs1, 0;
selp.b32	%r734, 128, 129, %p3;
shl.b32 %r735, %r2, 1;
add.s32 %r736, %r735, 1;
mad.lo.s32 %r737, %r735, %r734, %r5;
cvta.to.global.u64 %rd12, %rd7;
mul.wide.u32 %rd13, %r737, 4;
add.s64 %rd4, %rd12, %rd13;
mad.lo.s32 %r738, %r736, %r734, %r5;
mul.wide.u32 %rd14, %r738, 4;
add.s64 %rd5, %rd12, %rd14;
@%p1 bra BB7_6;
bra.uni BB7_5;

BB7_6:
st.global.u32 [%rd4], %r719;
st.global.u32 [%rd4+256], %r725;
bra.uni BB7_7;

BB7_5:
shl.b32 %r739, %r3, 1;
add.s32 %r740, %r739, -1;
st.global.u32 [%rd4], %r719;
st.global.u32 [%rd4+256], %r725;
setp.ge.u32	%p5, %r740, %r1;
@%p5 bra BB7_8;

BB7_7:
st.global.u32 [%rd5], %r722;
st.global.u32 [%rd5+256], %r728;

BB7_8:
ret;
}


.weak .entry _Z14vector_fft_c2rILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 128, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<10>;
.reg .f32 %f<54>;
.reg .b32 %r<912>;
.reg .f64 %fd<2>;
.reg .b64 %rd<15>;


ld.param.u32 %r1, [_Z14vector_fft_c2rILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd6, [_Z14vector_fft_c2rILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
mul.lo.s32 %r123, %r2, 514;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p1, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r124, %r123, %r5;
mov.u32 %r125, 128;
sub.s32 %r126, %r125, %r5;
add.s32 %r127, %r123, %r126;
cvta.to.global.u64 %rd8, %rd6;
mul.wide.u32 %rd9, %r124, 4;
add.s64 %rd1, %rd8, %rd9;
mul.wide.u32 %rd10, %r127, 4;
add.s64 %rd2, %rd8, %rd10;
add.s32 %r128, %r127, 257;
mul.wide.u32 %rd11, %r128, 4;
add.s64 %rd3, %rd8, %rd11;
@%p1 bra BB8_3;
bra.uni BB8_1;

BB8_3:
ld.global.u32 %r911, [%rd1];
ld.global.u32 %r136, [%rd2];
xor.b32 %r905, %r136, -2147483648;
ld.global.u32 %r909, [%rd1+512];
ld.global.u32 %r137, [%rd2+512];
xor.b32 %r907, %r137, -2147483648;
ld.global.u32 %r910, [%rd1+1028];
ld.global.u32 %r138, [%rd3];
xor.b32 %r904, %r138, -2147483648;
ld.global.u32 %r908, [%rd1+1540];
ld.global.u32 %r139, [%rd3+512];
xor.b32 %r906, %r139, -2147483648;
bra.uni BB8_4;

BB8_1:
shl.b32 %r130, %r3, 1;
add.s32 %r131, %r130, -1;
ld.global.u32 %r911, [%rd1];
ld.global.u32 %r132, [%rd2];
xor.b32 %r905, %r132, -2147483648;
ld.global.u32 %r909, [%rd1+512];
ld.global.u32 %r133, [%rd2+512];
xor.b32 %r907, %r133, -2147483648;
setp.ge.u32	%p2, %r131, %r1;
@%p2 bra BB8_4;

ld.global.u32 %r910, [%rd1+1028];
ld.global.u32 %r134, [%rd3];
xor.b32 %r904, %r134, -2147483648;
ld.global.u32 %r908, [%rd1+1540];
ld.global.u32 %r135, [%rd3+512];
xor.b32 %r906, %r135, -2147483648;

BB8_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r911;
mov.b32 {blow,bhigh}, %r910;
mov.b32 %r140, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r911;
mov.b32 {blow,bhigh}, %r910;
mov.b32 %r143, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r909;
mov.b32 {blow,bhigh}, %r908;
mov.b32 %r146, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r909;
mov.b32 {blow,bhigh}, %r908;
mov.b32 %r149, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r907;
mov.b32 {blow,bhigh}, %r906;
mov.b32 %r152, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r907;
mov.b32 {blow,bhigh}, %r906;
mov.b32 %r155, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r905;
mov.b32 {blow,bhigh}, %r904;
mov.b32 %r158, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r905;
mov.b32 {blow,bhigh}, %r904;
mov.b32 %r161, {ahigh,bhigh};}


	
	{add.f16x2 %r164,%r140,%r152;
}

	
	{add.f16x2 %r167,%r143,%r155;
}

	
	{sub.f16x2 %r170,%r140,%r152;
}

	
	{sub.f16x2 %r173,%r143,%r155;
}

	
	{add.f16x2 %r176,%r146,%r158;
}

	
	{add.f16x2 %r179,%r149,%r161;
}

	
	{sub.f16x2 %r182,%r146,%r158;
}

	
	{sub.f16x2 %r185,%r149,%r161;
}

	
	{xor.b32 %r188,%r185,0x80008000;
}

	
	{add.f16x2 %r190,%r164,%r176;
}

	
	{add.f16x2 %r193,%r167,%r179;
}

	
	{sub.f16x2 %r196,%r164,%r176;
}

	
	{sub.f16x2 %r199,%r167,%r179;
}

	
	{add.f16x2 %r202,%r170,%r188;
}

	
	{add.f16x2 %r205,%r173,%r182;
}

	
	{sub.f16x2 %r208,%r170,%r188;
}

	
	{sub.f16x2 %r211,%r173,%r182;
}

	and.b32 %r32, %r5, 127;
cvt.rn.f32.u32	%f12, %r32;
mul.f32 %f13, %f12, 0f3C490FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r214, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r214;
mov.b32 %r217, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r214;
mov.b32 %r219, {high,high};}


	
	{mul.f16x2 %r221,%r205,%r219;
}

	
	{fma.rn.f16x2 %r224,%r202,%r217,%r221;
}

	
	{mul.f16x2 %r228,%r202,%r219;
}

	
	{xor.b32 %r231,%r228,0x80008000;
}

	
	{fma.rn.f16x2 %r233,%r205,%r217,%r231;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r214;
mov.b32 %r237, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r214;
mov.b32 %r239, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r241, {low,high};}


	
	{mul.f16x2 %r242,%r239,%r241;
}

	
	{mul.f16x2 %r245,%r214,%r237;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r214;
mov.b32 %r248, {high,low};}


	
	{fma.rn.f16x2 %r250,%r242,%r248,%r245;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r250;
mov.b32 %r254, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r250;
mov.b32 %r256, {high,high};}


	
	{mul.f16x2 %r258,%r199,%r256;
}

	
	{fma.rn.f16x2 %r261,%r196,%r254,%r258;
}

	
	{mul.f16x2 %r265,%r196,%r256;
}

	
	{xor.b32 %r268,%r265,0x80008000;
}

	
	{fma.rn.f16x2 %r270,%r199,%r254,%r268;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r214;
mov.b32 %r274, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r214;
mov.b32 %r276, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r278, {low,high};}


	
	{mul.f16x2 %r279,%r276,%r278;
}

	
	{mul.f16x2 %r282,%r250,%r274;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r250;
mov.b32 %r285, {high,low};}


	
	{fma.rn.f16x2 %r287,%r279,%r285,%r282;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r287;
mov.b32 %r291, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r287;
mov.b32 %r293, {high,high};}


	
	{mul.f16x2 %r295,%r211,%r293;
}

	
	{fma.rn.f16x2 %r298,%r208,%r291,%r295;
}

	
	{mul.f16x2 %r302,%r208,%r293;
}

	
	{xor.b32 %r305,%r302,0x80008000;
}

	
	{fma.rn.f16x2 %r307,%r211,%r291,%r305;
}

	shl.b32 %r328, %r5, 2;
and.b32 %r39, %r328, -512;
barrier.sync 0;
shl.b32 %r329, %r32, 2;
add.s32 %r330, %r329, %r39;
shl.b32 %r331, %r330, 2;
mov.u32 %r332, smem_full;
add.s32 %r40, %r332, %r331;
st.shared.u32 [%r40], %r190;
st.shared.u32 [%r40+4], %r224;
st.shared.u32 [%r40+8], %r261;
st.shared.u32 [%r40+12], %r298;
barrier.sync 0;
add.s32 %r333, %r32, %r39;
shl.b32 %r334, %r333, 2;
add.s32 %r41, %r332, %r334;
ld.shared.u32 %r42, [%r41];
ld.shared.u32 %r43, [%r41+512];
ld.shared.u32 %r44, [%r41+1024];
ld.shared.u32 %r45, [%r41+1536];
barrier.sync 0;
st.shared.u32 [%r40], %r193;
st.shared.u32 [%r40+4], %r233;
st.shared.u32 [%r40+8], %r270;
st.shared.u32 [%r40+12], %r307;
barrier.sync 0;
ld.shared.u32 %r340, [%r41];
ld.shared.u32 %r352, [%r41+512];
ld.shared.u32 %r341, [%r41+1024];
ld.shared.u32 %r353, [%r41+1536];

	{add.f16x2 %r336,%r42,%r44;
}

	
	{add.f16x2 %r339,%r340,%r341;
}

	
	{sub.f16x2 %r342,%r42,%r44;
}

	
	{sub.f16x2 %r345,%r340,%r341;
}

	
	{add.f16x2 %r348,%r43,%r45;
}

	
	{add.f16x2 %r351,%r352,%r353;
}

	
	{sub.f16x2 %r354,%r43,%r45;
}

	
	{sub.f16x2 %r357,%r352,%r353;
}

	
	{xor.b32 %r360,%r357,0x80008000;
}

	
	{add.f16x2 %r362,%r336,%r348;
}

	
	{add.f16x2 %r365,%r339,%r351;
}

	
	{sub.f16x2 %r368,%r336,%r348;
}

	
	{sub.f16x2 %r371,%r339,%r351;
}

	
	{add.f16x2 %r374,%r342,%r360;
}

	
	{add.f16x2 %r377,%r345,%r354;
}

	
	{sub.f16x2 %r380,%r342,%r360;
}

	
	{sub.f16x2 %r383,%r345,%r354;
}

	and.b32 %r48, %r5, 124;
bfe.u32 %r500, %r5, 2, 5;
cvt.rn.f32.u32	%f25, %r500;
mul.f32 %f26, %f25, 0f3D490FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r386, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r386;
mov.b32 %r389, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r386;
mov.b32 %r391, {high,high};}


	
	{mul.f16x2 %r393,%r377,%r391;
}

	
	{fma.rn.f16x2 %r396,%r374,%r389,%r393;
}

	
	{mul.f16x2 %r400,%r374,%r391;
}

	
	{xor.b32 %r403,%r400,0x80008000;
}

	
	{fma.rn.f16x2 %r405,%r377,%r389,%r403;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r386;
mov.b32 %r409, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r386;
mov.b32 %r411, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r413, {low,high};}


	
	{mul.f16x2 %r414,%r411,%r413;
}

	
	{mul.f16x2 %r417,%r386,%r409;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r386;
mov.b32 %r420, {high,low};}


	
	{fma.rn.f16x2 %r422,%r414,%r420,%r417;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r422;
mov.b32 %r426, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r422;
mov.b32 %r428, {high,high};}


	
	{mul.f16x2 %r430,%r371,%r428;
}

	
	{fma.rn.f16x2 %r433,%r368,%r426,%r430;
}

	
	{mul.f16x2 %r437,%r368,%r428;
}

	
	{xor.b32 %r440,%r437,0x80008000;
}

	
	{fma.rn.f16x2 %r442,%r371,%r426,%r440;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r386;
mov.b32 %r446, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r386;
mov.b32 %r448, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r450, {low,high};}


	
	{mul.f16x2 %r451,%r448,%r450;
}

	
	{mul.f16x2 %r454,%r422,%r446;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r422;
mov.b32 %r457, {high,low};}


	
	{fma.rn.f16x2 %r459,%r451,%r457,%r454;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r459;
mov.b32 %r463, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r459;
mov.b32 %r465, {high,high};}


	
	{mul.f16x2 %r467,%r383,%r465;
}

	
	{fma.rn.f16x2 %r470,%r380,%r463,%r467;
}

	
	{mul.f16x2 %r474,%r380,%r465;
}

	
	{xor.b32 %r477,%r474,0x80008000;
}

	
	{fma.rn.f16x2 %r479,%r383,%r463,%r477;
}

	and.b32 %r501, %r5, 3;
add.s32 %r55, %r39, %r501;
barrier.sync 0;
shl.b32 %r502, %r48, 2;
add.s32 %r503, %r502, %r55;
shl.b32 %r504, %r503, 2;
add.s32 %r56, %r332, %r504;
st.shared.u32 [%r56], %r362;
st.shared.u32 [%r56+16], %r396;
st.shared.u32 [%r56+32], %r433;
st.shared.u32 [%r56+48], %r470;
barrier.sync 0;
add.s32 %r506, %r48, %r55;
shl.b32 %r507, %r506, 2;
add.s32 %r57, %r332, %r507;
ld.shared.u32 %r58, [%r57];
ld.shared.u32 %r59, [%r57+512];
ld.shared.u32 %r60, [%r57+1024];
ld.shared.u32 %r61, [%r57+1536];
barrier.sync 0;
st.shared.u32 [%r56], %r365;
st.shared.u32 [%r56+16], %r405;
st.shared.u32 [%r56+32], %r442;
st.shared.u32 [%r56+48], %r479;
barrier.sync 0;
ld.shared.u32 %r513, [%r57];
ld.shared.u32 %r525, [%r57+512];
ld.shared.u32 %r514, [%r57+1024];
ld.shared.u32 %r526, [%r57+1536];

	{add.f16x2 %r509,%r58,%r60;
}

	
	{add.f16x2 %r512,%r513,%r514;
}

	
	{sub.f16x2 %r515,%r58,%r60;
}

	
	{sub.f16x2 %r518,%r513,%r514;
}

	
	{add.f16x2 %r521,%r59,%r61;
}

	
	{add.f16x2 %r524,%r525,%r526;
}

	
	{sub.f16x2 %r527,%r59,%r61;
}

	
	{sub.f16x2 %r530,%r525,%r526;
}

	
	{xor.b32 %r533,%r530,0x80008000;
}

	
	{add.f16x2 %r535,%r509,%r521;
}

	
	{add.f16x2 %r538,%r512,%r524;
}

	
	{sub.f16x2 %r541,%r509,%r521;
}

	
	{sub.f16x2 %r544,%r512,%r524;
}

	
	{add.f16x2 %r547,%r515,%r533;
}

	
	{add.f16x2 %r550,%r518,%r527;
}

	
	{sub.f16x2 %r553,%r515,%r533;
}

	
	{sub.f16x2 %r556,%r518,%r527;
}

	and.b32 %r64, %r5, 112;
bfe.u32 %r673, %r5, 4, 3;
cvt.rn.f32.u32	%f38, %r673;
mul.f32 %f39, %f38, 0f3E490FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r559, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r559;
mov.b32 %r562, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r559;
mov.b32 %r564, {high,high};}


	
	{mul.f16x2 %r566,%r550,%r564;
}

	
	{fma.rn.f16x2 %r569,%r547,%r562,%r566;
}

	
	{mul.f16x2 %r573,%r547,%r564;
}

	
	{xor.b32 %r576,%r573,0x80008000;
}

	
	{fma.rn.f16x2 %r578,%r550,%r562,%r576;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r559;
mov.b32 %r582, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r559;
mov.b32 %r584, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r586, {low,high};}


	
	{mul.f16x2 %r587,%r584,%r586;
}

	
	{mul.f16x2 %r590,%r559,%r582;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r559;
mov.b32 %r593, {high,low};}


	
	{fma.rn.f16x2 %r595,%r587,%r593,%r590;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r595;
mov.b32 %r599, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r595;
mov.b32 %r601, {high,high};}


	
	{mul.f16x2 %r603,%r544,%r601;
}

	
	{fma.rn.f16x2 %r606,%r541,%r599,%r603;
}

	
	{mul.f16x2 %r610,%r541,%r601;
}

	
	{xor.b32 %r613,%r610,0x80008000;
}

	
	{fma.rn.f16x2 %r615,%r544,%r599,%r613;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r559;
mov.b32 %r619, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r559;
mov.b32 %r621, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r623, {low,high};}


	
	{mul.f16x2 %r624,%r621,%r623;
}

	
	{mul.f16x2 %r627,%r595,%r619;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r595;
mov.b32 %r630, {high,low};}


	
	{fma.rn.f16x2 %r632,%r624,%r630,%r627;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r632;
mov.b32 %r636, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r632;
mov.b32 %r638, {high,high};}


	
	{mul.f16x2 %r640,%r556,%r638;
}

	
	{fma.rn.f16x2 %r643,%r553,%r636,%r640;
}

	
	{mul.f16x2 %r647,%r553,%r638;
}

	
	{xor.b32 %r650,%r647,0x80008000;
}

	
	{fma.rn.f16x2 %r652,%r556,%r636,%r650;
}

	and.b32 %r674, %r5, 15;
add.s32 %r71, %r39, %r674;
barrier.sync 0;
shl.b32 %r675, %r64, 2;
add.s32 %r676, %r675, %r71;
shl.b32 %r677, %r676, 2;
add.s32 %r72, %r332, %r677;
st.shared.u32 [%r72], %r535;
st.shared.u32 [%r72+64], %r569;
st.shared.u32 [%r72+128], %r606;
st.shared.u32 [%r72+192], %r643;
barrier.sync 0;
add.s32 %r679, %r64, %r71;
shl.b32 %r680, %r679, 2;
add.s32 %r73, %r332, %r680;
ld.shared.u32 %r74, [%r73];
ld.shared.u32 %r75, [%r73+512];
ld.shared.u32 %r76, [%r73+1024];
ld.shared.u32 %r77, [%r73+1536];
barrier.sync 0;
st.shared.u32 [%r72], %r538;
st.shared.u32 [%r72+64], %r578;
st.shared.u32 [%r72+128], %r615;
st.shared.u32 [%r72+192], %r652;
barrier.sync 0;
ld.shared.u32 %r686, [%r73];
ld.shared.u32 %r698, [%r73+512];
ld.shared.u32 %r687, [%r73+1024];
ld.shared.u32 %r699, [%r73+1536];

	{add.f16x2 %r682,%r74,%r76;
}

	
	{add.f16x2 %r685,%r686,%r687;
}

	
	{sub.f16x2 %r688,%r74,%r76;
}

	
	{sub.f16x2 %r691,%r686,%r687;
}

	
	{add.f16x2 %r694,%r75,%r77;
}

	
	{add.f16x2 %r697,%r698,%r699;
}

	
	{sub.f16x2 %r700,%r75,%r77;
}

	
	{sub.f16x2 %r703,%r698,%r699;
}

	
	{xor.b32 %r706,%r703,0x80008000;
}

	
	{add.f16x2 %r708,%r682,%r694;
}

	
	{add.f16x2 %r711,%r685,%r697;
}

	
	{sub.f16x2 %r714,%r682,%r694;
}

	
	{sub.f16x2 %r717,%r685,%r697;
}

	
	{add.f16x2 %r720,%r688,%r706;
}

	
	{add.f16x2 %r723,%r691,%r700;
}

	
	{sub.f16x2 %r726,%r688,%r706;
}

	
	{sub.f16x2 %r729,%r691,%r700;
}

	and.b32 %r80, %r5, 64;
bfe.u32 %r846, %r5, 6, 1;
cvt.rn.f32.u32	%f51, %r846;
mul.f32 %f52, %f51, 0f3F490FDB;
cos.approx.f32 %f41, %f52;
sin.approx.f32 %f53, %f52;
neg.f32 %f42, %f53;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f41;
cvt.rn.f16.f32 high, %f42;
mov.b32 %r732, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r732;
mov.b32 %r735, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r732;
mov.b32 %r737, {high,high};}


	
	{mul.f16x2 %r739,%r723,%r737;
}

	
	{fma.rn.f16x2 %r742,%r720,%r735,%r739;
}

	
	{mul.f16x2 %r746,%r720,%r737;
}

	
	{xor.b32 %r749,%r746,0x80008000;
}

	
	{fma.rn.f16x2 %r751,%r723,%r735,%r749;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r732;
mov.b32 %r755, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r732;
mov.b32 %r757, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r759, {low,high};}


	
	{mul.f16x2 %r760,%r757,%r759;
}

	
	{mul.f16x2 %r763,%r732,%r755;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r732;
mov.b32 %r766, {high,low};}


	
	{fma.rn.f16x2 %r768,%r760,%r766,%r763;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r768;
mov.b32 %r772, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r768;
mov.b32 %r774, {high,high};}


	
	{mul.f16x2 %r776,%r717,%r774;
}

	
	{fma.rn.f16x2 %r779,%r714,%r772,%r776;
}

	
	{mul.f16x2 %r783,%r714,%r774;
}

	
	{xor.b32 %r786,%r783,0x80008000;
}

	
	{fma.rn.f16x2 %r788,%r717,%r772,%r786;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r732;
mov.b32 %r792, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r732;
mov.b32 %r794, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r796, {low,high};}


	
	{mul.f16x2 %r797,%r794,%r796;
}

	
	{mul.f16x2 %r800,%r768,%r792;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r768;
mov.b32 %r803, {high,low};}


	
	{fma.rn.f16x2 %r805,%r797,%r803,%r800;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r805;
mov.b32 %r809, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r805;
mov.b32 %r811, {high,high};}


	
	{mul.f16x2 %r813,%r729,%r811;
}

	
	{fma.rn.f16x2 %r816,%r726,%r809,%r813;
}

	
	{mul.f16x2 %r820,%r726,%r811;
}

	
	{xor.b32 %r823,%r820,0x80008000;
}

	
	{fma.rn.f16x2 %r825,%r729,%r809,%r823;
}

	and.b32 %r847, %r5, 63;
add.s32 %r87, %r39, %r847;
barrier.sync 0;
shl.b32 %r848, %r80, 2;
add.s32 %r849, %r848, %r87;
shl.b32 %r850, %r849, 2;
add.s32 %r88, %r332, %r850;
st.shared.u32 [%r88], %r708;
st.shared.u32 [%r88+256], %r742;
st.shared.u32 [%r88+512], %r779;
st.shared.u32 [%r88+768], %r816;
barrier.sync 0;
add.s32 %r852, %r80, %r87;
shl.b32 %r853, %r852, 2;
add.s32 %r89, %r332, %r853;
ld.shared.u32 %r90, [%r89];
ld.shared.u32 %r91, [%r89+512];
ld.shared.u32 %r92, [%r89+1024];
ld.shared.u32 %r93, [%r89+1536];
barrier.sync 0;
st.shared.u32 [%r88], %r711;
st.shared.u32 [%r88+256], %r751;
st.shared.u32 [%r88+512], %r788;
st.shared.u32 [%r88+768], %r825;
barrier.sync 0;

	{add.f16x2 %r855,%r90,%r92;
}

	
	{sub.f16x2 %r861,%r90,%r92;
}

	
	{add.f16x2 %r867,%r91,%r93;
}

	
	{sub.f16x2 %r873,%r91,%r93;
}

	barrier.sync 0;
add.s32 %r881, %r332, %r328;
st.shared.u32 [%r881], %r855;
st.shared.u32 [%r881+512], %r867;
st.shared.u32 [%r881+1024], %r861;
st.shared.u32 [%r881+1536], %r873;
barrier.sync 0;
shl.b32 %r894, %r5, 3;
add.s32 %r896, %r332, %r894;
ld.shared.u32 %r883, [%r896];
ld.shared.u32 %r884, [%r896+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r883;
mov.b32 {blow,bhigh}, %r884;
mov.b32 %r882, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r883;
mov.b32 {blow,bhigh}, %r884;
mov.b32 %r885, {ahigh,bhigh};}


	ld.shared.u32 %r889, [%r896+1024];
ld.shared.u32 %r890, [%r896+1028];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r889;
mov.b32 {blow,bhigh}, %r890;
mov.b32 %r888, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r889;
mov.b32 {blow,bhigh}, %r890;
mov.b32 %r891, {ahigh,bhigh};}


	setp.eq.s16	%p3, %rs1, 0;
selp.b32	%r897, 256, 257, %p3;
shl.b32 %r898, %r2, 1;
add.s32 %r899, %r898, 1;
mad.lo.s32 %r900, %r898, %r897, %r5;
cvta.to.global.u64 %rd12, %rd7;
mul.wide.u32 %rd13, %r900, 4;
add.s64 %rd4, %rd12, %rd13;
mad.lo.s32 %r901, %r899, %r897, %r5;
mul.wide.u32 %rd14, %r901, 4;
add.s64 %rd5, %rd12, %rd14;
@%p1 bra BB8_6;
bra.uni BB8_5;

BB8_6:
st.global.u32 [%rd4], %r882;
st.global.u32 [%rd4+512], %r888;
bra.uni BB8_7;

BB8_5:
shl.b32 %r902, %r3, 1;
add.s32 %r903, %r902, -1;
st.global.u32 [%rd4], %r882;
st.global.u32 [%rd4+512], %r888;
setp.ge.u32	%p5, %r903, %r1;
@%p5 bra BB8_8;

BB8_7:
st.global.u32 [%rd5], %r885;
st.global.u32 [%rd5+512], %r891;

BB8_8:
ret;
}


.weak .entry _Z14vector_fft_c2rILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 256, 1, 1
{
.reg .pred %p<7>;
.reg .b16 %rs<10>;
.reg .f32 %f<58>;
.reg .b32 %r<944>;
.reg .f64 %fd<2>;
.reg .b64 %rd<15>;


ld.param.u32 %r1, [_Z14vector_fft_c2rILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd6, [_Z14vector_fft_c2rILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
mul.lo.s32 %r129, %r2, 1026;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p1, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r130, %r129, %r5;
mov.u32 %r131, 256;
sub.s32 %r132, %r131, %r5;
add.s32 %r133, %r129, %r132;
cvta.to.global.u64 %rd8, %rd6;
mul.wide.u32 %rd9, %r130, 4;
add.s64 %rd1, %rd8, %rd9;
mul.wide.u32 %rd10, %r133, 4;
add.s64 %rd2, %rd8, %rd10;
add.s32 %r134, %r133, 513;
mul.wide.u32 %rd11, %r134, 4;
add.s64 %rd3, %rd8, %rd11;
@%p1 bra BB9_3;
bra.uni BB9_1;

BB9_3:
ld.global.u32 %r943, [%rd1];
ld.global.u32 %r142, [%rd2];
xor.b32 %r937, %r142, -2147483648;
ld.global.u32 %r941, [%rd1+1024];
ld.global.u32 %r143, [%rd2+1024];
xor.b32 %r939, %r143, -2147483648;
ld.global.u32 %r942, [%rd1+2052];
ld.global.u32 %r144, [%rd3];
xor.b32 %r936, %r144, -2147483648;
ld.global.u32 %r940, [%rd1+3076];
ld.global.u32 %r145, [%rd3+1024];
xor.b32 %r938, %r145, -2147483648;
bra.uni BB9_4;

BB9_1:
shl.b32 %r136, %r3, 1;
add.s32 %r137, %r136, -1;
ld.global.u32 %r943, [%rd1];
ld.global.u32 %r138, [%rd2];
xor.b32 %r937, %r138, -2147483648;
ld.global.u32 %r941, [%rd1+1024];
ld.global.u32 %r139, [%rd2+1024];
xor.b32 %r939, %r139, -2147483648;
setp.ge.u32	%p2, %r137, %r1;
@%p2 bra BB9_4;

ld.global.u32 %r942, [%rd1+2052];
ld.global.u32 %r140, [%rd3];
xor.b32 %r936, %r140, -2147483648;
ld.global.u32 %r940, [%rd1+3076];
ld.global.u32 %r141, [%rd3+1024];
xor.b32 %r938, %r141, -2147483648;

BB9_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r943;
mov.b32 {blow,bhigh}, %r942;
mov.b32 %r146, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r943;
mov.b32 {blow,bhigh}, %r942;
mov.b32 %r149, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r941;
mov.b32 {blow,bhigh}, %r940;
mov.b32 %r152, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r941;
mov.b32 {blow,bhigh}, %r940;
mov.b32 %r155, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r939;
mov.b32 {blow,bhigh}, %r938;
mov.b32 %r158, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r939;
mov.b32 {blow,bhigh}, %r938;
mov.b32 %r161, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r937;
mov.b32 {blow,bhigh}, %r936;
mov.b32 %r164, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r937;
mov.b32 {blow,bhigh}, %r936;
mov.b32 %r167, {ahigh,bhigh};}


	
	{add.f16x2 %r170,%r146,%r158;
}

	
	{add.f16x2 %r173,%r149,%r161;
}

	
	{sub.f16x2 %r176,%r146,%r158;
}

	
	{sub.f16x2 %r179,%r149,%r161;
}

	
	{add.f16x2 %r182,%r152,%r164;
}

	
	{add.f16x2 %r185,%r155,%r167;
}

	
	{sub.f16x2 %r188,%r152,%r164;
}

	
	{sub.f16x2 %r191,%r155,%r167;
}

	
	{xor.b32 %r194,%r191,0x80008000;
}

	
	{add.f16x2 %r196,%r170,%r182;
}

	
	{add.f16x2 %r199,%r173,%r185;
}

	
	{sub.f16x2 %r202,%r170,%r182;
}

	
	{sub.f16x2 %r205,%r173,%r185;
}

	
	{add.f16x2 %r208,%r176,%r194;
}

	
	{add.f16x2 %r211,%r179,%r188;
}

	
	{sub.f16x2 %r214,%r176,%r194;
}

	
	{sub.f16x2 %r217,%r179,%r188;
}

	and.b32 %r38, %r5, 255;
cvt.rn.f32.u32	%f6, %r38;
mul.f32 %f1, %f6, 0f3BC90FDB;
setp.eq.s32	%p3, %r38, 255;
mov.f32 %f57, 0f3BC90F88;
@%p3 bra BB9_6;

cos.approx.f32 %f57, %f1;

BB9_6:
shl.b32 %r334, %r5, 2;
and.b32 %r39, %r334, -1024;
sin.approx.f32 %f17, %f1;
neg.f32 %f8, %f17;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f57;
cvt.rn.f16.f32 high, %f8;
mov.b32 %r220, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r220;
mov.b32 %r223, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r220;
mov.b32 %r225, {high,high};}


	
	{mul.f16x2 %r227,%r211,%r225;
}

	
	{fma.rn.f16x2 %r230,%r208,%r223,%r227;
}

	
	{mul.f16x2 %r234,%r208,%r225;
}

	
	{xor.b32 %r237,%r234,0x80008000;
}

	
	{fma.rn.f16x2 %r239,%r211,%r223,%r237;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r220;
mov.b32 %r243, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r220;
mov.b32 %r245, {high,high};}


	mov.f32 %f13, 0fBF800000;
mov.f32 %f14, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r247, {low,high};}


	
	{mul.f16x2 %r248,%r245,%r247;
}

	
	{mul.f16x2 %r251,%r220,%r243;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r220;
mov.b32 %r254, {high,low};}


	
	{fma.rn.f16x2 %r256,%r248,%r254,%r251;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r256;
mov.b32 %r260, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r256;
mov.b32 %r262, {high,high};}


	
	{mul.f16x2 %r264,%r205,%r262;
}

	
	{fma.rn.f16x2 %r267,%r202,%r260,%r264;
}

	
	{mul.f16x2 %r271,%r202,%r262;
}

	
	{xor.b32 %r274,%r271,0x80008000;
}

	
	{fma.rn.f16x2 %r276,%r205,%r260,%r274;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r220;
mov.b32 %r280, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r220;
mov.b32 %r282, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r284, {low,high};}


	
	{mul.f16x2 %r285,%r282,%r284;
}

	
	{mul.f16x2 %r288,%r256,%r280;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r256;
mov.b32 %r291, {high,low};}


	
	{fma.rn.f16x2 %r293,%r285,%r291,%r288;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r293;
mov.b32 %r297, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r293;
mov.b32 %r299, {high,high};}


	
	{mul.f16x2 %r301,%r217,%r299;
}

	
	{fma.rn.f16x2 %r304,%r214,%r297,%r301;
}

	
	{mul.f16x2 %r308,%r214,%r299;
}

	
	{xor.b32 %r311,%r308,0x80008000;
}

	
	{fma.rn.f16x2 %r313,%r217,%r297,%r311;
}

	barrier.sync 0;
shl.b32 %r335, %r38, 2;
add.s32 %r336, %r335, %r39;
shl.b32 %r337, %r336, 2;
mov.u32 %r338, smem_full;
add.s32 %r46, %r338, %r337;
st.shared.u32 [%r46], %r196;
st.shared.u32 [%r46+4], %r230;
st.shared.u32 [%r46+8], %r267;
st.shared.u32 [%r46+12], %r304;
barrier.sync 0;
add.s32 %r339, %r38, %r39;
shl.b32 %r340, %r339, 2;
add.s32 %r47, %r338, %r340;
ld.shared.u32 %r48, [%r47];
ld.shared.u32 %r49, [%r47+1024];
ld.shared.u32 %r50, [%r47+2048];
ld.shared.u32 %r51, [%r47+3072];
barrier.sync 0;
st.shared.u32 [%r46], %r199;
st.shared.u32 [%r46+4], %r239;
st.shared.u32 [%r46+8], %r276;
st.shared.u32 [%r46+12], %r313;
barrier.sync 0;
ld.shared.u32 %r346, [%r47];
ld.shared.u32 %r358, [%r47+1024];
ld.shared.u32 %r347, [%r47+2048];
ld.shared.u32 %r359, [%r47+3072];

	{add.f16x2 %r342,%r48,%r50;
}

	
	{add.f16x2 %r345,%r346,%r347;
}

	
	{sub.f16x2 %r348,%r48,%r50;
}

	
	{sub.f16x2 %r351,%r346,%r347;
}

	
	{add.f16x2 %r354,%r49,%r51;
}

	
	{add.f16x2 %r357,%r358,%r359;
}

	
	{sub.f16x2 %r360,%r49,%r51;
}

	
	{sub.f16x2 %r363,%r358,%r359;
}

	
	{xor.b32 %r366,%r363,0x80008000;
}

	
	{add.f16x2 %r368,%r342,%r354;
}

	
	{add.f16x2 %r371,%r345,%r357;
}

	
	{sub.f16x2 %r374,%r342,%r354;
}

	
	{sub.f16x2 %r377,%r345,%r357;
}

	
	{add.f16x2 %r380,%r348,%r366;
}

	
	{add.f16x2 %r383,%r351,%r360;
}

	
	{sub.f16x2 %r386,%r348,%r366;
}

	
	{sub.f16x2 %r389,%r351,%r360;
}

	and.b32 %r54, %r5, 252;
bfe.u32 %r506, %r5, 2, 6;
cvt.rn.f32.u32	%f28, %r506;
mul.f32 %f29, %f28, 0f3CC90FDB;
cos.approx.f32 %f18, %f29;
sin.approx.f32 %f30, %f29;
neg.f32 %f19, %f30;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f18;
cvt.rn.f16.f32 high, %f19;
mov.b32 %r392, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r392;
mov.b32 %r395, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r392;
mov.b32 %r397, {high,high};}


	
	{mul.f16x2 %r399,%r383,%r397;
}

	
	{fma.rn.f16x2 %r402,%r380,%r395,%r399;
}

	
	{mul.f16x2 %r406,%r380,%r397;
}

	
	{xor.b32 %r409,%r406,0x80008000;
}

	
	{fma.rn.f16x2 %r411,%r383,%r395,%r409;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r392;
mov.b32 %r415, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r392;
mov.b32 %r417, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r419, {low,high};}


	
	{mul.f16x2 %r420,%r417,%r419;
}

	
	{mul.f16x2 %r423,%r392,%r415;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r392;
mov.b32 %r426, {high,low};}


	
	{fma.rn.f16x2 %r428,%r420,%r426,%r423;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r428;
mov.b32 %r432, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r428;
mov.b32 %r434, {high,high};}


	
	{mul.f16x2 %r436,%r377,%r434;
}

	
	{fma.rn.f16x2 %r439,%r374,%r432,%r436;
}

	
	{mul.f16x2 %r443,%r374,%r434;
}

	
	{xor.b32 %r446,%r443,0x80008000;
}

	
	{fma.rn.f16x2 %r448,%r377,%r432,%r446;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r392;
mov.b32 %r452, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r392;
mov.b32 %r454, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r456, {low,high};}


	
	{mul.f16x2 %r457,%r454,%r456;
}

	
	{mul.f16x2 %r460,%r428,%r452;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r428;
mov.b32 %r463, {high,low};}


	
	{fma.rn.f16x2 %r465,%r457,%r463,%r460;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r465;
mov.b32 %r469, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r465;
mov.b32 %r471, {high,high};}


	
	{mul.f16x2 %r473,%r389,%r471;
}

	
	{fma.rn.f16x2 %r476,%r386,%r469,%r473;
}

	
	{mul.f16x2 %r480,%r386,%r471;
}

	
	{xor.b32 %r483,%r480,0x80008000;
}

	
	{fma.rn.f16x2 %r485,%r389,%r469,%r483;
}

	and.b32 %r507, %r5, 3;
add.s32 %r61, %r39, %r507;
barrier.sync 0;
shl.b32 %r508, %r54, 2;
add.s32 %r509, %r508, %r61;
shl.b32 %r510, %r509, 2;
add.s32 %r62, %r338, %r510;
st.shared.u32 [%r62], %r368;
st.shared.u32 [%r62+16], %r402;
st.shared.u32 [%r62+32], %r439;
st.shared.u32 [%r62+48], %r476;
barrier.sync 0;
add.s32 %r512, %r54, %r61;
shl.b32 %r513, %r512, 2;
add.s32 %r63, %r338, %r513;
ld.shared.u32 %r64, [%r63];
ld.shared.u32 %r65, [%r63+1024];
ld.shared.u32 %r66, [%r63+2048];
ld.shared.u32 %r67, [%r63+3072];
barrier.sync 0;
st.shared.u32 [%r62], %r371;
st.shared.u32 [%r62+16], %r411;
st.shared.u32 [%r62+32], %r448;
st.shared.u32 [%r62+48], %r485;
barrier.sync 0;
ld.shared.u32 %r519, [%r63];
ld.shared.u32 %r531, [%r63+1024];
ld.shared.u32 %r520, [%r63+2048];
ld.shared.u32 %r532, [%r63+3072];

	{add.f16x2 %r515,%r64,%r66;
}

	
	{add.f16x2 %r518,%r519,%r520;
}

	
	{sub.f16x2 %r521,%r64,%r66;
}

	
	{sub.f16x2 %r524,%r519,%r520;
}

	
	{add.f16x2 %r527,%r65,%r67;
}

	
	{add.f16x2 %r530,%r531,%r532;
}

	
	{sub.f16x2 %r533,%r65,%r67;
}

	
	{sub.f16x2 %r536,%r531,%r532;
}

	
	{xor.b32 %r539,%r536,0x80008000;
}

	
	{add.f16x2 %r541,%r515,%r527;
}

	
	{add.f16x2 %r544,%r518,%r530;
}

	
	{sub.f16x2 %r547,%r515,%r527;
}

	
	{sub.f16x2 %r550,%r518,%r530;
}

	
	{add.f16x2 %r553,%r521,%r539;
}

	
	{add.f16x2 %r556,%r524,%r533;
}

	
	{sub.f16x2 %r559,%r521,%r539;
}

	
	{sub.f16x2 %r562,%r524,%r533;
}

	and.b32 %r70, %r5, 240;
bfe.u32 %r679, %r5, 4, 4;
cvt.rn.f32.u32	%f41, %r679;
mul.f32 %f42, %f41, 0f3DC90FDB;
cos.approx.f32 %f31, %f42;
sin.approx.f32 %f43, %f42;
neg.f32 %f32, %f43;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f31;
cvt.rn.f16.f32 high, %f32;
mov.b32 %r565, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r568, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r570, {high,high};}


	
	{mul.f16x2 %r572,%r556,%r570;
}

	
	{fma.rn.f16x2 %r575,%r553,%r568,%r572;
}

	
	{mul.f16x2 %r579,%r553,%r570;
}

	
	{xor.b32 %r582,%r579,0x80008000;
}

	
	{fma.rn.f16x2 %r584,%r556,%r568,%r582;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r588, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r590, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r592, {low,high};}


	
	{mul.f16x2 %r593,%r590,%r592;
}

	
	{mul.f16x2 %r596,%r565,%r588;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r599, {high,low};}


	
	{fma.rn.f16x2 %r601,%r593,%r599,%r596;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r601;
mov.b32 %r605, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r601;
mov.b32 %r607, {high,high};}


	
	{mul.f16x2 %r609,%r550,%r607;
}

	
	{fma.rn.f16x2 %r612,%r547,%r605,%r609;
}

	
	{mul.f16x2 %r616,%r547,%r607;
}

	
	{xor.b32 %r619,%r616,0x80008000;
}

	
	{fma.rn.f16x2 %r621,%r550,%r605,%r619;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r625, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r627, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r629, {low,high};}


	
	{mul.f16x2 %r630,%r627,%r629;
}

	
	{mul.f16x2 %r633,%r601,%r625;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r601;
mov.b32 %r636, {high,low};}


	
	{fma.rn.f16x2 %r638,%r630,%r636,%r633;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r638;
mov.b32 %r642, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r638;
mov.b32 %r644, {high,high};}


	
	{mul.f16x2 %r646,%r562,%r644;
}

	
	{fma.rn.f16x2 %r649,%r559,%r642,%r646;
}

	
	{mul.f16x2 %r653,%r559,%r644;
}

	
	{xor.b32 %r656,%r653,0x80008000;
}

	
	{fma.rn.f16x2 %r658,%r562,%r642,%r656;
}

	and.b32 %r680, %r5, 15;
add.s32 %r77, %r39, %r680;
barrier.sync 0;
shl.b32 %r681, %r70, 2;
add.s32 %r682, %r681, %r77;
shl.b32 %r683, %r682, 2;
add.s32 %r78, %r338, %r683;
st.shared.u32 [%r78], %r541;
st.shared.u32 [%r78+64], %r575;
st.shared.u32 [%r78+128], %r612;
st.shared.u32 [%r78+192], %r649;
barrier.sync 0;
add.s32 %r685, %r70, %r77;
shl.b32 %r686, %r685, 2;
add.s32 %r79, %r338, %r686;
ld.shared.u32 %r80, [%r79];
ld.shared.u32 %r81, [%r79+1024];
ld.shared.u32 %r82, [%r79+2048];
ld.shared.u32 %r83, [%r79+3072];
barrier.sync 0;
st.shared.u32 [%r78], %r544;
st.shared.u32 [%r78+64], %r584;
st.shared.u32 [%r78+128], %r621;
st.shared.u32 [%r78+192], %r658;
barrier.sync 0;
ld.shared.u32 %r692, [%r79];
ld.shared.u32 %r704, [%r79+1024];
ld.shared.u32 %r693, [%r79+2048];
ld.shared.u32 %r705, [%r79+3072];

	{add.f16x2 %r688,%r80,%r82;
}

	
	{add.f16x2 %r691,%r692,%r693;
}

	
	{sub.f16x2 %r694,%r80,%r82;
}

	
	{sub.f16x2 %r697,%r692,%r693;
}

	
	{add.f16x2 %r700,%r81,%r83;
}

	
	{add.f16x2 %r703,%r704,%r705;
}

	
	{sub.f16x2 %r706,%r81,%r83;
}

	
	{sub.f16x2 %r709,%r704,%r705;
}

	
	{xor.b32 %r712,%r709,0x80008000;
}

	
	{add.f16x2 %r714,%r688,%r700;
}

	
	{add.f16x2 %r717,%r691,%r703;
}

	
	{sub.f16x2 %r720,%r688,%r700;
}

	
	{sub.f16x2 %r723,%r691,%r703;
}

	
	{add.f16x2 %r726,%r694,%r712;
}

	
	{add.f16x2 %r729,%r697,%r706;
}

	
	{sub.f16x2 %r732,%r694,%r712;
}

	
	{sub.f16x2 %r735,%r697,%r706;
}

	and.b32 %r86, %r5, 192;
bfe.u32 %r852, %r5, 6, 2;
cvt.rn.f32.u32	%f54, %r852;
mul.f32 %f55, %f54, 0f3EC90FDB;
cos.approx.f32 %f44, %f55;
sin.approx.f32 %f56, %f55;
neg.f32 %f45, %f56;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r738, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r738;
mov.b32 %r741, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r738;
mov.b32 %r743, {high,high};}


	
	{mul.f16x2 %r745,%r729,%r743;
}

	
	{fma.rn.f16x2 %r748,%r726,%r741,%r745;
}

	
	{mul.f16x2 %r752,%r726,%r743;
}

	
	{xor.b32 %r755,%r752,0x80008000;
}

	
	{fma.rn.f16x2 %r757,%r729,%r741,%r755;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r738;
mov.b32 %r761, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r738;
mov.b32 %r763, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r765, {low,high};}


	
	{mul.f16x2 %r766,%r763,%r765;
}

	
	{mul.f16x2 %r769,%r738,%r761;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r738;
mov.b32 %r772, {high,low};}


	
	{fma.rn.f16x2 %r774,%r766,%r772,%r769;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r774;
mov.b32 %r778, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r774;
mov.b32 %r780, {high,high};}


	
	{mul.f16x2 %r782,%r723,%r780;
}

	
	{fma.rn.f16x2 %r785,%r720,%r778,%r782;
}

	
	{mul.f16x2 %r789,%r720,%r780;
}

	
	{xor.b32 %r792,%r789,0x80008000;
}

	
	{fma.rn.f16x2 %r794,%r723,%r778,%r792;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r738;
mov.b32 %r798, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r738;
mov.b32 %r800, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r802, {low,high};}


	
	{mul.f16x2 %r803,%r800,%r802;
}

	
	{mul.f16x2 %r806,%r774,%r798;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r774;
mov.b32 %r809, {high,low};}


	
	{fma.rn.f16x2 %r811,%r803,%r809,%r806;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r811;
mov.b32 %r815, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r811;
mov.b32 %r817, {high,high};}


	
	{mul.f16x2 %r819,%r735,%r817;
}

	
	{fma.rn.f16x2 %r822,%r732,%r815,%r819;
}

	
	{mul.f16x2 %r826,%r732,%r817;
}

	
	{xor.b32 %r829,%r826,0x80008000;
}

	
	{fma.rn.f16x2 %r831,%r735,%r815,%r829;
}

	and.b32 %r853, %r5, 63;
add.s32 %r93, %r39, %r853;
barrier.sync 0;
shl.b32 %r854, %r86, 2;
add.s32 %r855, %r854, %r93;
shl.b32 %r856, %r855, 2;
add.s32 %r94, %r338, %r856;
st.shared.u32 [%r94], %r714;
st.shared.u32 [%r94+256], %r748;
st.shared.u32 [%r94+512], %r785;
st.shared.u32 [%r94+768], %r822;
barrier.sync 0;
add.s32 %r858, %r86, %r93;
shl.b32 %r859, %r858, 2;
add.s32 %r95, %r338, %r859;
ld.shared.u32 %r96, [%r95];
ld.shared.u32 %r97, [%r95+1024];
ld.shared.u32 %r98, [%r95+2048];
ld.shared.u32 %r99, [%r95+3072];
barrier.sync 0;
st.shared.u32 [%r94], %r717;
st.shared.u32 [%r94+256], %r757;
st.shared.u32 [%r94+512], %r794;
st.shared.u32 [%r94+768], %r831;
barrier.sync 0;
ld.shared.u32 %r883, [%r95+1024];
ld.shared.u32 %r884, [%r95+3072];

	{add.f16x2 %r861,%r96,%r98;
}

	
	{sub.f16x2 %r867,%r96,%r98;
}

	
	{add.f16x2 %r873,%r97,%r99;
}

	
	{sub.f16x2 %r882,%r883,%r884;
}

	
	{xor.b32 %r885,%r882,0x80008000;
}

	
	{add.f16x2 %r887,%r861,%r873;
}

	
	{sub.f16x2 %r893,%r861,%r873;
}

	
	{add.f16x2 %r899,%r867,%r885;
}

	
	{sub.f16x2 %r905,%r867,%r885;
}

	barrier.sync 0;
add.s32 %r913, %r338, %r334;
st.shared.u32 [%r913], %r887;
st.shared.u32 [%r913+1024], %r899;
st.shared.u32 [%r913+2048], %r893;
st.shared.u32 [%r913+3072], %r905;
barrier.sync 0;
shl.b32 %r926, %r5, 3;
add.s32 %r928, %r338, %r926;
ld.shared.u32 %r915, [%r928];
ld.shared.u32 %r916, [%r928+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r915;
mov.b32 {blow,bhigh}, %r916;
mov.b32 %r914, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r915;
mov.b32 {blow,bhigh}, %r916;
mov.b32 %r917, {ahigh,bhigh};}


	ld.shared.u32 %r921, [%r928+2048];
ld.shared.u32 %r922, [%r928+2052];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r921;
mov.b32 {blow,bhigh}, %r922;
mov.b32 %r920, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r921;
mov.b32 {blow,bhigh}, %r922;
mov.b32 %r923, {ahigh,bhigh};}


	setp.eq.s16	%p4, %rs1, 0;
selp.b32	%r929, 512, 513, %p4;
shl.b32 %r930, %r2, 1;
add.s32 %r931, %r930, 1;
mad.lo.s32 %r932, %r930, %r929, %r5;
cvta.to.global.u64 %rd12, %rd7;
mul.wide.u32 %rd13, %r932, 4;
add.s64 %rd4, %rd12, %rd13;
mad.lo.s32 %r933, %r931, %r929, %r5;
mul.wide.u32 %rd14, %r933, 4;
add.s64 %rd5, %rd12, %rd14;
@%p1 bra BB9_8;
bra.uni BB9_7;

BB9_8:
st.global.u32 [%rd4], %r914;
st.global.u32 [%rd4+1024], %r920;
bra.uni BB9_9;

BB9_7:
shl.b32 %r934, %r3, 1;
add.s32 %r935, %r934, -1;
st.global.u32 [%rd4], %r914;
st.global.u32 [%rd4+1024], %r920;
setp.ge.u32	%p6, %r935, %r1;
@%p6 bra BB9_10;

BB9_9:
st.global.u32 [%rd5], %r917;
st.global.u32 [%rd5+1024], %r923;

BB9_10:
ret;
}


.weak .entry _Z14vector_fft_c2rILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 512, 1, 1
{
.reg .pred %p<8>;
.reg .b16 %rs<10>;
.reg .f32 %f<72>;
.reg .b32 %r<1084>;
.reg .f64 %fd<2>;
.reg .b64 %rd<15>;


ld.param.u32 %r1, [_Z14vector_fft_c2rILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd6, [_Z14vector_fft_c2rILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r121, %ctaid.x;
mul.lo.s32 %r122, %r121, 2050;
mov.u32 %r2, %nctaid.x;
add.s32 %r3, %r2, -1;
setp.lt.u32	%p2, %r121, %r3;
mov.u32 %r4, %tid.x;
add.s32 %r123, %r122, %r4;
mov.u32 %r124, 512;
sub.s32 %r125, %r124, %r4;
add.s32 %r126, %r122, %r125;
cvta.to.global.u64 %rd8, %rd6;
mul.wide.u32 %rd9, %r123, 4;
add.s64 %rd1, %rd8, %rd9;
mul.wide.u32 %rd10, %r126, 4;
add.s64 %rd2, %rd8, %rd10;
add.s32 %r127, %r126, 1025;
mul.wide.u32 %rd11, %r127, 4;
add.s64 %rd3, %rd8, %rd11;
@%p2 bra BB10_3;
bra.uni BB10_1;

BB10_3:
ld.global.u32 %r1083, [%rd1];
ld.global.u32 %r135, [%rd2];
xor.b32 %r1077, %r135, -2147483648;
ld.global.u32 %r1081, [%rd1+2048];
ld.global.u32 %r136, [%rd2+2048];
xor.b32 %r1079, %r136, -2147483648;
ld.global.u32 %r1082, [%rd1+4100];
ld.global.u32 %r137, [%rd3];
xor.b32 %r1076, %r137, -2147483648;
ld.global.u32 %r1080, [%rd1+6148];
ld.global.u32 %r138, [%rd3+2048];
xor.b32 %r1078, %r138, -2147483648;
bra.uni BB10_4;

BB10_1:
shl.b32 %r129, %r2, 1;
add.s32 %r130, %r129, -1;
ld.global.u32 %r1083, [%rd1];
ld.global.u32 %r131, [%rd2];
xor.b32 %r1077, %r131, -2147483648;
ld.global.u32 %r1081, [%rd1+2048];
ld.global.u32 %r132, [%rd2+2048];
xor.b32 %r1079, %r132, -2147483648;
setp.ge.u32	%p3, %r130, %r1;
@%p3 bra BB10_4;

ld.global.u32 %r1082, [%rd1+4100];
ld.global.u32 %r133, [%rd3];
xor.b32 %r1076, %r133, -2147483648;
ld.global.u32 %r1080, [%rd1+6148];
ld.global.u32 %r134, [%rd3+2048];
xor.b32 %r1078, %r134, -2147483648;

BB10_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1083;
mov.b32 {blow,bhigh}, %r1082;
mov.b32 %r139, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1083;
mov.b32 {blow,bhigh}, %r1082;
mov.b32 %r142, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1081;
mov.b32 {blow,bhigh}, %r1080;
mov.b32 %r145, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1081;
mov.b32 {blow,bhigh}, %r1080;
mov.b32 %r148, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1079;
mov.b32 {blow,bhigh}, %r1078;
mov.b32 %r151, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1079;
mov.b32 {blow,bhigh}, %r1078;
mov.b32 %r154, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1077;
mov.b32 {blow,bhigh}, %r1076;
mov.b32 %r157, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1077;
mov.b32 {blow,bhigh}, %r1076;
mov.b32 %r160, {ahigh,bhigh};}


	
	{add.f16x2 %r163,%r139,%r151;
}

	
	{add.f16x2 %r166,%r142,%r154;
}

	
	{sub.f16x2 %r169,%r139,%r151;
}

	
	{sub.f16x2 %r172,%r142,%r154;
}

	
	{add.f16x2 %r175,%r145,%r157;
}

	
	{add.f16x2 %r178,%r148,%r160;
}

	
	{sub.f16x2 %r181,%r145,%r157;
}

	
	{sub.f16x2 %r184,%r148,%r160;
}

	
	{xor.b32 %r187,%r184,0x80008000;
}

	
	{add.f16x2 %r189,%r163,%r175;
}

	
	{add.f16x2 %r192,%r166,%r178;
}

	
	{sub.f16x2 %r195,%r163,%r175;
}

	
	{sub.f16x2 %r198,%r166,%r178;
}

	
	{add.f16x2 %r201,%r169,%r187;
}

	
	{add.f16x2 %r204,%r172,%r181;
}

	
	{sub.f16x2 %r207,%r169,%r187;
}

	
	{sub.f16x2 %r210,%r172,%r181;
}

	and.b32 %r37, %r4, 511;
cvt.rn.f32.u32	%f6, %r37;
mul.f32 %f1, %f6, 0f3B490FDB;
setp.eq.s32	%p4, %r37, 510;
mov.f32 %f71, 0f3BC90F88;
@%p4 bra BB10_7;

setp.eq.s32	%p5, %r37, 511;
mov.f32 %f71, 0f3B490FC6;
@%p5 bra BB10_7;

cos.approx.f32 %f71, %f1;

BB10_7:
shl.b32 %r327, %r4, 2;
and.b32 %r38, %r327, -2048;
sin.approx.f32 %f18, %f1;
neg.f32 %f9, %f18;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f71;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r213, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r213;
mov.b32 %r216, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r213;
mov.b32 %r218, {high,high};}


	
	{mul.f16x2 %r220,%r204,%r218;
}

	
	{fma.rn.f16x2 %r223,%r201,%r216,%r220;
}

	
	{mul.f16x2 %r227,%r201,%r218;
}

	
	{xor.b32 %r230,%r227,0x80008000;
}

	
	{fma.rn.f16x2 %r232,%r204,%r216,%r230;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r213;
mov.b32 %r236, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r213;
mov.b32 %r238, {high,high};}


	mov.f32 %f14, 0fBF800000;
mov.f32 %f15, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r240, {low,high};}


	
	{mul.f16x2 %r241,%r238,%r240;
}

	
	{mul.f16x2 %r244,%r213,%r236;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r213;
mov.b32 %r247, {high,low};}


	
	{fma.rn.f16x2 %r249,%r241,%r247,%r244;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r249;
mov.b32 %r253, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r249;
mov.b32 %r255, {high,high};}


	
	{mul.f16x2 %r257,%r198,%r255;
}

	
	{fma.rn.f16x2 %r260,%r195,%r253,%r257;
}

	
	{mul.f16x2 %r264,%r195,%r255;
}

	
	{xor.b32 %r267,%r264,0x80008000;
}

	
	{fma.rn.f16x2 %r269,%r198,%r253,%r267;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r213;
mov.b32 %r273, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r213;
mov.b32 %r275, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r277, {low,high};}


	
	{mul.f16x2 %r278,%r275,%r277;
}

	
	{mul.f16x2 %r281,%r249,%r273;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r249;
mov.b32 %r284, {high,low};}


	
	{fma.rn.f16x2 %r286,%r278,%r284,%r281;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r286;
mov.b32 %r290, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r286;
mov.b32 %r292, {high,high};}


	
	{mul.f16x2 %r294,%r210,%r292;
}

	
	{fma.rn.f16x2 %r297,%r207,%r290,%r294;
}

	
	{mul.f16x2 %r301,%r207,%r292;
}

	
	{xor.b32 %r304,%r301,0x80008000;
}

	
	{fma.rn.f16x2 %r306,%r210,%r290,%r304;
}

	barrier.sync 0;
shl.b32 %r328, %r38, 3;
mov.u32 %r329, smem_full;
add.s32 %r46, %r329, %r328;
shl.b32 %r330, %r37, 5;
add.s32 %r331, %r46, %r330;
st.shared.u32 [%r331], %r189;
st.shared.u32 [%r331+4], %r192;
st.shared.u32 [%r331+8], %r223;
st.shared.u32 [%r331+12], %r232;
st.shared.u32 [%r331+16], %r260;
st.shared.u32 [%r331+20], %r269;
st.shared.u32 [%r331+24], %r297;
st.shared.u32 [%r331+28], %r306;
barrier.sync 0;
shl.b32 %r496, %r37, 3;
add.s32 %r497, %r46, %r496;
ld.shared.u32 %r333, [%r497];
ld.shared.u32 %r336, [%r497+4];
ld.shared.u32 %r345, [%r497+4096];
ld.shared.u32 %r348, [%r497+4100];
ld.shared.u32 %r334, [%r497+8192];
ld.shared.u32 %r337, [%r497+8196];
ld.shared.u32 %r346, [%r497+12288];
ld.shared.u32 %r349, [%r497+12292];

	{add.f16x2 %r332,%r333,%r334;
}

	
	{add.f16x2 %r335,%r336,%r337;
}

	
	{sub.f16x2 %r338,%r333,%r334;
}

	
	{sub.f16x2 %r341,%r336,%r337;
}

	
	{add.f16x2 %r344,%r345,%r346;
}

	
	{add.f16x2 %r347,%r348,%r349;
}

	
	{sub.f16x2 %r350,%r345,%r346;
}

	
	{sub.f16x2 %r353,%r348,%r349;
}

	
	{xor.b32 %r356,%r353,0x80008000;
}

	
	{add.f16x2 %r358,%r332,%r344;
}

	
	{add.f16x2 %r361,%r335,%r347;
}

	
	{sub.f16x2 %r364,%r332,%r344;
}

	
	{sub.f16x2 %r367,%r335,%r347;
}

	
	{add.f16x2 %r370,%r338,%r356;
}

	
	{add.f16x2 %r373,%r341,%r350;
}

	
	{sub.f16x2 %r376,%r338,%r356;
}

	
	{sub.f16x2 %r379,%r341,%r350;
}

	and.b32 %r50, %r4, 508;
and.b32 %r498, %r4, 3;
bfe.u32 %r499, %r4, 2, 7;
add.s32 %r500, %r38, %r498;
cvt.rn.f32.u32	%f29, %r499;
mul.f32 %f30, %f29, 0f3C490FDB;
cos.approx.f32 %f19, %f30;
sin.approx.f32 %f31, %f30;
neg.f32 %f20, %f31;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f19;
cvt.rn.f16.f32 high, %f20;
mov.b32 %r382, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r385, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r387, {high,high};}


	
	{mul.f16x2 %r389,%r373,%r387;
}

	
	{fma.rn.f16x2 %r392,%r370,%r385,%r389;
}

	
	{mul.f16x2 %r396,%r370,%r387;
}

	
	{xor.b32 %r399,%r396,0x80008000;
}

	
	{fma.rn.f16x2 %r401,%r373,%r385,%r399;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r405, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r407, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r409, {low,high};}


	
	{mul.f16x2 %r410,%r407,%r409;
}

	
	{mul.f16x2 %r413,%r382,%r405;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r416, {high,low};}


	
	{fma.rn.f16x2 %r418,%r410,%r416,%r413;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r422, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r424, {high,high};}


	
	{mul.f16x2 %r426,%r367,%r424;
}

	
	{fma.rn.f16x2 %r429,%r364,%r422,%r426;
}

	
	{mul.f16x2 %r433,%r364,%r424;
}

	
	{xor.b32 %r436,%r433,0x80008000;
}

	
	{fma.rn.f16x2 %r438,%r367,%r422,%r436;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r442, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r444, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r446, {low,high};}


	
	{mul.f16x2 %r447,%r444,%r446;
}

	
	{mul.f16x2 %r450,%r418,%r442;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r453, {high,low};}


	
	{fma.rn.f16x2 %r455,%r447,%r453,%r450;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r455;
mov.b32 %r459, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r455;
mov.b32 %r461, {high,high};}


	
	{mul.f16x2 %r463,%r379,%r461;
}

	
	{fma.rn.f16x2 %r466,%r376,%r459,%r463;
}

	
	{mul.f16x2 %r470,%r376,%r461;
}

	
	{xor.b32 %r473,%r470,0x80008000;
}

	
	{fma.rn.f16x2 %r475,%r379,%r459,%r473;
}

	shl.b32 %r501, %r500, 3;
add.s32 %r57, %r329, %r501;
barrier.sync 0;
shl.b32 %r503, %r50, 5;
add.s32 %r504, %r57, %r503;
st.shared.u32 [%r504], %r358;
st.shared.u32 [%r504+4], %r361;
st.shared.u32 [%r504+32], %r392;
st.shared.u32 [%r504+36], %r401;
st.shared.u32 [%r504+64], %r429;
st.shared.u32 [%r504+68], %r438;
st.shared.u32 [%r504+96], %r466;
st.shared.u32 [%r504+100], %r475;
barrier.sync 0;
shl.b32 %r669, %r50, 3;
add.s32 %r670, %r57, %r669;
ld.shared.u32 %r506, [%r670];
ld.shared.u32 %r509, [%r670+4];
ld.shared.u32 %r518, [%r670+4096];
ld.shared.u32 %r521, [%r670+4100];
ld.shared.u32 %r507, [%r670+8192];
ld.shared.u32 %r510, [%r670+8196];
ld.shared.u32 %r519, [%r670+12288];
ld.shared.u32 %r522, [%r670+12292];

	{add.f16x2 %r505,%r506,%r507;
}

	
	{add.f16x2 %r508,%r509,%r510;
}

	
	{sub.f16x2 %r511,%r506,%r507;
}

	
	{sub.f16x2 %r514,%r509,%r510;
}

	
	{add.f16x2 %r517,%r518,%r519;
}

	
	{add.f16x2 %r520,%r521,%r522;
}

	
	{sub.f16x2 %r523,%r518,%r519;
}

	
	{sub.f16x2 %r526,%r521,%r522;
}

	
	{xor.b32 %r529,%r526,0x80008000;
}

	
	{add.f16x2 %r531,%r505,%r517;
}

	
	{add.f16x2 %r534,%r508,%r520;
}

	
	{sub.f16x2 %r537,%r505,%r517;
}

	
	{sub.f16x2 %r540,%r508,%r520;
}

	
	{add.f16x2 %r543,%r511,%r529;
}

	
	{add.f16x2 %r546,%r514,%r523;
}

	
	{sub.f16x2 %r549,%r511,%r529;
}

	
	{sub.f16x2 %r552,%r514,%r523;
}

	and.b32 %r61, %r4, 496;
and.b32 %r671, %r4, 15;
bfe.u32 %r672, %r4, 4, 5;
add.s32 %r673, %r38, %r671;
cvt.rn.f32.u32	%f42, %r672;
mul.f32 %f43, %f42, 0f3D490FDB;
cos.approx.f32 %f32, %f43;
sin.approx.f32 %f44, %f43;
neg.f32 %f33, %f44;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f32;
cvt.rn.f16.f32 high, %f33;
mov.b32 %r555, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r558, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r560, {high,high};}


	
	{mul.f16x2 %r562,%r546,%r560;
}

	
	{fma.rn.f16x2 %r565,%r543,%r558,%r562;
}

	
	{mul.f16x2 %r569,%r543,%r560;
}

	
	{xor.b32 %r572,%r569,0x80008000;
}

	
	{fma.rn.f16x2 %r574,%r546,%r558,%r572;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r578, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r580, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r582, {low,high};}


	
	{mul.f16x2 %r583,%r580,%r582;
}

	
	{mul.f16x2 %r586,%r555,%r578;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r589, {high,low};}


	
	{fma.rn.f16x2 %r591,%r583,%r589,%r586;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r591;
mov.b32 %r595, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r591;
mov.b32 %r597, {high,high};}


	
	{mul.f16x2 %r599,%r540,%r597;
}

	
	{fma.rn.f16x2 %r602,%r537,%r595,%r599;
}

	
	{mul.f16x2 %r606,%r537,%r597;
}

	
	{xor.b32 %r609,%r606,0x80008000;
}

	
	{fma.rn.f16x2 %r611,%r540,%r595,%r609;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r615, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r617, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r619, {low,high};}


	
	{mul.f16x2 %r620,%r617,%r619;
}

	
	{mul.f16x2 %r623,%r591,%r615;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r591;
mov.b32 %r626, {high,low};}


	
	{fma.rn.f16x2 %r628,%r620,%r626,%r623;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r628;
mov.b32 %r632, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r628;
mov.b32 %r634, {high,high};}


	
	{mul.f16x2 %r636,%r552,%r634;
}

	
	{fma.rn.f16x2 %r639,%r549,%r632,%r636;
}

	
	{mul.f16x2 %r643,%r549,%r634;
}

	
	{xor.b32 %r646,%r643,0x80008000;
}

	
	{fma.rn.f16x2 %r648,%r552,%r632,%r646;
}

	shl.b32 %r674, %r673, 3;
add.s32 %r68, %r329, %r674;
barrier.sync 0;
shl.b32 %r676, %r61, 5;
add.s32 %r677, %r68, %r676;
st.shared.u32 [%r677], %r531;
st.shared.u32 [%r677+4], %r534;
st.shared.u32 [%r677+128], %r565;
st.shared.u32 [%r677+132], %r574;
st.shared.u32 [%r677+256], %r602;
st.shared.u32 [%r677+260], %r611;
st.shared.u32 [%r677+384], %r639;
st.shared.u32 [%r677+388], %r648;
barrier.sync 0;
shl.b32 %r842, %r61, 3;
add.s32 %r843, %r68, %r842;
ld.shared.u32 %r679, [%r843];
ld.shared.u32 %r682, [%r843+4];
ld.shared.u32 %r691, [%r843+4096];
ld.shared.u32 %r694, [%r843+4100];
ld.shared.u32 %r680, [%r843+8192];
ld.shared.u32 %r683, [%r843+8196];
ld.shared.u32 %r692, [%r843+12288];
ld.shared.u32 %r695, [%r843+12292];

	{add.f16x2 %r678,%r679,%r680;
}

	
	{add.f16x2 %r681,%r682,%r683;
}

	
	{sub.f16x2 %r684,%r679,%r680;
}

	
	{sub.f16x2 %r687,%r682,%r683;
}

	
	{add.f16x2 %r690,%r691,%r692;
}

	
	{add.f16x2 %r693,%r694,%r695;
}

	
	{sub.f16x2 %r696,%r691,%r692;
}

	
	{sub.f16x2 %r699,%r694,%r695;
}

	
	{xor.b32 %r702,%r699,0x80008000;
}

	
	{add.f16x2 %r704,%r678,%r690;
}

	
	{add.f16x2 %r707,%r681,%r693;
}

	
	{sub.f16x2 %r710,%r678,%r690;
}

	
	{sub.f16x2 %r713,%r681,%r693;
}

	
	{add.f16x2 %r716,%r684,%r702;
}

	
	{add.f16x2 %r719,%r687,%r696;
}

	
	{sub.f16x2 %r722,%r684,%r702;
}

	
	{sub.f16x2 %r725,%r687,%r696;
}

	and.b32 %r72, %r4, 448;
and.b32 %r844, %r4, 63;
bfe.u32 %r845, %r4, 6, 3;
add.s32 %r846, %r38, %r844;
cvt.rn.f32.u32	%f55, %r845;
mul.f32 %f56, %f55, 0f3E490FDB;
cos.approx.f32 %f45, %f56;
sin.approx.f32 %f57, %f56;
neg.f32 %f46, %f57;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f45;
cvt.rn.f16.f32 high, %f46;
mov.b32 %r728, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r728;
mov.b32 %r731, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r728;
mov.b32 %r733, {high,high};}


	
	{mul.f16x2 %r735,%r719,%r733;
}

	
	{fma.rn.f16x2 %r738,%r716,%r731,%r735;
}

	
	{mul.f16x2 %r742,%r716,%r733;
}

	
	{xor.b32 %r745,%r742,0x80008000;
}

	
	{fma.rn.f16x2 %r747,%r719,%r731,%r745;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r728;
mov.b32 %r751, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r728;
mov.b32 %r753, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r755, {low,high};}


	
	{mul.f16x2 %r756,%r753,%r755;
}

	
	{mul.f16x2 %r759,%r728,%r751;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r728;
mov.b32 %r762, {high,low};}


	
	{fma.rn.f16x2 %r764,%r756,%r762,%r759;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r764;
mov.b32 %r768, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r764;
mov.b32 %r770, {high,high};}


	
	{mul.f16x2 %r772,%r713,%r770;
}

	
	{fma.rn.f16x2 %r775,%r710,%r768,%r772;
}

	
	{mul.f16x2 %r779,%r710,%r770;
}

	
	{xor.b32 %r782,%r779,0x80008000;
}

	
	{fma.rn.f16x2 %r784,%r713,%r768,%r782;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r728;
mov.b32 %r788, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r728;
mov.b32 %r790, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r792, {low,high};}


	
	{mul.f16x2 %r793,%r790,%r792;
}

	
	{mul.f16x2 %r796,%r764,%r788;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r764;
mov.b32 %r799, {high,low};}


	
	{fma.rn.f16x2 %r801,%r793,%r799,%r796;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r801;
mov.b32 %r805, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r801;
mov.b32 %r807, {high,high};}


	
	{mul.f16x2 %r809,%r725,%r807;
}

	
	{fma.rn.f16x2 %r812,%r722,%r805,%r809;
}

	
	{mul.f16x2 %r816,%r722,%r807;
}

	
	{xor.b32 %r819,%r816,0x80008000;
}

	
	{fma.rn.f16x2 %r821,%r725,%r805,%r819;
}

	shl.b32 %r847, %r846, 3;
add.s32 %r79, %r329, %r847;
barrier.sync 0;
shl.b32 %r849, %r72, 5;
add.s32 %r850, %r79, %r849;
st.shared.u32 [%r850], %r704;
st.shared.u32 [%r850+4], %r707;
st.shared.u32 [%r850+512], %r738;
st.shared.u32 [%r850+516], %r747;
st.shared.u32 [%r850+1024], %r775;
st.shared.u32 [%r850+1028], %r784;
st.shared.u32 [%r850+1536], %r812;
st.shared.u32 [%r850+1540], %r821;
barrier.sync 0;
shl.b32 %r1015, %r72, 3;
add.s32 %r1016, %r79, %r1015;
ld.shared.u32 %r852, [%r1016];
ld.shared.u32 %r855, [%r1016+4];
ld.shared.u32 %r864, [%r1016+4096];
ld.shared.u32 %r867, [%r1016+4100];
ld.shared.u32 %r853, [%r1016+8192];
ld.shared.u32 %r856, [%r1016+8196];
ld.shared.u32 %r865, [%r1016+12288];
ld.shared.u32 %r868, [%r1016+12292];

	{add.f16x2 %r851,%r852,%r853;
}

	
	{add.f16x2 %r854,%r855,%r856;
}

	
	{sub.f16x2 %r857,%r852,%r853;
}

	
	{sub.f16x2 %r860,%r855,%r856;
}

	
	{add.f16x2 %r863,%r864,%r865;
}

	
	{add.f16x2 %r866,%r867,%r868;
}

	
	{sub.f16x2 %r869,%r864,%r865;
}

	
	{sub.f16x2 %r872,%r867,%r868;
}

	
	{xor.b32 %r875,%r872,0x80008000;
}

	
	{add.f16x2 %r877,%r851,%r863;
}

	
	{add.f16x2 %r880,%r854,%r866;
}

	
	{sub.f16x2 %r883,%r851,%r863;
}

	
	{sub.f16x2 %r886,%r854,%r866;
}

	
	{add.f16x2 %r889,%r857,%r875;
}

	
	{add.f16x2 %r892,%r860,%r869;
}

	
	{sub.f16x2 %r895,%r857,%r875;
}

	
	{sub.f16x2 %r898,%r860,%r869;
}

	and.b32 %r83, %r4, 256;
and.b32 %r1017, %r4, 255;
bfe.u32 %r1018, %r4, 8, 1;
add.s32 %r1019, %r38, %r1017;
cvt.rn.f32.u32	%f68, %r1018;
mul.f32 %f69, %f68, 0f3F490FDB;
cos.approx.f32 %f58, %f69;
sin.approx.f32 %f70, %f69;
neg.f32 %f59, %f70;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f58;
cvt.rn.f16.f32 high, %f59;
mov.b32 %r901, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r901;
mov.b32 %r904, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r901;
mov.b32 %r906, {high,high};}


	
	{mul.f16x2 %r908,%r892,%r906;
}

	
	{fma.rn.f16x2 %r911,%r889,%r904,%r908;
}

	
	{mul.f16x2 %r915,%r889,%r906;
}

	
	{xor.b32 %r918,%r915,0x80008000;
}

	
	{fma.rn.f16x2 %r920,%r892,%r904,%r918;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r901;
mov.b32 %r924, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r901;
mov.b32 %r926, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r928, {low,high};}


	
	{mul.f16x2 %r929,%r926,%r928;
}

	
	{mul.f16x2 %r932,%r901,%r924;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r901;
mov.b32 %r935, {high,low};}


	
	{fma.rn.f16x2 %r937,%r929,%r935,%r932;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r937;
mov.b32 %r941, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r937;
mov.b32 %r943, {high,high};}


	
	{mul.f16x2 %r945,%r886,%r943;
}

	
	{fma.rn.f16x2 %r948,%r883,%r941,%r945;
}

	
	{mul.f16x2 %r952,%r883,%r943;
}

	
	{xor.b32 %r955,%r952,0x80008000;
}

	
	{fma.rn.f16x2 %r957,%r886,%r941,%r955;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r901;
mov.b32 %r961, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r901;
mov.b32 %r963, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r965, {low,high};}


	
	{mul.f16x2 %r966,%r963,%r965;
}

	
	{mul.f16x2 %r969,%r937,%r961;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r937;
mov.b32 %r972, {high,low};}


	
	{fma.rn.f16x2 %r974,%r966,%r972,%r969;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r974;
mov.b32 %r978, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r974;
mov.b32 %r980, {high,high};}


	
	{mul.f16x2 %r982,%r898,%r980;
}

	
	{fma.rn.f16x2 %r985,%r895,%r978,%r982;
}

	
	{mul.f16x2 %r989,%r895,%r980;
}

	
	{xor.b32 %r992,%r989,0x80008000;
}

	
	{fma.rn.f16x2 %r994,%r898,%r978,%r992;
}

	shl.b32 %r1020, %r1019, 3;
add.s32 %r90, %r329, %r1020;
barrier.sync 0;
shl.b32 %r1022, %r83, 5;
add.s32 %r1023, %r90, %r1022;
st.shared.u32 [%r1023], %r877;
st.shared.u32 [%r1023+4], %r880;
st.shared.u32 [%r1023+2048], %r911;
st.shared.u32 [%r1023+2052], %r920;
st.shared.u32 [%r1023+4096], %r948;
st.shared.u32 [%r1023+4100], %r957;
st.shared.u32 [%r1023+6144], %r985;
st.shared.u32 [%r1023+6148], %r994;
barrier.sync 0;
shl.b32 %r1048, %r83, 3;
add.s32 %r1049, %r90, %r1048;
ld.shared.u32 %r1025, [%r1049];
ld.shared.u32 %r1037, [%r1049+4096];
ld.shared.u32 %r1026, [%r1049+8192];
ld.shared.u32 %r1038, [%r1049+12288];

	{add.f16x2 %r1024,%r1025,%r1026;
}

	
	{sub.f16x2 %r1030,%r1025,%r1026;
}

	
	{add.f16x2 %r1036,%r1037,%r1038;
}

	
	{sub.f16x2 %r1042,%r1037,%r1038;
}

	barrier.sync 0;
add.s32 %r1052, %r329, %r327;
st.shared.u32 [%r1052], %r1024;
st.shared.u32 [%r1052+2048], %r1036;
st.shared.u32 [%r1052+4096], %r1030;
st.shared.u32 [%r1052+6144], %r1042;
barrier.sync 0;
shl.b32 %r1065, %r4, 3;
add.s32 %r1067, %r329, %r1065;
ld.shared.u32 %r1054, [%r1067];
ld.shared.u32 %r1055, [%r1067+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1054;
mov.b32 {blow,bhigh}, %r1055;
mov.b32 %r1053, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1054;
mov.b32 {blow,bhigh}, %r1055;
mov.b32 %r1056, {ahigh,bhigh};}


	ld.shared.u32 %r1060, [%r1067+4096];
ld.shared.u32 %r1061, [%r1067+4100];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1060;
mov.b32 {blow,bhigh}, %r1061;
mov.b32 %r1059, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1060;
mov.b32 {blow,bhigh}, %r1061;
mov.b32 %r1062, {ahigh,bhigh};}


	setp.eq.s16	%p6, %rs1, 0;
selp.b32	%r1068, 1024, 1025, %p6;
shl.b32 %r1069, %r121, 1;
add.s32 %r1070, %r1069, 1;
mad.lo.s32 %r1071, %r1069, %r1068, %r4;
cvta.to.global.u64 %rd12, %rd7;
mul.wide.u32 %rd13, %r1071, 4;
add.s64 %rd4, %rd12, %rd13;
mad.lo.s32 %r1072, %r1070, %r1068, %r4;
mul.wide.u32 %rd14, %r1072, 4;
add.s64 %rd5, %rd12, %rd14;
@%p2 bra BB10_9;
bra.uni BB10_8;

BB10_9:
st.global.u32 [%rd4], %r1053;
st.global.u32 [%rd4+2048], %r1059;
bra.uni BB10_10;

BB10_8:
shl.b32 %r1074, %r2, 1;
add.s32 %r1075, %r1074, -1;
st.global.u32 [%rd4], %r1053;
st.global.u32 [%rd4+2048], %r1059;
setp.ge.u32	%p7, %r1075, %r1;
@%p7 bra BB10_11;

BB10_10:
st.global.u32 [%rd5], %r1056;
st.global.u32 [%rd5+2048], %r1062;

BB10_11:
ret;
}


.weak .entry _Z14vector_fft_c2rILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 512, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<10>;
.reg .f32 %f<177>;
.reg .b32 %r<1870>;
.reg .f64 %fd<2>;
.reg .b64 %rd<15>;


ld.param.u32 %r1, [_Z14vector_fft_c2rILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd6, [_Z14vector_fft_c2rILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
mul.lo.s32 %r149, %r2, 4098;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p1, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r150, %r149, %r5;
mov.u32 %r151, 512;
sub.s32 %r152, %r151, %r5;
add.s32 %r153, %r149, %r152;
cvta.to.global.u64 %rd8, %rd6;
mul.wide.u32 %rd9, %r150, 4;
add.s64 %rd1, %rd8, %rd9;
mul.wide.u32 %rd10, %r153, 4;
add.s64 %rd2, %rd8, %rd10;
add.s32 %r154, %r153, 2049;
mul.wide.u32 %rd11, %r154, 4;
add.s64 %rd3, %rd8, %rd11;
@%p1 bra BB11_3;
bra.uni BB11_1;

BB11_3:
ld.global.u32 %r1869, [%rd1];
ld.global.u32 %r166, [%rd2];
xor.b32 %r1855, %r166, -2147483648;
ld.global.u32 %r1867, [%rd1+2048];
ld.global.u32 %r167, [%rd2+2048];
xor.b32 %r1857, %r167, -2147483648;
ld.global.u32 %r1865, [%rd1+4096];
ld.global.u32 %r168, [%rd2+4096];
xor.b32 %r1859, %r168, -2147483648;
ld.global.u32 %r1863, [%rd1+6144];
ld.global.u32 %r169, [%rd2+6144];
xor.b32 %r1861, %r169, -2147483648;
ld.global.u32 %r1868, [%rd1+8196];
ld.global.u32 %r170, [%rd3];
xor.b32 %r1854, %r170, -2147483648;
ld.global.u32 %r1866, [%rd1+10244];
ld.global.u32 %r171, [%rd3+2048];
xor.b32 %r1856, %r171, -2147483648;
ld.global.u32 %r1864, [%rd1+12292];
ld.global.u32 %r172, [%rd3+4096];
xor.b32 %r1858, %r172, -2147483648;
ld.global.u32 %r1862, [%rd1+14340];
ld.global.u32 %r173, [%rd3+6144];
xor.b32 %r1860, %r173, -2147483648;
bra.uni BB11_4;

BB11_1:
shl.b32 %r156, %r3, 1;
add.s32 %r157, %r156, -1;
ld.global.u32 %r1869, [%rd1];
ld.global.u32 %r158, [%rd2];
xor.b32 %r1855, %r158, -2147483648;
ld.global.u32 %r1867, [%rd1+2048];
ld.global.u32 %r159, [%rd2+2048];
xor.b32 %r1857, %r159, -2147483648;
ld.global.u32 %r1865, [%rd1+4096];
ld.global.u32 %r160, [%rd2+4096];
xor.b32 %r1859, %r160, -2147483648;
ld.global.u32 %r1863, [%rd1+6144];
ld.global.u32 %r161, [%rd2+6144];
xor.b32 %r1861, %r161, -2147483648;
setp.ge.u32	%p2, %r157, %r1;
@%p2 bra BB11_4;

ld.global.u32 %r1868, [%rd1+8196];
ld.global.u32 %r162, [%rd3];
xor.b32 %r1854, %r162, -2147483648;
ld.global.u32 %r1866, [%rd1+10244];
ld.global.u32 %r163, [%rd3+2048];
xor.b32 %r1856, %r163, -2147483648;
ld.global.u32 %r1864, [%rd1+12292];
ld.global.u32 %r164, [%rd3+4096];
xor.b32 %r1858, %r164, -2147483648;
ld.global.u32 %r1862, [%rd1+14340];
ld.global.u32 %r165, [%rd3+6144];
xor.b32 %r1860, %r165, -2147483648;

BB11_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1869;
mov.b32 {blow,bhigh}, %r1868;
mov.b32 %r174, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1869;
mov.b32 {blow,bhigh}, %r1868;
mov.b32 %r177, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1867;
mov.b32 {blow,bhigh}, %r1866;
mov.b32 %r180, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1867;
mov.b32 {blow,bhigh}, %r1866;
mov.b32 %r183, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1865;
mov.b32 {blow,bhigh}, %r1864;
mov.b32 %r186, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1865;
mov.b32 {blow,bhigh}, %r1864;
mov.b32 %r189, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1863;
mov.b32 {blow,bhigh}, %r1862;
mov.b32 %r192, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1863;
mov.b32 {blow,bhigh}, %r1862;
mov.b32 %r195, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1861;
mov.b32 {blow,bhigh}, %r1860;
mov.b32 %r198, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1861;
mov.b32 {blow,bhigh}, %r1860;
mov.b32 %r201, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1859;
mov.b32 {blow,bhigh}, %r1858;
mov.b32 %r204, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1859;
mov.b32 {blow,bhigh}, %r1858;
mov.b32 %r207, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1857;
mov.b32 {blow,bhigh}, %r1856;
mov.b32 %r210, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1857;
mov.b32 {blow,bhigh}, %r1856;
mov.b32 %r213, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1855;
mov.b32 {blow,bhigh}, %r1854;
mov.b32 %r216, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1855;
mov.b32 {blow,bhigh}, %r1854;
mov.b32 %r219, {ahigh,bhigh};}


	
	{add.f16x2 %r222,%r174,%r198;
}

	
	{add.f16x2 %r225,%r177,%r201;
}

	
	{sub.f16x2 %r228,%r174,%r198;
}

	
	{sub.f16x2 %r231,%r177,%r201;
}

	
	{add.f16x2 %r234,%r186,%r210;
}

	
	{add.f16x2 %r237,%r189,%r213;
}

	
	{sub.f16x2 %r240,%r186,%r210;
}

	
	{sub.f16x2 %r243,%r189,%r213;
}

	
	{xor.b32 %r246,%r243,0x80008000;
}

	
	{add.f16x2 %r248,%r222,%r234;
}

	
	{add.f16x2 %r251,%r225,%r237;
}

	
	{sub.f16x2 %r254,%r222,%r234;
}

	
	{sub.f16x2 %r257,%r225,%r237;
}

	
	{add.f16x2 %r260,%r228,%r246;
}

	
	{add.f16x2 %r263,%r231,%r240;
}

	
	{sub.f16x2 %r266,%r228,%r246;
}

	
	{sub.f16x2 %r269,%r231,%r240;
}

	
	{add.f16x2 %r272,%r180,%r204;
}

	
	{add.f16x2 %r275,%r183,%r207;
}

	
	{sub.f16x2 %r278,%r180,%r204;
}

	
	{sub.f16x2 %r281,%r183,%r207;
}

	
	{add.f16x2 %r284,%r192,%r216;
}

	
	{add.f16x2 %r287,%r195,%r219;
}

	
	{sub.f16x2 %r290,%r192,%r216;
}

	
	{sub.f16x2 %r293,%r195,%r219;
}

	
	{xor.b32 %r296,%r293,0x80008000;
}

	
	{add.f16x2 %r298,%r272,%r284;
}

	
	{add.f16x2 %r301,%r275,%r287;
}

	
	{sub.f16x2 %r304,%r272,%r284;
}

	
	{sub.f16x2 %r307,%r275,%r287;
}

	
	{add.f16x2 %r310,%r278,%r296;
}

	
	{add.f16x2 %r313,%r281,%r290;
}

	
	{sub.f16x2 %r316,%r278,%r296;
}

	
	{sub.f16x2 %r319,%r281,%r290;
}

	mov.f32 %f13, 0f3F3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r322, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r323, {low,high};}


	mov.f32 %f11, 0fBF3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f11;
mov.b32 %r326, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r327, {low,high};}


	
	{mul.f16x2 %r336,%r310,%r322;
}

	
	{mul.f16x2 %r339,%r313,%r323;
}

	
	{sub.f16x2 %r342,%r336,%r339;
}

	
	{mul.f16x2 %r345,%r310,%r323;
}

	
	{fma.rn.f16x2 %r348,%r313,%r322,%r345;
}

	
	{xor.b32 %r352,%r307,0x80008000;
}

	
	{mul.f16x2 %r354,%r316,%r326;
}

	
	{mul.f16x2 %r357,%r319,%r327;
}

	
	{sub.f16x2 %r360,%r354,%r357;
}

	
	{mul.f16x2 %r363,%r316,%r327;
}

	
	{fma.rn.f16x2 %r366,%r319,%r326,%r363;
}

	
	{add.f16x2 %r370,%r248,%r298;
}

	
	{add.f16x2 %r373,%r251,%r301;
}

	
	{sub.f16x2 %r376,%r248,%r298;
}

	
	{sub.f16x2 %r379,%r251,%r301;
}

	
	{add.f16x2 %r382,%r260,%r342;
}

	
	{add.f16x2 %r385,%r263,%r348;
}

	
	{sub.f16x2 %r388,%r260,%r342;
}

	
	{sub.f16x2 %r391,%r263,%r348;
}

	
	{add.f16x2 %r394,%r254,%r352;
}

	
	{add.f16x2 %r397,%r257,%r304;
}

	
	{sub.f16x2 %r400,%r254,%r352;
}

	
	{sub.f16x2 %r403,%r257,%r304;
}

	
	{add.f16x2 %r406,%r266,%r360;
}

	
	{add.f16x2 %r409,%r269,%r366;
}

	
	{sub.f16x2 %r412,%r266,%r360;
}

	
	{sub.f16x2 %r415,%r269,%r366;
}

	shr.u32 %r680, %r5, 9;
shl.b32 %r56, %r680, 12;
and.b32 %r57, %r5, 511;
cvt.rn.f32.u32	%f48, %r57;
mul.f32 %f49, %f48, 0f3AC90FDB;
cos.approx.f32 %f30, %f49;
sin.approx.f32 %f50, %f49;
neg.f32 %f31, %f50;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f30;
cvt.rn.f16.f32 high, %f31;
mov.b32 %r418, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r421, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r423, {high,high};}


	
	{mul.f16x2 %r425,%r385,%r423;
}

	
	{fma.rn.f16x2 %r428,%r382,%r421,%r425;
}

	
	{mul.f16x2 %r432,%r382,%r423;
}

	
	{xor.b32 %r435,%r432,0x80008000;
}

	
	{fma.rn.f16x2 %r437,%r385,%r421,%r435;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r441, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r443, {high,high};}


	mov.f32 %f44, 0fBF800000;
mov.f32 %f45, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r445, {low,high};}


	
	{mul.f16x2 %r446,%r443,%r445;
}

	
	{mul.f16x2 %r449,%r418,%r441;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r452, {high,low};}


	
	{fma.rn.f16x2 %r454,%r446,%r452,%r449;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r454;
mov.b32 %r458, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r454;
mov.b32 %r460, {high,high};}


	
	{mul.f16x2 %r462,%r397,%r460;
}

	
	{fma.rn.f16x2 %r465,%r394,%r458,%r462;
}

	
	{mul.f16x2 %r469,%r394,%r460;
}

	
	{xor.b32 %r472,%r469,0x80008000;
}

	
	{fma.rn.f16x2 %r474,%r397,%r458,%r472;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r478, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r480, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r482, {low,high};}


	
	{mul.f16x2 %r483,%r480,%r482;
}

	
	{mul.f16x2 %r486,%r454,%r478;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r454;
mov.b32 %r489, {high,low};}


	
	{fma.rn.f16x2 %r491,%r483,%r489,%r486;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r491;
mov.b32 %r495, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r491;
mov.b32 %r497, {high,high};}


	
	{mul.f16x2 %r499,%r409,%r497;
}

	
	{fma.rn.f16x2 %r502,%r406,%r495,%r499;
}

	
	{mul.f16x2 %r506,%r406,%r497;
}

	
	{xor.b32 %r509,%r506,0x80008000;
}

	
	{fma.rn.f16x2 %r511,%r409,%r495,%r509;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r515, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r517, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r519, {low,high};}


	
	{mul.f16x2 %r520,%r517,%r519;
}

	
	{mul.f16x2 %r523,%r491,%r515;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r491;
mov.b32 %r526, {high,low};}


	
	{fma.rn.f16x2 %r528,%r520,%r526,%r523;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r528;
mov.b32 %r532, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r528;
mov.b32 %r534, {high,high};}


	
	{mul.f16x2 %r536,%r379,%r534;
}

	
	{fma.rn.f16x2 %r539,%r376,%r532,%r536;
}

	
	{mul.f16x2 %r543,%r376,%r534;
}

	
	{xor.b32 %r546,%r543,0x80008000;
}

	
	{fma.rn.f16x2 %r548,%r379,%r532,%r546;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r552, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r554, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r556, {low,high};}


	
	{mul.f16x2 %r557,%r554,%r556;
}

	
	{mul.f16x2 %r560,%r528,%r552;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r528;
mov.b32 %r563, {high,low};}


	
	{fma.rn.f16x2 %r565,%r557,%r563,%r560;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r569, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r571, {high,high};}


	
	{mul.f16x2 %r573,%r391,%r571;
}

	
	{fma.rn.f16x2 %r576,%r388,%r569,%r573;
}

	
	{mul.f16x2 %r580,%r388,%r571;
}

	
	{xor.b32 %r583,%r580,0x80008000;
}

	
	{fma.rn.f16x2 %r585,%r391,%r569,%r583;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r589, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r591, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r593, {low,high};}


	
	{mul.f16x2 %r594,%r591,%r593;
}

	
	{mul.f16x2 %r597,%r565,%r589;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r565;
mov.b32 %r600, {high,low};}


	
	{fma.rn.f16x2 %r602,%r594,%r600,%r597;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r602;
mov.b32 %r606, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r602;
mov.b32 %r608, {high,high};}


	
	{mul.f16x2 %r610,%r403,%r608;
}

	
	{fma.rn.f16x2 %r613,%r400,%r606,%r610;
}

	
	{mul.f16x2 %r617,%r400,%r608;
}

	
	{xor.b32 %r620,%r617,0x80008000;
}

	
	{fma.rn.f16x2 %r622,%r403,%r606,%r620;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r626, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r418;
mov.b32 %r628, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r630, {low,high};}


	
	{mul.f16x2 %r631,%r628,%r630;
}

	
	{mul.f16x2 %r634,%r602,%r626;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r602;
mov.b32 %r637, {high,low};}


	
	{fma.rn.f16x2 %r639,%r631,%r637,%r634;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r639;
mov.b32 %r643, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r639;
mov.b32 %r645, {high,high};}


	
	{mul.f16x2 %r647,%r415,%r645;
}

	
	{fma.rn.f16x2 %r650,%r412,%r643,%r647;
}

	
	{mul.f16x2 %r654,%r412,%r645;
}

	
	{xor.b32 %r657,%r654,0x80008000;
}

	
	{fma.rn.f16x2 %r659,%r415,%r643,%r657;
}

	shl.b32 %r681, %r680, 15;
mov.u32 %r682, smem_full;
add.s32 %r72, %r682, %r681;
barrier.sync 0;
shl.b32 %r683, %r57, 6;
add.s32 %r684, %r72, %r683;
st.shared.u32 [%r684], %r370;
st.shared.u32 [%r684+4], %r373;
st.shared.u32 [%r684+8], %r428;
st.shared.u32 [%r684+12], %r437;
st.shared.u32 [%r684+16], %r465;
st.shared.u32 [%r684+20], %r474;
st.shared.u32 [%r684+24], %r502;
st.shared.u32 [%r684+28], %r511;
st.shared.u32 [%r684+32], %r539;
st.shared.u32 [%r684+36], %r548;
st.shared.u32 [%r684+40], %r576;
st.shared.u32 [%r684+44], %r585;
st.shared.u32 [%r684+48], %r613;
st.shared.u32 [%r684+52], %r622;
st.shared.u32 [%r684+56], %r650;
st.shared.u32 [%r684+60], %r659;
barrier.sync 0;
shl.b32 %r1143, %r57, 3;
add.s32 %r1144, %r72, %r1143;
ld.shared.u32 %r686, [%r1144];
ld.shared.u32 %r689, [%r1144+4];
ld.shared.u32 %r736, [%r1144+4096];
ld.shared.u32 %r739, [%r1144+4100];
ld.shared.u32 %r698, [%r1144+8192];
ld.shared.u32 %r701, [%r1144+8196];
ld.shared.u32 %r748, [%r1144+12288];
ld.shared.u32 %r751, [%r1144+12292];
ld.shared.u32 %r687, [%r1144+16384];
ld.shared.u32 %r690, [%r1144+16388];
ld.shared.u32 %r737, [%r1144+20480];
ld.shared.u32 %r740, [%r1144+20484];
ld.shared.u32 %r699, [%r1144+24576];
ld.shared.u32 %r702, [%r1144+24580];
ld.shared.u32 %r749, [%r1144+28672];
ld.shared.u32 %r752, [%r1144+28676];

	{add.f16x2 %r685,%r686,%r687;
}

	
	{add.f16x2 %r688,%r689,%r690;
}

	
	{sub.f16x2 %r691,%r686,%r687;
}

	
	{sub.f16x2 %r694,%r689,%r690;
}

	
	{add.f16x2 %r697,%r698,%r699;
}

	
	{add.f16x2 %r700,%r701,%r702;
}

	
	{sub.f16x2 %r703,%r698,%r699;
}

	
	{sub.f16x2 %r706,%r701,%r702;
}

	
	{xor.b32 %r709,%r706,0x80008000;
}

	
	{add.f16x2 %r711,%r685,%r697;
}

	
	{add.f16x2 %r714,%r688,%r700;
}

	
	{sub.f16x2 %r717,%r685,%r697;
}

	
	{sub.f16x2 %r720,%r688,%r700;
}

	
	{add.f16x2 %r723,%r691,%r709;
}

	
	{add.f16x2 %r726,%r694,%r703;
}

	
	{sub.f16x2 %r729,%r691,%r709;
}

	
	{sub.f16x2 %r732,%r694,%r703;
}

	
	{add.f16x2 %r735,%r736,%r737;
}

	
	{add.f16x2 %r738,%r739,%r740;
}

	
	{sub.f16x2 %r741,%r736,%r737;
}

	
	{sub.f16x2 %r744,%r739,%r740;
}

	
	{add.f16x2 %r747,%r748,%r749;
}

	
	{add.f16x2 %r750,%r751,%r752;
}

	
	{sub.f16x2 %r753,%r748,%r749;
}

	
	{sub.f16x2 %r756,%r751,%r752;
}

	
	{xor.b32 %r759,%r756,0x80008000;
}

	
	{add.f16x2 %r761,%r735,%r747;
}

	
	{add.f16x2 %r764,%r738,%r750;
}

	
	{sub.f16x2 %r767,%r735,%r747;
}

	
	{sub.f16x2 %r770,%r738,%r750;
}

	
	{add.f16x2 %r773,%r741,%r759;
}

	
	{add.f16x2 %r776,%r744,%r753;
}

	
	{sub.f16x2 %r779,%r741,%r759;
}

	
	{sub.f16x2 %r782,%r744,%r753;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r785, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r786, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f11;
mov.b32 %r789, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r790, {low,high};}


	
	{mul.f16x2 %r799,%r773,%r785;
}

	
	{mul.f16x2 %r802,%r776,%r786;
}

	
	{sub.f16x2 %r805,%r799,%r802;
}

	
	{mul.f16x2 %r808,%r773,%r786;
}

	
	{fma.rn.f16x2 %r811,%r776,%r785,%r808;
}

	
	{xor.b32 %r815,%r770,0x80008000;
}

	
	{mul.f16x2 %r817,%r779,%r789;
}

	
	{mul.f16x2 %r820,%r782,%r790;
}

	
	{sub.f16x2 %r823,%r817,%r820;
}

	
	{mul.f16x2 %r826,%r779,%r790;
}

	
	{fma.rn.f16x2 %r829,%r782,%r789,%r826;
}

	
	{add.f16x2 %r833,%r711,%r761;
}

	
	{add.f16x2 %r836,%r714,%r764;
}

	
	{sub.f16x2 %r839,%r711,%r761;
}

	
	{sub.f16x2 %r842,%r714,%r764;
}

	
	{add.f16x2 %r845,%r723,%r805;
}

	
	{add.f16x2 %r848,%r726,%r811;
}

	
	{sub.f16x2 %r851,%r723,%r805;
}

	
	{sub.f16x2 %r854,%r726,%r811;
}

	
	{add.f16x2 %r857,%r717,%r815;
}

	
	{add.f16x2 %r860,%r720,%r767;
}

	
	{sub.f16x2 %r863,%r717,%r815;
}

	
	{sub.f16x2 %r866,%r720,%r767;
}

	
	{add.f16x2 %r869,%r729,%r823;
}

	
	{add.f16x2 %r872,%r732,%r829;
}

	
	{sub.f16x2 %r875,%r729,%r823;
}

	
	{sub.f16x2 %r878,%r732,%r829;
}

	and.b32 %r76, %r5, 504;
bfe.u32 %r1145, %r5, 3, 6;
and.b32 %r1146, %r5, 7;
add.s32 %r1147, %r56, %r1146;
cvt.rn.f32.u32	%f97, %r1145;
mul.f32 %f98, %f97, 0f3C490FDB;
cos.approx.f32 %f79, %f98;
sin.approx.f32 %f99, %f98;
neg.f32 %f80, %f99;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f79;
cvt.rn.f16.f32 high, %f80;
mov.b32 %r881, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r884, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r886, {high,high};}


	
	{mul.f16x2 %r888,%r848,%r886;
}

	
	{fma.rn.f16x2 %r891,%r845,%r884,%r888;
}

	
	{mul.f16x2 %r895,%r845,%r886;
}

	
	{xor.b32 %r898,%r895,0x80008000;
}

	
	{fma.rn.f16x2 %r900,%r848,%r884,%r898;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r904, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r906, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r908, {low,high};}


	
	{mul.f16x2 %r909,%r906,%r908;
}

	
	{mul.f16x2 %r912,%r881,%r904;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r915, {high,low};}


	
	{fma.rn.f16x2 %r917,%r909,%r915,%r912;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r917;
mov.b32 %r921, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r917;
mov.b32 %r923, {high,high};}


	
	{mul.f16x2 %r925,%r860,%r923;
}

	
	{fma.rn.f16x2 %r928,%r857,%r921,%r925;
}

	
	{mul.f16x2 %r932,%r857,%r923;
}

	
	{xor.b32 %r935,%r932,0x80008000;
}

	
	{fma.rn.f16x2 %r937,%r860,%r921,%r935;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r941, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r943, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r945, {low,high};}


	
	{mul.f16x2 %r946,%r943,%r945;
}

	
	{mul.f16x2 %r949,%r917,%r941;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r917;
mov.b32 %r952, {high,low};}


	
	{fma.rn.f16x2 %r954,%r946,%r952,%r949;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r954;
mov.b32 %r958, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r954;
mov.b32 %r960, {high,high};}


	
	{mul.f16x2 %r962,%r872,%r960;
}

	
	{fma.rn.f16x2 %r965,%r869,%r958,%r962;
}

	
	{mul.f16x2 %r969,%r869,%r960;
}

	
	{xor.b32 %r972,%r969,0x80008000;
}

	
	{fma.rn.f16x2 %r974,%r872,%r958,%r972;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r978, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r980, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r982, {low,high};}


	
	{mul.f16x2 %r983,%r980,%r982;
}

	
	{mul.f16x2 %r986,%r954,%r978;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r954;
mov.b32 %r989, {high,low};}


	
	{fma.rn.f16x2 %r991,%r983,%r989,%r986;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r991;
mov.b32 %r995, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r991;
mov.b32 %r997, {high,high};}


	
	{mul.f16x2 %r999,%r842,%r997;
}

	
	{fma.rn.f16x2 %r1002,%r839,%r995,%r999;
}

	
	{mul.f16x2 %r1006,%r839,%r997;
}

	
	{xor.b32 %r1009,%r1006,0x80008000;
}

	
	{fma.rn.f16x2 %r1011,%r842,%r995,%r1009;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r1015, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r1017, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1019, {low,high};}


	
	{mul.f16x2 %r1020,%r1017,%r1019;
}

	
	{mul.f16x2 %r1023,%r991,%r1015;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r991;
mov.b32 %r1026, {high,low};}


	
	{fma.rn.f16x2 %r1028,%r1020,%r1026,%r1023;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1028;
mov.b32 %r1032, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1028;
mov.b32 %r1034, {high,high};}


	
	{mul.f16x2 %r1036,%r854,%r1034;
}

	
	{fma.rn.f16x2 %r1039,%r851,%r1032,%r1036;
}

	
	{mul.f16x2 %r1043,%r851,%r1034;
}

	
	{xor.b32 %r1046,%r1043,0x80008000;
}

	
	{fma.rn.f16x2 %r1048,%r854,%r1032,%r1046;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r1052, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r1054, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1056, {low,high};}


	
	{mul.f16x2 %r1057,%r1054,%r1056;
}

	
	{mul.f16x2 %r1060,%r1028,%r1052;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1028;
mov.b32 %r1063, {high,low};}


	
	{fma.rn.f16x2 %r1065,%r1057,%r1063,%r1060;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1065;
mov.b32 %r1069, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1065;
mov.b32 %r1071, {high,high};}


	
	{mul.f16x2 %r1073,%r866,%r1071;
}

	
	{fma.rn.f16x2 %r1076,%r863,%r1069,%r1073;
}

	
	{mul.f16x2 %r1080,%r863,%r1071;
}

	
	{xor.b32 %r1083,%r1080,0x80008000;
}

	
	{fma.rn.f16x2 %r1085,%r866,%r1069,%r1083;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r1089, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r881;
mov.b32 %r1091, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1093, {low,high};}


	
	{mul.f16x2 %r1094,%r1091,%r1093;
}

	
	{mul.f16x2 %r1097,%r1065,%r1089;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1065;
mov.b32 %r1100, {high,low};}


	
	{fma.rn.f16x2 %r1102,%r1094,%r1100,%r1097;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1102;
mov.b32 %r1106, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1102;
mov.b32 %r1108, {high,high};}


	
	{mul.f16x2 %r1110,%r878,%r1108;
}

	
	{fma.rn.f16x2 %r1113,%r875,%r1106,%r1110;
}

	
	{mul.f16x2 %r1117,%r875,%r1108;
}

	
	{xor.b32 %r1120,%r1117,0x80008000;
}

	
	{fma.rn.f16x2 %r1122,%r878,%r1106,%r1120;
}

	shl.b32 %r1148, %r1147, 3;
add.s32 %r91, %r682, %r1148;
barrier.sync 0;
shl.b32 %r1150, %r76, 6;
add.s32 %r1151, %r91, %r1150;
st.shared.u32 [%r1151], %r833;
st.shared.u32 [%r1151+4], %r836;
st.shared.u32 [%r1151+64], %r891;
st.shared.u32 [%r1151+68], %r900;
st.shared.u32 [%r1151+128], %r928;
st.shared.u32 [%r1151+132], %r937;
st.shared.u32 [%r1151+192], %r965;
st.shared.u32 [%r1151+196], %r974;
st.shared.u32 [%r1151+256], %r1002;
st.shared.u32 [%r1151+260], %r1011;
st.shared.u32 [%r1151+320], %r1039;
st.shared.u32 [%r1151+324], %r1048;
st.shared.u32 [%r1151+384], %r1076;
st.shared.u32 [%r1151+388], %r1085;
st.shared.u32 [%r1151+448], %r1113;
st.shared.u32 [%r1151+452], %r1122;
barrier.sync 0;
shl.b32 %r1610, %r76, 3;
add.s32 %r1611, %r91, %r1610;
ld.shared.u32 %r1153, [%r1611];
ld.shared.u32 %r1156, [%r1611+4];
ld.shared.u32 %r1203, [%r1611+4096];
ld.shared.u32 %r1206, [%r1611+4100];
ld.shared.u32 %r1165, [%r1611+8192];
ld.shared.u32 %r1168, [%r1611+8196];
ld.shared.u32 %r1215, [%r1611+12288];
ld.shared.u32 %r1218, [%r1611+12292];
ld.shared.u32 %r1154, [%r1611+16384];
ld.shared.u32 %r1157, [%r1611+16388];
ld.shared.u32 %r1204, [%r1611+20480];
ld.shared.u32 %r1207, [%r1611+20484];
ld.shared.u32 %r1166, [%r1611+24576];
ld.shared.u32 %r1169, [%r1611+24580];
ld.shared.u32 %r1216, [%r1611+28672];
ld.shared.u32 %r1219, [%r1611+28676];

	{add.f16x2 %r1152,%r1153,%r1154;
}

	
	{add.f16x2 %r1155,%r1156,%r1157;
}

	
	{sub.f16x2 %r1158,%r1153,%r1154;
}

	
	{sub.f16x2 %r1161,%r1156,%r1157;
}

	
	{add.f16x2 %r1164,%r1165,%r1166;
}

	
	{add.f16x2 %r1167,%r1168,%r1169;
}

	
	{sub.f16x2 %r1170,%r1165,%r1166;
}

	
	{sub.f16x2 %r1173,%r1168,%r1169;
}

	
	{xor.b32 %r1176,%r1173,0x80008000;
}

	
	{add.f16x2 %r1178,%r1152,%r1164;
}

	
	{add.f16x2 %r1181,%r1155,%r1167;
}

	
	{sub.f16x2 %r1184,%r1152,%r1164;
}

	
	{sub.f16x2 %r1187,%r1155,%r1167;
}

	
	{add.f16x2 %r1190,%r1158,%r1176;
}

	
	{add.f16x2 %r1193,%r1161,%r1170;
}

	
	{sub.f16x2 %r1196,%r1158,%r1176;
}

	
	{sub.f16x2 %r1199,%r1161,%r1170;
}

	
	{add.f16x2 %r1202,%r1203,%r1204;
}

	
	{add.f16x2 %r1205,%r1206,%r1207;
}

	
	{sub.f16x2 %r1208,%r1203,%r1204;
}

	
	{sub.f16x2 %r1211,%r1206,%r1207;
}

	
	{add.f16x2 %r1214,%r1215,%r1216;
}

	
	{add.f16x2 %r1217,%r1218,%r1219;
}

	
	{sub.f16x2 %r1220,%r1215,%r1216;
}

	
	{sub.f16x2 %r1223,%r1218,%r1219;
}

	
	{xor.b32 %r1226,%r1223,0x80008000;
}

	
	{add.f16x2 %r1228,%r1202,%r1214;
}

	
	{add.f16x2 %r1231,%r1205,%r1217;
}

	
	{sub.f16x2 %r1234,%r1202,%r1214;
}

	
	{sub.f16x2 %r1237,%r1205,%r1217;
}

	
	{add.f16x2 %r1240,%r1208,%r1226;
}

	
	{add.f16x2 %r1243,%r1211,%r1220;
}

	
	{sub.f16x2 %r1246,%r1208,%r1226;
}

	
	{sub.f16x2 %r1249,%r1211,%r1220;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1252, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1253, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f11;
mov.b32 %r1256, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1257, {low,high};}


	
	{mul.f16x2 %r1266,%r1240,%r1252;
}

	
	{mul.f16x2 %r1269,%r1243,%r1253;
}

	
	{sub.f16x2 %r1272,%r1266,%r1269;
}

	
	{mul.f16x2 %r1275,%r1240,%r1253;
}

	
	{fma.rn.f16x2 %r1278,%r1243,%r1252,%r1275;
}

	
	{xor.b32 %r1282,%r1237,0x80008000;
}

	
	{mul.f16x2 %r1284,%r1246,%r1256;
}

	
	{mul.f16x2 %r1287,%r1249,%r1257;
}

	
	{sub.f16x2 %r1290,%r1284,%r1287;
}

	
	{mul.f16x2 %r1293,%r1246,%r1257;
}

	
	{fma.rn.f16x2 %r1296,%r1249,%r1256,%r1293;
}

	
	{add.f16x2 %r1300,%r1178,%r1228;
}

	
	{add.f16x2 %r1303,%r1181,%r1231;
}

	
	{sub.f16x2 %r1306,%r1178,%r1228;
}

	
	{sub.f16x2 %r1309,%r1181,%r1231;
}

	
	{add.f16x2 %r1312,%r1190,%r1272;
}

	
	{add.f16x2 %r1315,%r1193,%r1278;
}

	
	{sub.f16x2 %r1318,%r1190,%r1272;
}

	
	{sub.f16x2 %r1321,%r1193,%r1278;
}

	
	{add.f16x2 %r1324,%r1184,%r1282;
}

	
	{add.f16x2 %r1327,%r1187,%r1234;
}

	
	{sub.f16x2 %r1330,%r1184,%r1282;
}

	
	{sub.f16x2 %r1333,%r1187,%r1234;
}

	
	{add.f16x2 %r1336,%r1196,%r1290;
}

	
	{add.f16x2 %r1339,%r1199,%r1296;
}

	
	{sub.f16x2 %r1342,%r1196,%r1290;
}

	
	{sub.f16x2 %r1345,%r1199,%r1296;
}

	and.b32 %r95, %r5, 448;
bfe.u32 %r1612, %r5, 6, 3;
and.b32 %r1613, %r5, 63;
add.s32 %r1614, %r56, %r1613;
cvt.rn.f32.u32	%f146, %r1612;
mul.f32 %f147, %f146, 0f3DC90FDB;
cos.approx.f32 %f128, %f147;
sin.approx.f32 %f148, %f147;
neg.f32 %f129, %f148;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f128;
cvt.rn.f16.f32 high, %f129;
mov.b32 %r1348, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1351, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1353, {high,high};}


	
	{mul.f16x2 %r1355,%r1315,%r1353;
}

	
	{fma.rn.f16x2 %r1358,%r1312,%r1351,%r1355;
}

	
	{mul.f16x2 %r1362,%r1312,%r1353;
}

	
	{xor.b32 %r1365,%r1362,0x80008000;
}

	
	{fma.rn.f16x2 %r1367,%r1315,%r1351,%r1365;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1371, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1373, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1375, {low,high};}


	
	{mul.f16x2 %r1376,%r1373,%r1375;
}

	
	{mul.f16x2 %r1379,%r1348,%r1371;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1382, {high,low};}


	
	{fma.rn.f16x2 %r1384,%r1376,%r1382,%r1379;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1384;
mov.b32 %r1388, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1384;
mov.b32 %r1390, {high,high};}


	
	{mul.f16x2 %r1392,%r1327,%r1390;
}

	
	{fma.rn.f16x2 %r1395,%r1324,%r1388,%r1392;
}

	
	{mul.f16x2 %r1399,%r1324,%r1390;
}

	
	{xor.b32 %r1402,%r1399,0x80008000;
}

	
	{fma.rn.f16x2 %r1404,%r1327,%r1388,%r1402;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1408, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1410, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1412, {low,high};}


	
	{mul.f16x2 %r1413,%r1410,%r1412;
}

	
	{mul.f16x2 %r1416,%r1384,%r1408;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1384;
mov.b32 %r1419, {high,low};}


	
	{fma.rn.f16x2 %r1421,%r1413,%r1419,%r1416;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1421;
mov.b32 %r1425, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1421;
mov.b32 %r1427, {high,high};}


	
	{mul.f16x2 %r1429,%r1339,%r1427;
}

	
	{fma.rn.f16x2 %r1432,%r1336,%r1425,%r1429;
}

	
	{mul.f16x2 %r1436,%r1336,%r1427;
}

	
	{xor.b32 %r1439,%r1436,0x80008000;
}

	
	{fma.rn.f16x2 %r1441,%r1339,%r1425,%r1439;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1445, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1447, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1449, {low,high};}


	
	{mul.f16x2 %r1450,%r1447,%r1449;
}

	
	{mul.f16x2 %r1453,%r1421,%r1445;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1421;
mov.b32 %r1456, {high,low};}


	
	{fma.rn.f16x2 %r1458,%r1450,%r1456,%r1453;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1458;
mov.b32 %r1462, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1458;
mov.b32 %r1464, {high,high};}


	
	{mul.f16x2 %r1466,%r1309,%r1464;
}

	
	{fma.rn.f16x2 %r1469,%r1306,%r1462,%r1466;
}

	
	{mul.f16x2 %r1473,%r1306,%r1464;
}

	
	{xor.b32 %r1476,%r1473,0x80008000;
}

	
	{fma.rn.f16x2 %r1478,%r1309,%r1462,%r1476;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1482, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1484, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1486, {low,high};}


	
	{mul.f16x2 %r1487,%r1484,%r1486;
}

	
	{mul.f16x2 %r1490,%r1458,%r1482;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1458;
mov.b32 %r1493, {high,low};}


	
	{fma.rn.f16x2 %r1495,%r1487,%r1493,%r1490;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1495;
mov.b32 %r1499, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1495;
mov.b32 %r1501, {high,high};}


	
	{mul.f16x2 %r1503,%r1321,%r1501;
}

	
	{fma.rn.f16x2 %r1506,%r1318,%r1499,%r1503;
}

	
	{mul.f16x2 %r1510,%r1318,%r1501;
}

	
	{xor.b32 %r1513,%r1510,0x80008000;
}

	
	{fma.rn.f16x2 %r1515,%r1321,%r1499,%r1513;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1519, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1521, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1523, {low,high};}


	
	{mul.f16x2 %r1524,%r1521,%r1523;
}

	
	{mul.f16x2 %r1527,%r1495,%r1519;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1495;
mov.b32 %r1530, {high,low};}


	
	{fma.rn.f16x2 %r1532,%r1524,%r1530,%r1527;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1532;
mov.b32 %r1536, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1532;
mov.b32 %r1538, {high,high};}


	
	{mul.f16x2 %r1540,%r1333,%r1538;
}

	
	{fma.rn.f16x2 %r1543,%r1330,%r1536,%r1540;
}

	
	{mul.f16x2 %r1547,%r1330,%r1538;
}

	
	{xor.b32 %r1550,%r1547,0x80008000;
}

	
	{fma.rn.f16x2 %r1552,%r1333,%r1536,%r1550;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1556, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1348;
mov.b32 %r1558, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1560, {low,high};}


	
	{mul.f16x2 %r1561,%r1558,%r1560;
}

	
	{mul.f16x2 %r1564,%r1532,%r1556;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1532;
mov.b32 %r1567, {high,low};}


	
	{fma.rn.f16x2 %r1569,%r1561,%r1567,%r1564;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1569;
mov.b32 %r1573, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1569;
mov.b32 %r1575, {high,high};}


	
	{mul.f16x2 %r1577,%r1345,%r1575;
}

	
	{fma.rn.f16x2 %r1580,%r1342,%r1573,%r1577;
}

	
	{mul.f16x2 %r1584,%r1342,%r1575;
}

	
	{xor.b32 %r1587,%r1584,0x80008000;
}

	
	{fma.rn.f16x2 %r1589,%r1345,%r1573,%r1587;
}

	shl.b32 %r1615, %r1614, 3;
add.s32 %r110, %r682, %r1615;
barrier.sync 0;
shl.b32 %r1617, %r95, 6;
add.s32 %r1618, %r110, %r1617;
st.shared.u32 [%r1618], %r1300;
st.shared.u32 [%r1618+4], %r1303;
st.shared.u32 [%r1618+512], %r1358;
st.shared.u32 [%r1618+516], %r1367;
st.shared.u32 [%r1618+1024], %r1395;
st.shared.u32 [%r1618+1028], %r1404;
st.shared.u32 [%r1618+1536], %r1432;
st.shared.u32 [%r1618+1540], %r1441;
st.shared.u32 [%r1618+2048], %r1469;
st.shared.u32 [%r1618+2052], %r1478;
st.shared.u32 [%r1618+2560], %r1506;
st.shared.u32 [%r1618+2564], %r1515;
st.shared.u32 [%r1618+3072], %r1543;
st.shared.u32 [%r1618+3076], %r1552;
st.shared.u32 [%r1618+3584], %r1580;
st.shared.u32 [%r1618+3588], %r1589;
barrier.sync 0;
shl.b32 %r1815, %r95, 3;
add.s32 %r1816, %r110, %r1815;
ld.shared.u32 %r1620, [%r1816];
ld.shared.u32 %r1670, [%r1816+4096];
ld.shared.u32 %r1673, [%r1816+4100];
ld.shared.u32 %r1632, [%r1816+8192];
ld.shared.u32 %r1641, [%r1816+8196];
ld.shared.u32 %r1682, [%r1816+12288];
ld.shared.u32 %r1685, [%r1816+12292];
ld.shared.u32 %r1621, [%r1816+16384];
ld.shared.u32 %r1671, [%r1816+20480];
ld.shared.u32 %r1674, [%r1816+20484];
ld.shared.u32 %r1633, [%r1816+24576];
ld.shared.u32 %r1642, [%r1816+24580];
ld.shared.u32 %r1683, [%r1816+28672];
ld.shared.u32 %r1686, [%r1816+28676];

	{add.f16x2 %r1619,%r1620,%r1621;
}

	
	{sub.f16x2 %r1625,%r1620,%r1621;
}

	
	{add.f16x2 %r1631,%r1632,%r1633;
}

	
	{sub.f16x2 %r1640,%r1641,%r1642;
}

	
	{xor.b32 %r1643,%r1640,0x80008000;
}

	
	{add.f16x2 %r1645,%r1619,%r1631;
}

	
	{sub.f16x2 %r1651,%r1619,%r1631;
}

	
	{add.f16x2 %r1657,%r1625,%r1643;
}

	
	{sub.f16x2 %r1663,%r1625,%r1643;
}

	
	{add.f16x2 %r1669,%r1670,%r1671;
}

	
	{add.f16x2 %r1672,%r1673,%r1674;
}

	
	{sub.f16x2 %r1675,%r1670,%r1671;
}

	
	{sub.f16x2 %r1678,%r1673,%r1674;
}

	
	{add.f16x2 %r1681,%r1682,%r1683;
}

	
	{add.f16x2 %r1684,%r1685,%r1686;
}

	
	{sub.f16x2 %r1687,%r1682,%r1683;
}

	
	{sub.f16x2 %r1690,%r1685,%r1686;
}

	
	{xor.b32 %r1693,%r1690,0x80008000;
}

	
	{add.f16x2 %r1695,%r1669,%r1681;
}

	
	{sub.f16x2 %r1704,%r1672,%r1684;
}

	
	{add.f16x2 %r1707,%r1675,%r1693;
}

	
	{add.f16x2 %r1710,%r1678,%r1687;
}

	
	{sub.f16x2 %r1713,%r1675,%r1693;
}

	
	{sub.f16x2 %r1716,%r1678,%r1687;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1719, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1720, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f11;
mov.b32 %r1723, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1724, {low,high};}


	
	{mul.f16x2 %r1733,%r1707,%r1719;
}

	
	{mul.f16x2 %r1736,%r1710,%r1720;
}

	
	{sub.f16x2 %r1739,%r1733,%r1736;
}

	
	{xor.b32 %r1749,%r1704,0x80008000;
}

	
	{mul.f16x2 %r1751,%r1713,%r1723;
}

	
	{mul.f16x2 %r1754,%r1716,%r1724;
}

	
	{sub.f16x2 %r1757,%r1751,%r1754;
}

	
	{add.f16x2 %r1767,%r1645,%r1695;
}

	
	{sub.f16x2 %r1773,%r1645,%r1695;
}

	
	{add.f16x2 %r1779,%r1657,%r1739;
}

	
	{sub.f16x2 %r1785,%r1657,%r1739;
}

	
	{add.f16x2 %r1791,%r1651,%r1749;
}

	
	{sub.f16x2 %r1797,%r1651,%r1749;
}

	
	{add.f16x2 %r1803,%r1663,%r1757;
}

	
	{sub.f16x2 %r1809,%r1663,%r1757;
}

	barrier.sync 0;
shl.b32 %r1817, %r5, 2;
add.s32 %r1819, %r682, %r1817;
st.shared.u32 [%r1819], %r1767;
st.shared.u32 [%r1819+2048], %r1779;
st.shared.u32 [%r1819+4096], %r1791;
st.shared.u32 [%r1819+6144], %r1803;
st.shared.u32 [%r1819+8192], %r1773;
st.shared.u32 [%r1819+10240], %r1785;
st.shared.u32 [%r1819+12288], %r1797;
st.shared.u32 [%r1819+14336], %r1809;
barrier.sync 0;
shl.b32 %r1844, %r5, 3;
add.s32 %r1846, %r682, %r1844;
ld.shared.u32 %r1821, [%r1846];
ld.shared.u32 %r1822, [%r1846+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1821;
mov.b32 {blow,bhigh}, %r1822;
mov.b32 %r1820, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1821;
mov.b32 {blow,bhigh}, %r1822;
mov.b32 %r1823, {ahigh,bhigh};}


	ld.shared.u32 %r1827, [%r1846+4096];
ld.shared.u32 %r1828, [%r1846+4100];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1827;
mov.b32 {blow,bhigh}, %r1828;
mov.b32 %r1826, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1827;
mov.b32 {blow,bhigh}, %r1828;
mov.b32 %r1829, {ahigh,bhigh};}


	ld.shared.u32 %r1833, [%r1846+8192];
ld.shared.u32 %r1834, [%r1846+8196];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1833;
mov.b32 {blow,bhigh}, %r1834;
mov.b32 %r1832, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1833;
mov.b32 {blow,bhigh}, %r1834;
mov.b32 %r1835, {ahigh,bhigh};}


	ld.shared.u32 %r1839, [%r1846+12288];
ld.shared.u32 %r1840, [%r1846+12292];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1839;
mov.b32 {blow,bhigh}, %r1840;
mov.b32 %r1838, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1839;
mov.b32 {blow,bhigh}, %r1840;
mov.b32 %r1841, {ahigh,bhigh};}


	setp.eq.s16	%p3, %rs1, 0;
selp.b32	%r1847, 2048, 2049, %p3;
shl.b32 %r1848, %r2, 1;
add.s32 %r1849, %r1848, 1;
mad.lo.s32 %r1850, %r1848, %r1847, %r5;
cvta.to.global.u64 %rd12, %rd7;
mul.wide.u32 %rd13, %r1850, 4;
add.s64 %rd4, %rd12, %rd13;
mad.lo.s32 %r1851, %r1849, %r1847, %r5;
mul.wide.u32 %rd14, %r1851, 4;
add.s64 %rd5, %rd12, %rd14;
@%p1 bra BB11_6;
bra.uni BB11_5;

BB11_6:
st.global.u32 [%rd4], %r1820;
st.global.u32 [%rd4+2048], %r1826;
st.global.u32 [%rd4+4096], %r1832;
st.global.u32 [%rd4+6144], %r1838;
bra.uni BB11_7;

BB11_5:
shl.b32 %r1852, %r3, 1;
add.s32 %r1853, %r1852, -1;
st.global.u32 [%rd4], %r1820;
st.global.u32 [%rd4+2048], %r1826;
st.global.u32 [%rd4+4096], %r1832;
st.global.u32 [%rd4+6144], %r1838;
setp.ge.u32	%p5, %r1853, %r1;
@%p5 bra BB11_8;

BB11_7:
st.global.u32 [%rd5], %r1823;
st.global.u32 [%rd5+2048], %r1829;
st.global.u32 [%rd5+4096], %r1835;
st.global.u32 [%rd5+6144], %r1841;

BB11_8:
ret;
}


.weak .entry _Z14vector_fft_c2rILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_c2rILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 1024, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<10>;
.reg .f32 %f<198>;
.reg .b32 %r<2258>;
.reg .f64 %fd<2>;
.reg .b64 %rd<19>;


ld.param.u32 %r1, [_Z14vector_fft_c2rILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd7, [_Z14vector_fft_c2rILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd6, [_Z14vector_fft_c2rILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_c2rILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r204, %ctaid.x;
mul.lo.s32 %r2, %r204, 8194;
mov.u32 %r3, %nctaid.x;
add.s32 %r205, %r3, -1;
setp.lt.u32	%p2, %r204, %r205;
mov.u32 %r206, %tid.x;
add.s32 %r207, %r2, %r206;
mov.u32 %r208, 1024;
sub.s32 %r4, %r208, %r206;
add.s32 %r209, %r2, %r4;
cvta.to.global.u64 %rd1, %rd6;
mul.wide.u32 %rd8, %r207, 4;
add.s64 %rd2, %rd1, %rd8;
mul.wide.u32 %rd9, %r209, 4;
add.s64 %rd3, %rd1, %rd9;
@%p2 bra BB12_3;
bra.uni BB12_1;

BB12_3:
add.s32 %r230, %r209, 4097;
mul.wide.u32 %rd14, %r230, 4;
add.s64 %rd15, %rd1, %rd14;
ld.global.u32 %r2257, [%rd2];
ld.global.u32 %r231, [%rd3];
xor.b32 %r2243, %r231, -2147483648;
ld.global.u32 %r2255, [%rd2+4096];
ld.global.u32 %r232, [%rd3+4096];
xor.b32 %r2245, %r232, -2147483648;
ld.global.u32 %r2253, [%rd2+8192];
ld.global.u32 %r233, [%rd3+8192];
xor.b32 %r2247, %r233, -2147483648;
ld.global.u32 %r2251, [%rd2+12288];
ld.global.u32 %r234, [%rd3+12288];
xor.b32 %r2249, %r234, -2147483648;
ld.global.u32 %r2256, [%rd2+16388];
ld.global.u32 %r235, [%rd15];
xor.b32 %r2242, %r235, -2147483648;
ld.global.u32 %r2254, [%rd2+20484];
ld.global.u32 %r236, [%rd15+4096];
xor.b32 %r2244, %r236, -2147483648;
ld.global.u32 %r2252, [%rd2+24580];
ld.global.u32 %r237, [%rd15+8192];
xor.b32 %r2246, %r237, -2147483648;
ld.global.u32 %r2250, [%rd2+28676];
ld.global.u32 %r238, [%rd15+12288];
xor.b32 %r2248, %r238, -2147483648;
bra.uni BB12_4;

BB12_1:
shl.b32 %r211, %r3, 1;
add.s32 %r212, %r211, -1;
ld.global.u32 %r2257, [%rd2];
ld.global.u32 %r213, [%rd3];
xor.b32 %r2243, %r213, -2147483648;
ld.global.u32 %r2255, [%rd2+4096];
ld.global.u32 %r214, [%rd3+4096];
xor.b32 %r2245, %r214, -2147483648;
ld.global.u32 %r2253, [%rd2+8192];
ld.global.u32 %r215, [%rd3+8192];
xor.b32 %r2247, %r215, -2147483648;
ld.global.u32 %r2251, [%rd2+12288];
ld.global.u32 %r216, [%rd3+12288];
xor.b32 %r2249, %r216, -2147483648;
setp.ge.u32	%p3, %r212, %r1;
@%p3 bra BB12_4;

ld.global.u32 %r2256, [%rd2+16388];
add.s32 %r224, %r209, 4097;
mul.wide.u32 %rd12, %r224, 4;
add.s64 %rd13, %rd1, %rd12;
ld.global.u32 %r225, [%rd13];
xor.b32 %r2242, %r225, -2147483648;
ld.global.u32 %r2254, [%rd2+20484];
ld.global.u32 %r226, [%rd13+4096];
xor.b32 %r2244, %r226, -2147483648;
ld.global.u32 %r2252, [%rd2+24580];
ld.global.u32 %r227, [%rd13+8192];
xor.b32 %r2246, %r227, -2147483648;
ld.global.u32 %r2250, [%rd2+28676];
ld.global.u32 %r228, [%rd13+12288];
xor.b32 %r2248, %r228, -2147483648;

BB12_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2257;
mov.b32 {blow,bhigh}, %r2256;
mov.b32 %r239, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2257;
mov.b32 {blow,bhigh}, %r2256;
mov.b32 %r242, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2255;
mov.b32 {blow,bhigh}, %r2254;
mov.b32 %r245, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2255;
mov.b32 {blow,bhigh}, %r2254;
mov.b32 %r248, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2253;
mov.b32 {blow,bhigh}, %r2252;
mov.b32 %r251, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2253;
mov.b32 {blow,bhigh}, %r2252;
mov.b32 %r254, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2251;
mov.b32 {blow,bhigh}, %r2250;
mov.b32 %r257, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2251;
mov.b32 {blow,bhigh}, %r2250;
mov.b32 %r260, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2249;
mov.b32 {blow,bhigh}, %r2248;
mov.b32 %r263, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2249;
mov.b32 {blow,bhigh}, %r2248;
mov.b32 %r266, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2247;
mov.b32 {blow,bhigh}, %r2246;
mov.b32 %r269, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2247;
mov.b32 {blow,bhigh}, %r2246;
mov.b32 %r272, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2245;
mov.b32 {blow,bhigh}, %r2244;
mov.b32 %r275, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2245;
mov.b32 {blow,bhigh}, %r2244;
mov.b32 %r278, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2243;
mov.b32 {blow,bhigh}, %r2242;
mov.b32 %r281, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2243;
mov.b32 {blow,bhigh}, %r2242;
mov.b32 %r284, {ahigh,bhigh};}


	
	{add.f16x2 %r287,%r239,%r263;
}

	
	{add.f16x2 %r290,%r242,%r266;
}

	
	{sub.f16x2 %r293,%r239,%r263;
}

	
	{sub.f16x2 %r296,%r242,%r266;
}

	
	{add.f16x2 %r299,%r251,%r275;
}

	
	{add.f16x2 %r302,%r254,%r278;
}

	
	{sub.f16x2 %r305,%r251,%r275;
}

	
	{sub.f16x2 %r308,%r254,%r278;
}

	
	{xor.b32 %r311,%r308,0x80008000;
}

	
	{add.f16x2 %r313,%r287,%r299;
}

	
	{add.f16x2 %r316,%r290,%r302;
}

	
	{sub.f16x2 %r319,%r287,%r299;
}

	
	{sub.f16x2 %r322,%r290,%r302;
}

	
	{add.f16x2 %r325,%r293,%r311;
}

	
	{add.f16x2 %r328,%r296,%r305;
}

	
	{sub.f16x2 %r331,%r293,%r311;
}

	
	{sub.f16x2 %r334,%r296,%r305;
}

	
	{add.f16x2 %r337,%r245,%r269;
}

	
	{add.f16x2 %r340,%r248,%r272;
}

	
	{sub.f16x2 %r343,%r245,%r269;
}

	
	{sub.f16x2 %r346,%r248,%r272;
}

	
	{add.f16x2 %r349,%r257,%r281;
}

	
	{add.f16x2 %r352,%r260,%r284;
}

	
	{sub.f16x2 %r355,%r257,%r281;
}

	
	{sub.f16x2 %r358,%r260,%r284;
}

	
	{xor.b32 %r361,%r358,0x80008000;
}

	
	{add.f16x2 %r363,%r337,%r349;
}

	
	{add.f16x2 %r366,%r340,%r352;
}

	
	{sub.f16x2 %r369,%r337,%r349;
}

	
	{sub.f16x2 %r372,%r340,%r352;
}

	
	{add.f16x2 %r375,%r343,%r361;
}

	
	{add.f16x2 %r378,%r346,%r355;
}

	
	{sub.f16x2 %r381,%r343,%r361;
}

	
	{sub.f16x2 %r384,%r346,%r355;
}

	mov.f32 %f13, 0f3F3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r387, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r388, {low,high};}


	mov.f32 %f11, 0fBF3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f11;
mov.b32 %r391, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r392, {low,high};}


	
	{mul.f16x2 %r401,%r375,%r387;
}

	
	{mul.f16x2 %r404,%r378,%r388;
}

	
	{sub.f16x2 %r407,%r401,%r404;
}

	
	{mul.f16x2 %r410,%r375,%r388;
}

	
	{fma.rn.f16x2 %r413,%r378,%r387,%r410;
}

	
	{xor.b32 %r417,%r372,0x80008000;
}

	
	{mul.f16x2 %r419,%r381,%r391;
}

	
	{mul.f16x2 %r422,%r384,%r392;
}

	
	{sub.f16x2 %r425,%r419,%r422;
}

	
	{mul.f16x2 %r428,%r381,%r392;
}

	
	{fma.rn.f16x2 %r431,%r384,%r391,%r428;
}

	
	{add.f16x2 %r435,%r313,%r363;
}

	
	{add.f16x2 %r438,%r316,%r366;
}

	
	{sub.f16x2 %r441,%r313,%r363;
}

	
	{sub.f16x2 %r444,%r316,%r366;
}

	
	{add.f16x2 %r447,%r325,%r407;
}

	
	{add.f16x2 %r450,%r328,%r413;
}

	
	{sub.f16x2 %r453,%r325,%r407;
}

	
	{sub.f16x2 %r456,%r328,%r413;
}

	
	{add.f16x2 %r459,%r319,%r417;
}

	
	{add.f16x2 %r462,%r322,%r369;
}

	
	{sub.f16x2 %r465,%r319,%r417;
}

	
	{sub.f16x2 %r468,%r322,%r369;
}

	
	{add.f16x2 %r471,%r331,%r425;
}

	
	{add.f16x2 %r474,%r334,%r431;
}

	
	{sub.f16x2 %r477,%r331,%r425;
}

	
	{sub.f16x2 %r480,%r334,%r431;
}

	and.b32 %r57, %r206, 1023;
cvt.rn.f32.u32	%f48, %r57;
mul.f32 %f49, %f48, 0f3A490FDB;
cos.approx.f32 %f30, %f49;
sin.approx.f32 %f50, %f49;
neg.f32 %f31, %f50;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f30;
cvt.rn.f16.f32 high, %f31;
mov.b32 %r483, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r486, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r488, {high,high};}


	
	{mul.f16x2 %r490,%r450,%r488;
}

	
	{fma.rn.f16x2 %r493,%r447,%r486,%r490;
}

	
	{mul.f16x2 %r497,%r447,%r488;
}

	
	{xor.b32 %r500,%r497,0x80008000;
}

	
	{fma.rn.f16x2 %r502,%r450,%r486,%r500;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r506, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r508, {high,high};}


	mov.f32 %f44, 0fBF800000;
mov.f32 %f45, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r510, {low,high};}


	
	{mul.f16x2 %r511,%r508,%r510;
}

	
	{mul.f16x2 %r514,%r483,%r506;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r517, {high,low};}


	
	{fma.rn.f16x2 %r519,%r511,%r517,%r514;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r523, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r525, {high,high};}


	
	{mul.f16x2 %r527,%r462,%r525;
}

	
	{fma.rn.f16x2 %r530,%r459,%r523,%r527;
}

	
	{mul.f16x2 %r534,%r459,%r525;
}

	
	{xor.b32 %r537,%r534,0x80008000;
}

	
	{fma.rn.f16x2 %r539,%r462,%r523,%r537;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r543, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r545, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r547, {low,high};}


	
	{mul.f16x2 %r548,%r545,%r547;
}

	
	{mul.f16x2 %r551,%r519,%r543;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r554, {high,low};}


	
	{fma.rn.f16x2 %r556,%r548,%r554,%r551;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r556;
mov.b32 %r560, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r556;
mov.b32 %r562, {high,high};}


	
	{mul.f16x2 %r564,%r474,%r562;
}

	
	{fma.rn.f16x2 %r567,%r471,%r560,%r564;
}

	
	{mul.f16x2 %r571,%r471,%r562;
}

	
	{xor.b32 %r574,%r571,0x80008000;
}

	
	{fma.rn.f16x2 %r576,%r474,%r560,%r574;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r580, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r582, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r584, {low,high};}


	
	{mul.f16x2 %r585,%r582,%r584;
}

	
	{mul.f16x2 %r588,%r556,%r580;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r556;
mov.b32 %r591, {high,low};}


	
	{fma.rn.f16x2 %r593,%r585,%r591,%r588;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r593;
mov.b32 %r597, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r593;
mov.b32 %r599, {high,high};}


	
	{mul.f16x2 %r601,%r444,%r599;
}

	
	{fma.rn.f16x2 %r604,%r441,%r597,%r601;
}

	
	{mul.f16x2 %r608,%r441,%r599;
}

	
	{xor.b32 %r611,%r608,0x80008000;
}

	
	{fma.rn.f16x2 %r613,%r444,%r597,%r611;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r617, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r619, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r621, {low,high};}


	
	{mul.f16x2 %r622,%r619,%r621;
}

	
	{mul.f16x2 %r625,%r593,%r617;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r593;
mov.b32 %r628, {high,low};}


	
	{fma.rn.f16x2 %r630,%r622,%r628,%r625;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r630;
mov.b32 %r634, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r630;
mov.b32 %r636, {high,high};}


	
	{mul.f16x2 %r638,%r456,%r636;
}

	
	{fma.rn.f16x2 %r641,%r453,%r634,%r638;
}

	
	{mul.f16x2 %r645,%r453,%r636;
}

	
	{xor.b32 %r648,%r645,0x80008000;
}

	
	{fma.rn.f16x2 %r650,%r456,%r634,%r648;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r654, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r656, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r658, {low,high};}


	
	{mul.f16x2 %r659,%r656,%r658;
}

	
	{mul.f16x2 %r662,%r630,%r654;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r630;
mov.b32 %r665, {high,low};}


	
	{fma.rn.f16x2 %r667,%r659,%r665,%r662;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r667;
mov.b32 %r671, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r667;
mov.b32 %r673, {high,high};}


	
	{mul.f16x2 %r675,%r468,%r673;
}

	
	{fma.rn.f16x2 %r678,%r465,%r671,%r675;
}

	
	{mul.f16x2 %r682,%r465,%r673;
}

	
	{xor.b32 %r685,%r682,0x80008000;
}

	
	{fma.rn.f16x2 %r687,%r468,%r671,%r685;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r691, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r483;
mov.b32 %r693, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r695, {low,high};}


	
	{mul.f16x2 %r696,%r693,%r695;
}

	
	{mul.f16x2 %r699,%r667,%r691;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r667;
mov.b32 %r702, {high,low};}


	
	{fma.rn.f16x2 %r704,%r696,%r702,%r699;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r708, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r710, {high,high};}


	
	{mul.f16x2 %r712,%r480,%r710;
}

	
	{fma.rn.f16x2 %r715,%r477,%r708,%r712;
}

	
	{mul.f16x2 %r719,%r477,%r710;
}

	
	{xor.b32 %r722,%r719,0x80008000;
}

	
	{fma.rn.f16x2 %r724,%r480,%r708,%r722;
}

	shl.b32 %r747, %r206, 3;
and.b32 %r72, %r747, -8192;
barrier.sync 0;
shl.b32 %r748, %r57, 3;
add.s32 %r749, %r748, %r72;
shl.b32 %r750, %r749, 2;
mov.u32 %r751, smem_full;
add.s32 %r73, %r751, %r750;
st.shared.u32 [%r73], %r435;
st.shared.u32 [%r73+4], %r493;
st.shared.u32 [%r73+8], %r530;
st.shared.u32 [%r73+12], %r567;
st.shared.u32 [%r73+16], %r604;
st.shared.u32 [%r73+20], %r641;
st.shared.u32 [%r73+24], %r678;
st.shared.u32 [%r73+28], %r715;
barrier.sync 0;
add.s32 %r752, %r57, %r72;
shl.b32 %r753, %r752, 2;
add.s32 %r74, %r751, %r753;
ld.shared.u32 %r75, [%r74];
ld.shared.u32 %r76, [%r74+4096];
ld.shared.u32 %r77, [%r74+8192];
ld.shared.u32 %r78, [%r74+12288];
ld.shared.u32 %r79, [%r74+16384];
ld.shared.u32 %r80, [%r74+20480];
ld.shared.u32 %r81, [%r74+24576];
ld.shared.u32 %r82, [%r74+28672];
barrier.sync 0;
st.shared.u32 [%r73], %r438;
st.shared.u32 [%r73+4], %r502;
st.shared.u32 [%r73+8], %r539;
st.shared.u32 [%r73+12], %r576;
st.shared.u32 [%r73+16], %r613;
st.shared.u32 [%r73+20], %r650;
st.shared.u32 [%r73+24], %r687;
st.shared.u32 [%r73+28], %r724;
barrier.sync 0;
ld.shared.u32 %r759, [%r74];
ld.shared.u32 %r809, [%r74+4096];
ld.shared.u32 %r771, [%r74+8192];
ld.shared.u32 %r821, [%r74+12288];
ld.shared.u32 %r760, [%r74+16384];
ld.shared.u32 %r810, [%r74+20480];
ld.shared.u32 %r772, [%r74+24576];
ld.shared.u32 %r822, [%r74+28672];

	{add.f16x2 %r755,%r75,%r79;
}

	
	{add.f16x2 %r758,%r759,%r760;
}

	
	{sub.f16x2 %r761,%r75,%r79;
}

	
	{sub.f16x2 %r764,%r759,%r760;
}

	
	{add.f16x2 %r767,%r77,%r81;
}

	
	{add.f16x2 %r770,%r771,%r772;
}

	
	{sub.f16x2 %r773,%r77,%r81;
}

	
	{sub.f16x2 %r776,%r771,%r772;
}

	
	{xor.b32 %r779,%r776,0x80008000;
}

	
	{add.f16x2 %r781,%r755,%r767;
}

	
	{add.f16x2 %r784,%r758,%r770;
}

	
	{sub.f16x2 %r787,%r755,%r767;
}

	
	{sub.f16x2 %r790,%r758,%r770;
}

	
	{add.f16x2 %r793,%r761,%r779;
}

	
	{add.f16x2 %r796,%r764,%r773;
}

	
	{sub.f16x2 %r799,%r761,%r779;
}

	
	{sub.f16x2 %r802,%r764,%r773;
}

	
	{add.f16x2 %r805,%r76,%r80;
}

	
	{add.f16x2 %r808,%r809,%r810;
}

	
	{sub.f16x2 %r811,%r76,%r80;
}

	
	{sub.f16x2 %r814,%r809,%r810;
}

	
	{add.f16x2 %r817,%r78,%r82;
}

	
	{add.f16x2 %r820,%r821,%r822;
}

	
	{sub.f16x2 %r823,%r78,%r82;
}

	
	{sub.f16x2 %r826,%r821,%r822;
}

	
	{xor.b32 %r829,%r826,0x80008000;
}

	
	{add.f16x2 %r831,%r805,%r817;
}

	
	{add.f16x2 %r834,%r808,%r820;
}

	
	{sub.f16x2 %r837,%r805,%r817;
}

	
	{sub.f16x2 %r840,%r808,%r820;
}

	
	{add.f16x2 %r843,%r811,%r829;
}

	
	{add.f16x2 %r846,%r814,%r823;
}

	
	{sub.f16x2 %r849,%r811,%r829;
}

	
	{sub.f16x2 %r852,%r814,%r823;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r855, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r856, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f11;
mov.b32 %r859, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r860, {low,high};}


	
	{mul.f16x2 %r869,%r843,%r855;
}

	
	{mul.f16x2 %r872,%r846,%r856;
}

	
	{sub.f16x2 %r875,%r869,%r872;
}

	
	{mul.f16x2 %r878,%r843,%r856;
}

	
	{fma.rn.f16x2 %r881,%r846,%r855,%r878;
}

	
	{xor.b32 %r885,%r840,0x80008000;
}

	
	{mul.f16x2 %r887,%r849,%r859;
}

	
	{mul.f16x2 %r890,%r852,%r860;
}

	
	{sub.f16x2 %r893,%r887,%r890;
}

	
	{mul.f16x2 %r896,%r849,%r860;
}

	
	{fma.rn.f16x2 %r899,%r852,%r859,%r896;
}

	
	{add.f16x2 %r903,%r781,%r831;
}

	
	{add.f16x2 %r906,%r784,%r834;
}

	
	{sub.f16x2 %r909,%r781,%r831;
}

	
	{sub.f16x2 %r912,%r784,%r834;
}

	
	{add.f16x2 %r915,%r793,%r875;
}

	
	{add.f16x2 %r918,%r796,%r881;
}

	
	{sub.f16x2 %r921,%r793,%r875;
}

	
	{sub.f16x2 %r924,%r796,%r881;
}

	
	{add.f16x2 %r927,%r787,%r885;
}

	
	{add.f16x2 %r930,%r790,%r837;
}

	
	{sub.f16x2 %r933,%r787,%r885;
}

	
	{sub.f16x2 %r936,%r790,%r837;
}

	
	{add.f16x2 %r939,%r799,%r893;
}

	
	{add.f16x2 %r942,%r802,%r899;
}

	
	{sub.f16x2 %r945,%r799,%r893;
}

	
	{sub.f16x2 %r948,%r802,%r899;
}

	and.b32 %r85, %r206, 1016;
bfe.u32 %r1213, %r206, 3, 7;
cvt.rn.f32.u32	%f97, %r1213;
mul.f32 %f98, %f97, 0f3BC90FDB;
cos.approx.f32 %f79, %f98;
sin.approx.f32 %f99, %f98;
neg.f32 %f80, %f99;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f79;
cvt.rn.f16.f32 high, %f80;
mov.b32 %r951, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r954, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r956, {high,high};}


	
	{mul.f16x2 %r958,%r918,%r956;
}

	
	{fma.rn.f16x2 %r961,%r915,%r954,%r958;
}

	
	{mul.f16x2 %r965,%r915,%r956;
}

	
	{xor.b32 %r968,%r965,0x80008000;
}

	
	{fma.rn.f16x2 %r970,%r918,%r954,%r968;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r974, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r976, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r978, {low,high};}


	
	{mul.f16x2 %r979,%r976,%r978;
}

	
	{mul.f16x2 %r982,%r951,%r974;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r985, {high,low};}


	
	{fma.rn.f16x2 %r987,%r979,%r985,%r982;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r987;
mov.b32 %r991, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r987;
mov.b32 %r993, {high,high};}


	
	{mul.f16x2 %r995,%r930,%r993;
}

	
	{fma.rn.f16x2 %r998,%r927,%r991,%r995;
}

	
	{mul.f16x2 %r1002,%r927,%r993;
}

	
	{xor.b32 %r1005,%r1002,0x80008000;
}

	
	{fma.rn.f16x2 %r1007,%r930,%r991,%r1005;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1011, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1013, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1015, {low,high};}


	
	{mul.f16x2 %r1016,%r1013,%r1015;
}

	
	{mul.f16x2 %r1019,%r987,%r1011;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r987;
mov.b32 %r1022, {high,low};}


	
	{fma.rn.f16x2 %r1024,%r1016,%r1022,%r1019;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1024;
mov.b32 %r1028, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1024;
mov.b32 %r1030, {high,high};}


	
	{mul.f16x2 %r1032,%r942,%r1030;
}

	
	{fma.rn.f16x2 %r1035,%r939,%r1028,%r1032;
}

	
	{mul.f16x2 %r1039,%r939,%r1030;
}

	
	{xor.b32 %r1042,%r1039,0x80008000;
}

	
	{fma.rn.f16x2 %r1044,%r942,%r1028,%r1042;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1048, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1050, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1052, {low,high};}


	
	{mul.f16x2 %r1053,%r1050,%r1052;
}

	
	{mul.f16x2 %r1056,%r1024,%r1048;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1024;
mov.b32 %r1059, {high,low};}


	
	{fma.rn.f16x2 %r1061,%r1053,%r1059,%r1056;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1061;
mov.b32 %r1065, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1061;
mov.b32 %r1067, {high,high};}


	
	{mul.f16x2 %r1069,%r912,%r1067;
}

	
	{fma.rn.f16x2 %r1072,%r909,%r1065,%r1069;
}

	
	{mul.f16x2 %r1076,%r909,%r1067;
}

	
	{xor.b32 %r1079,%r1076,0x80008000;
}

	
	{fma.rn.f16x2 %r1081,%r912,%r1065,%r1079;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1085, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1087, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1089, {low,high};}


	
	{mul.f16x2 %r1090,%r1087,%r1089;
}

	
	{mul.f16x2 %r1093,%r1061,%r1085;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1061;
mov.b32 %r1096, {high,low};}


	
	{fma.rn.f16x2 %r1098,%r1090,%r1096,%r1093;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1098;
mov.b32 %r1102, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1098;
mov.b32 %r1104, {high,high};}


	
	{mul.f16x2 %r1106,%r924,%r1104;
}

	
	{fma.rn.f16x2 %r1109,%r921,%r1102,%r1106;
}

	
	{mul.f16x2 %r1113,%r921,%r1104;
}

	
	{xor.b32 %r1116,%r1113,0x80008000;
}

	
	{fma.rn.f16x2 %r1118,%r924,%r1102,%r1116;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1122, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1124, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1126, {low,high};}


	
	{mul.f16x2 %r1127,%r1124,%r1126;
}

	
	{mul.f16x2 %r1130,%r1098,%r1122;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1098;
mov.b32 %r1133, {high,low};}


	
	{fma.rn.f16x2 %r1135,%r1127,%r1133,%r1130;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1135;
mov.b32 %r1139, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1135;
mov.b32 %r1141, {high,high};}


	
	{mul.f16x2 %r1143,%r936,%r1141;
}

	
	{fma.rn.f16x2 %r1146,%r933,%r1139,%r1143;
}

	
	{mul.f16x2 %r1150,%r933,%r1141;
}

	
	{xor.b32 %r1153,%r1150,0x80008000;
}

	
	{fma.rn.f16x2 %r1155,%r936,%r1139,%r1153;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1159, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r951;
mov.b32 %r1161, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1163, {low,high};}


	
	{mul.f16x2 %r1164,%r1161,%r1163;
}

	
	{mul.f16x2 %r1167,%r1135,%r1159;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1135;
mov.b32 %r1170, {high,low};}


	
	{fma.rn.f16x2 %r1172,%r1164,%r1170,%r1167;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1172;
mov.b32 %r1176, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1172;
mov.b32 %r1178, {high,high};}


	
	{mul.f16x2 %r1180,%r948,%r1178;
}

	
	{fma.rn.f16x2 %r1183,%r945,%r1176,%r1180;
}

	
	{mul.f16x2 %r1187,%r945,%r1178;
}

	
	{xor.b32 %r1190,%r1187,0x80008000;
}

	
	{fma.rn.f16x2 %r1192,%r948,%r1176,%r1190;
}

	and.b32 %r1214, %r206, 7;
add.s32 %r100, %r72, %r1214;
barrier.sync 0;
shl.b32 %r1215, %r85, 3;
add.s32 %r1216, %r1215, %r100;
shl.b32 %r1217, %r1216, 2;
add.s32 %r101, %r751, %r1217;
st.shared.u32 [%r101], %r903;
st.shared.u32 [%r101+32], %r961;
st.shared.u32 [%r101+64], %r998;
st.shared.u32 [%r101+96], %r1035;
st.shared.u32 [%r101+128], %r1072;
st.shared.u32 [%r101+160], %r1109;
st.shared.u32 [%r101+192], %r1146;
st.shared.u32 [%r101+224], %r1183;
barrier.sync 0;
add.s32 %r1219, %r85, %r100;
shl.b32 %r1220, %r1219, 2;
add.s32 %r102, %r751, %r1220;
ld.shared.u32 %r103, [%r102];
ld.shared.u32 %r104, [%r102+4096];
ld.shared.u32 %r105, [%r102+8192];
ld.shared.u32 %r106, [%r102+12288];
ld.shared.u32 %r107, [%r102+16384];
ld.shared.u32 %r108, [%r102+20480];
ld.shared.u32 %r109, [%r102+24576];
ld.shared.u32 %r110, [%r102+28672];
barrier.sync 0;
st.shared.u32 [%r101], %r906;
st.shared.u32 [%r101+32], %r970;
st.shared.u32 [%r101+64], %r1007;
st.shared.u32 [%r101+96], %r1044;
st.shared.u32 [%r101+128], %r1081;
st.shared.u32 [%r101+160], %r1118;
st.shared.u32 [%r101+192], %r1155;
st.shared.u32 [%r101+224], %r1192;
barrier.sync 0;
ld.shared.u32 %r1226, [%r102];
ld.shared.u32 %r1276, [%r102+4096];
ld.shared.u32 %r1238, [%r102+8192];
ld.shared.u32 %r1288, [%r102+12288];
ld.shared.u32 %r1227, [%r102+16384];
ld.shared.u32 %r1277, [%r102+20480];
ld.shared.u32 %r1239, [%r102+24576];
ld.shared.u32 %r1289, [%r102+28672];

	{add.f16x2 %r1222,%r103,%r107;
}

	
	{add.f16x2 %r1225,%r1226,%r1227;
}

	
	{sub.f16x2 %r1228,%r103,%r107;
}

	
	{sub.f16x2 %r1231,%r1226,%r1227;
}

	
	{add.f16x2 %r1234,%r105,%r109;
}

	
	{add.f16x2 %r1237,%r1238,%r1239;
}

	
	{sub.f16x2 %r1240,%r105,%r109;
}

	
	{sub.f16x2 %r1243,%r1238,%r1239;
}

	
	{xor.b32 %r1246,%r1243,0x80008000;
}

	
	{add.f16x2 %r1248,%r1222,%r1234;
}

	
	{add.f16x2 %r1251,%r1225,%r1237;
}

	
	{sub.f16x2 %r1254,%r1222,%r1234;
}

	
	{sub.f16x2 %r1257,%r1225,%r1237;
}

	
	{add.f16x2 %r1260,%r1228,%r1246;
}

	
	{add.f16x2 %r1263,%r1231,%r1240;
}

	
	{sub.f16x2 %r1266,%r1228,%r1246;
}

	
	{sub.f16x2 %r1269,%r1231,%r1240;
}

	
	{add.f16x2 %r1272,%r104,%r108;
}

	
	{add.f16x2 %r1275,%r1276,%r1277;
}

	
	{sub.f16x2 %r1278,%r104,%r108;
}

	
	{sub.f16x2 %r1281,%r1276,%r1277;
}

	
	{add.f16x2 %r1284,%r106,%r110;
}

	
	{add.f16x2 %r1287,%r1288,%r1289;
}

	
	{sub.f16x2 %r1290,%r106,%r110;
}

	
	{sub.f16x2 %r1293,%r1288,%r1289;
}

	
	{xor.b32 %r1296,%r1293,0x80008000;
}

	
	{add.f16x2 %r1298,%r1272,%r1284;
}

	
	{add.f16x2 %r1301,%r1275,%r1287;
}

	
	{sub.f16x2 %r1304,%r1272,%r1284;
}

	
	{sub.f16x2 %r1307,%r1275,%r1287;
}

	
	{add.f16x2 %r1310,%r1278,%r1296;
}

	
	{add.f16x2 %r1313,%r1281,%r1290;
}

	
	{sub.f16x2 %r1316,%r1278,%r1296;
}

	
	{sub.f16x2 %r1319,%r1281,%r1290;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1322, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1323, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f11;
mov.b32 %r1326, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1327, {low,high};}


	
	{mul.f16x2 %r1336,%r1310,%r1322;
}

	
	{mul.f16x2 %r1339,%r1313,%r1323;
}

	
	{sub.f16x2 %r1342,%r1336,%r1339;
}

	
	{mul.f16x2 %r1345,%r1310,%r1323;
}

	
	{fma.rn.f16x2 %r1348,%r1313,%r1322,%r1345;
}

	
	{xor.b32 %r1352,%r1307,0x80008000;
}

	
	{mul.f16x2 %r1354,%r1316,%r1326;
}

	
	{mul.f16x2 %r1357,%r1319,%r1327;
}

	
	{sub.f16x2 %r1360,%r1354,%r1357;
}

	
	{mul.f16x2 %r1363,%r1316,%r1327;
}

	
	{fma.rn.f16x2 %r1366,%r1319,%r1326,%r1363;
}

	
	{add.f16x2 %r1370,%r1248,%r1298;
}

	
	{add.f16x2 %r1373,%r1251,%r1301;
}

	
	{sub.f16x2 %r1376,%r1248,%r1298;
}

	
	{sub.f16x2 %r1379,%r1251,%r1301;
}

	
	{add.f16x2 %r1382,%r1260,%r1342;
}

	
	{add.f16x2 %r1385,%r1263,%r1348;
}

	
	{sub.f16x2 %r1388,%r1260,%r1342;
}

	
	{sub.f16x2 %r1391,%r1263,%r1348;
}

	
	{add.f16x2 %r1394,%r1254,%r1352;
}

	
	{add.f16x2 %r1397,%r1257,%r1304;
}

	
	{sub.f16x2 %r1400,%r1254,%r1352;
}

	
	{sub.f16x2 %r1403,%r1257,%r1304;
}

	
	{add.f16x2 %r1406,%r1266,%r1360;
}

	
	{add.f16x2 %r1409,%r1269,%r1366;
}

	
	{sub.f16x2 %r1412,%r1266,%r1360;
}

	
	{sub.f16x2 %r1415,%r1269,%r1366;
}

	and.b32 %r113, %r206, 960;
bfe.u32 %r1680, %r206, 6, 4;
cvt.rn.f32.u32	%f146, %r1680;
mul.f32 %f147, %f146, 0f3D490FDB;
cos.approx.f32 %f128, %f147;
sin.approx.f32 %f148, %f147;
neg.f32 %f129, %f148;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f128;
cvt.rn.f16.f32 high, %f129;
mov.b32 %r1418, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1421, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1423, {high,high};}


	
	{mul.f16x2 %r1425,%r1385,%r1423;
}

	
	{fma.rn.f16x2 %r1428,%r1382,%r1421,%r1425;
}

	
	{mul.f16x2 %r1432,%r1382,%r1423;
}

	
	{xor.b32 %r1435,%r1432,0x80008000;
}

	
	{fma.rn.f16x2 %r1437,%r1385,%r1421,%r1435;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1441, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1443, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1445, {low,high};}


	
	{mul.f16x2 %r1446,%r1443,%r1445;
}

	
	{mul.f16x2 %r1449,%r1418,%r1441;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1452, {high,low};}


	
	{fma.rn.f16x2 %r1454,%r1446,%r1452,%r1449;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1454;
mov.b32 %r1458, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1454;
mov.b32 %r1460, {high,high};}


	
	{mul.f16x2 %r1462,%r1397,%r1460;
}

	
	{fma.rn.f16x2 %r1465,%r1394,%r1458,%r1462;
}

	
	{mul.f16x2 %r1469,%r1394,%r1460;
}

	
	{xor.b32 %r1472,%r1469,0x80008000;
}

	
	{fma.rn.f16x2 %r1474,%r1397,%r1458,%r1472;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1478, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1480, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1482, {low,high};}


	
	{mul.f16x2 %r1483,%r1480,%r1482;
}

	
	{mul.f16x2 %r1486,%r1454,%r1478;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1454;
mov.b32 %r1489, {high,low};}


	
	{fma.rn.f16x2 %r1491,%r1483,%r1489,%r1486;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1491;
mov.b32 %r1495, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1491;
mov.b32 %r1497, {high,high};}


	
	{mul.f16x2 %r1499,%r1409,%r1497;
}

	
	{fma.rn.f16x2 %r1502,%r1406,%r1495,%r1499;
}

	
	{mul.f16x2 %r1506,%r1406,%r1497;
}

	
	{xor.b32 %r1509,%r1506,0x80008000;
}

	
	{fma.rn.f16x2 %r1511,%r1409,%r1495,%r1509;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1515, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1517, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1519, {low,high};}


	
	{mul.f16x2 %r1520,%r1517,%r1519;
}

	
	{mul.f16x2 %r1523,%r1491,%r1515;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1491;
mov.b32 %r1526, {high,low};}


	
	{fma.rn.f16x2 %r1528,%r1520,%r1526,%r1523;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1528;
mov.b32 %r1532, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1528;
mov.b32 %r1534, {high,high};}


	
	{mul.f16x2 %r1536,%r1379,%r1534;
}

	
	{fma.rn.f16x2 %r1539,%r1376,%r1532,%r1536;
}

	
	{mul.f16x2 %r1543,%r1376,%r1534;
}

	
	{xor.b32 %r1546,%r1543,0x80008000;
}

	
	{fma.rn.f16x2 %r1548,%r1379,%r1532,%r1546;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1552, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1554, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1556, {low,high};}


	
	{mul.f16x2 %r1557,%r1554,%r1556;
}

	
	{mul.f16x2 %r1560,%r1528,%r1552;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1528;
mov.b32 %r1563, {high,low};}


	
	{fma.rn.f16x2 %r1565,%r1557,%r1563,%r1560;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1565;
mov.b32 %r1569, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1565;
mov.b32 %r1571, {high,high};}


	
	{mul.f16x2 %r1573,%r1391,%r1571;
}

	
	{fma.rn.f16x2 %r1576,%r1388,%r1569,%r1573;
}

	
	{mul.f16x2 %r1580,%r1388,%r1571;
}

	
	{xor.b32 %r1583,%r1580,0x80008000;
}

	
	{fma.rn.f16x2 %r1585,%r1391,%r1569,%r1583;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1589, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1591, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1593, {low,high};}


	
	{mul.f16x2 %r1594,%r1591,%r1593;
}

	
	{mul.f16x2 %r1597,%r1565,%r1589;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1565;
mov.b32 %r1600, {high,low};}


	
	{fma.rn.f16x2 %r1602,%r1594,%r1600,%r1597;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1602;
mov.b32 %r1606, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1602;
mov.b32 %r1608, {high,high};}


	
	{mul.f16x2 %r1610,%r1403,%r1608;
}

	
	{fma.rn.f16x2 %r1613,%r1400,%r1606,%r1610;
}

	
	{mul.f16x2 %r1617,%r1400,%r1608;
}

	
	{xor.b32 %r1620,%r1617,0x80008000;
}

	
	{fma.rn.f16x2 %r1622,%r1403,%r1606,%r1620;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1626, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1418;
mov.b32 %r1628, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1630, {low,high};}


	
	{mul.f16x2 %r1631,%r1628,%r1630;
}

	
	{mul.f16x2 %r1634,%r1602,%r1626;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1602;
mov.b32 %r1637, {high,low};}


	
	{fma.rn.f16x2 %r1639,%r1631,%r1637,%r1634;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1639;
mov.b32 %r1643, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1639;
mov.b32 %r1645, {high,high};}


	
	{mul.f16x2 %r1647,%r1415,%r1645;
}

	
	{fma.rn.f16x2 %r1650,%r1412,%r1643,%r1647;
}

	
	{mul.f16x2 %r1654,%r1412,%r1645;
}

	
	{xor.b32 %r1657,%r1654,0x80008000;
}

	
	{fma.rn.f16x2 %r1659,%r1415,%r1643,%r1657;
}

	and.b32 %r1681, %r206, 63;
add.s32 %r128, %r72, %r1681;
barrier.sync 0;
shl.b32 %r1682, %r113, 3;
add.s32 %r1683, %r1682, %r128;
shl.b32 %r1684, %r1683, 2;
add.s32 %r129, %r751, %r1684;
st.shared.u32 [%r129], %r1370;
st.shared.u32 [%r129+256], %r1428;
st.shared.u32 [%r129+512], %r1465;
st.shared.u32 [%r129+768], %r1502;
st.shared.u32 [%r129+1024], %r1539;
st.shared.u32 [%r129+1280], %r1576;
st.shared.u32 [%r129+1536], %r1613;
st.shared.u32 [%r129+1792], %r1650;
barrier.sync 0;
add.s32 %r1686, %r113, %r128;
shl.b32 %r1687, %r1686, 2;
add.s32 %r130, %r751, %r1687;
ld.shared.u32 %r131, [%r130];
ld.shared.u32 %r132, [%r130+4096];
ld.shared.u32 %r133, [%r130+8192];
ld.shared.u32 %r134, [%r130+12288];
ld.shared.u32 %r135, [%r130+16384];
ld.shared.u32 %r136, [%r130+20480];
ld.shared.u32 %r137, [%r130+24576];
ld.shared.u32 %r138, [%r130+28672];
barrier.sync 0;
st.shared.u32 [%r129], %r1373;
st.shared.u32 [%r129+256], %r1437;
st.shared.u32 [%r129+512], %r1474;
st.shared.u32 [%r129+768], %r1511;
st.shared.u32 [%r129+1024], %r1548;
st.shared.u32 [%r129+1280], %r1585;
st.shared.u32 [%r129+1536], %r1622;
st.shared.u32 [%r129+1792], %r1659;
barrier.sync 0;
ld.shared.u32 %r1693, [%r130];
ld.shared.u32 %r1743, [%r130+4096];
ld.shared.u32 %r1705, [%r130+8192];
ld.shared.u32 %r1755, [%r130+12288];
ld.shared.u32 %r1694, [%r130+16384];
ld.shared.u32 %r1744, [%r130+20480];
ld.shared.u32 %r1706, [%r130+24576];
ld.shared.u32 %r1756, [%r130+28672];

	{add.f16x2 %r1689,%r131,%r135;
}

	
	{add.f16x2 %r1692,%r1693,%r1694;
}

	
	{sub.f16x2 %r1695,%r131,%r135;
}

	
	{sub.f16x2 %r1698,%r1693,%r1694;
}

	
	{add.f16x2 %r1701,%r133,%r137;
}

	
	{add.f16x2 %r1704,%r1705,%r1706;
}

	
	{sub.f16x2 %r1707,%r133,%r137;
}

	
	{sub.f16x2 %r1710,%r1705,%r1706;
}

	
	{xor.b32 %r1713,%r1710,0x80008000;
}

	
	{add.f16x2 %r1715,%r1689,%r1701;
}

	
	{add.f16x2 %r1718,%r1692,%r1704;
}

	
	{sub.f16x2 %r1721,%r1689,%r1701;
}

	
	{sub.f16x2 %r1724,%r1692,%r1704;
}

	
	{add.f16x2 %r1727,%r1695,%r1713;
}

	
	{add.f16x2 %r1730,%r1698,%r1707;
}

	
	{sub.f16x2 %r1733,%r1695,%r1713;
}

	
	{sub.f16x2 %r1736,%r1698,%r1707;
}

	
	{add.f16x2 %r1739,%r132,%r136;
}

	
	{add.f16x2 %r1742,%r1743,%r1744;
}

	
	{sub.f16x2 %r1745,%r132,%r136;
}

	
	{sub.f16x2 %r1748,%r1743,%r1744;
}

	
	{add.f16x2 %r1751,%r134,%r138;
}

	
	{add.f16x2 %r1754,%r1755,%r1756;
}

	
	{sub.f16x2 %r1757,%r134,%r138;
}

	
	{sub.f16x2 %r1760,%r1755,%r1756;
}

	
	{xor.b32 %r1763,%r1760,0x80008000;
}

	
	{add.f16x2 %r1765,%r1739,%r1751;
}

	
	{add.f16x2 %r1768,%r1742,%r1754;
}

	
	{sub.f16x2 %r1771,%r1739,%r1751;
}

	
	{sub.f16x2 %r1774,%r1742,%r1754;
}

	
	{add.f16x2 %r1777,%r1745,%r1763;
}

	
	{add.f16x2 %r1780,%r1748,%r1757;
}

	
	{sub.f16x2 %r1783,%r1745,%r1763;
}

	
	{sub.f16x2 %r1786,%r1748,%r1757;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1789, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1790, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f11;
mov.b32 %r1793, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1794, {low,high};}


	
	{mul.f16x2 %r1803,%r1777,%r1789;
}

	
	{mul.f16x2 %r1806,%r1780,%r1790;
}

	
	{sub.f16x2 %r1809,%r1803,%r1806;
}

	
	{mul.f16x2 %r1812,%r1777,%r1790;
}

	
	{fma.rn.f16x2 %r1815,%r1780,%r1789,%r1812;
}

	
	{xor.b32 %r1819,%r1774,0x80008000;
}

	
	{mul.f16x2 %r1821,%r1783,%r1793;
}

	
	{mul.f16x2 %r1824,%r1786,%r1794;
}

	
	{sub.f16x2 %r1827,%r1821,%r1824;
}

	
	{mul.f16x2 %r1830,%r1783,%r1794;
}

	
	{fma.rn.f16x2 %r1833,%r1786,%r1793,%r1830;
}

	
	{add.f16x2 %r1837,%r1715,%r1765;
}

	
	{add.f16x2 %r1840,%r1718,%r1768;
}

	
	{sub.f16x2 %r1843,%r1715,%r1765;
}

	
	{sub.f16x2 %r1846,%r1718,%r1768;
}

	
	{add.f16x2 %r1849,%r1727,%r1809;
}

	
	{add.f16x2 %r1852,%r1730,%r1815;
}

	
	{sub.f16x2 %r1855,%r1727,%r1809;
}

	
	{sub.f16x2 %r1858,%r1730,%r1815;
}

	
	{add.f16x2 %r1861,%r1721,%r1819;
}

	
	{add.f16x2 %r1864,%r1724,%r1771;
}

	
	{sub.f16x2 %r1867,%r1721,%r1819;
}

	
	{sub.f16x2 %r1870,%r1724,%r1771;
}

	
	{add.f16x2 %r1873,%r1733,%r1827;
}

	
	{add.f16x2 %r1876,%r1736,%r1833;
}

	
	{sub.f16x2 %r1879,%r1733,%r1827;
}

	
	{sub.f16x2 %r1882,%r1736,%r1833;
}

	and.b32 %r141, %r206, 512;
bfe.u32 %r2147, %r206, 9, 1;
cvt.rn.f32.u32	%f195, %r2147;
mul.f32 %f196, %f195, 0f3EC90FDB;
cos.approx.f32 %f177, %f196;
sin.approx.f32 %f197, %f196;
neg.f32 %f178, %f197;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f177;
cvt.rn.f16.f32 high, %f178;
mov.b32 %r1885, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1888, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1890, {high,high};}


	
	{mul.f16x2 %r1892,%r1852,%r1890;
}

	
	{fma.rn.f16x2 %r1895,%r1849,%r1888,%r1892;
}

	
	{mul.f16x2 %r1899,%r1849,%r1890;
}

	
	{xor.b32 %r1902,%r1899,0x80008000;
}

	
	{fma.rn.f16x2 %r1904,%r1852,%r1888,%r1902;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1908, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1910, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1912, {low,high};}


	
	{mul.f16x2 %r1913,%r1910,%r1912;
}

	
	{mul.f16x2 %r1916,%r1885,%r1908;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1919, {high,low};}


	
	{fma.rn.f16x2 %r1921,%r1913,%r1919,%r1916;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1921;
mov.b32 %r1925, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1921;
mov.b32 %r1927, {high,high};}


	
	{mul.f16x2 %r1929,%r1864,%r1927;
}

	
	{fma.rn.f16x2 %r1932,%r1861,%r1925,%r1929;
}

	
	{mul.f16x2 %r1936,%r1861,%r1927;
}

	
	{xor.b32 %r1939,%r1936,0x80008000;
}

	
	{fma.rn.f16x2 %r1941,%r1864,%r1925,%r1939;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1945, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1947, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1949, {low,high};}


	
	{mul.f16x2 %r1950,%r1947,%r1949;
}

	
	{mul.f16x2 %r1953,%r1921,%r1945;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1921;
mov.b32 %r1956, {high,low};}


	
	{fma.rn.f16x2 %r1958,%r1950,%r1956,%r1953;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1958;
mov.b32 %r1962, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1958;
mov.b32 %r1964, {high,high};}


	
	{mul.f16x2 %r1966,%r1876,%r1964;
}

	
	{fma.rn.f16x2 %r1969,%r1873,%r1962,%r1966;
}

	
	{mul.f16x2 %r1973,%r1873,%r1964;
}

	
	{xor.b32 %r1976,%r1973,0x80008000;
}

	
	{fma.rn.f16x2 %r1978,%r1876,%r1962,%r1976;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1982, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r1984, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1986, {low,high};}


	
	{mul.f16x2 %r1987,%r1984,%r1986;
}

	
	{mul.f16x2 %r1990,%r1958,%r1982;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1958;
mov.b32 %r1993, {high,low};}


	
	{fma.rn.f16x2 %r1995,%r1987,%r1993,%r1990;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1995;
mov.b32 %r1999, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1995;
mov.b32 %r2001, {high,high};}


	
	{mul.f16x2 %r2003,%r1846,%r2001;
}

	
	{fma.rn.f16x2 %r2006,%r1843,%r1999,%r2003;
}

	
	{mul.f16x2 %r2010,%r1843,%r2001;
}

	
	{xor.b32 %r2013,%r2010,0x80008000;
}

	
	{fma.rn.f16x2 %r2015,%r1846,%r1999,%r2013;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r2019, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r2021, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r2023, {low,high};}


	
	{mul.f16x2 %r2024,%r2021,%r2023;
}

	
	{mul.f16x2 %r2027,%r1995,%r2019;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1995;
mov.b32 %r2030, {high,low};}


	
	{fma.rn.f16x2 %r2032,%r2024,%r2030,%r2027;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2032;
mov.b32 %r2036, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2032;
mov.b32 %r2038, {high,high};}


	
	{mul.f16x2 %r2040,%r1858,%r2038;
}

	
	{fma.rn.f16x2 %r2043,%r1855,%r2036,%r2040;
}

	
	{mul.f16x2 %r2047,%r1855,%r2038;
}

	
	{xor.b32 %r2050,%r2047,0x80008000;
}

	
	{fma.rn.f16x2 %r2052,%r1858,%r2036,%r2050;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r2056, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r2058, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r2060, {low,high};}


	
	{mul.f16x2 %r2061,%r2058,%r2060;
}

	
	{mul.f16x2 %r2064,%r2032,%r2056;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2032;
mov.b32 %r2067, {high,low};}


	
	{fma.rn.f16x2 %r2069,%r2061,%r2067,%r2064;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2069;
mov.b32 %r2073, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2069;
mov.b32 %r2075, {high,high};}


	
	{mul.f16x2 %r2077,%r1870,%r2075;
}

	
	{fma.rn.f16x2 %r2080,%r1867,%r2073,%r2077;
}

	
	{mul.f16x2 %r2084,%r1867,%r2075;
}

	
	{xor.b32 %r2087,%r2084,0x80008000;
}

	
	{fma.rn.f16x2 %r2089,%r1870,%r2073,%r2087;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r2093, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1885;
mov.b32 %r2095, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r2097, {low,high};}


	
	{mul.f16x2 %r2098,%r2095,%r2097;
}

	
	{mul.f16x2 %r2101,%r2069,%r2093;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2069;
mov.b32 %r2104, {high,low};}


	
	{fma.rn.f16x2 %r2106,%r2098,%r2104,%r2101;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2106;
mov.b32 %r2110, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2106;
mov.b32 %r2112, {high,high};}


	
	{mul.f16x2 %r2114,%r1882,%r2112;
}

	
	{fma.rn.f16x2 %r2117,%r1879,%r2110,%r2114;
}

	
	{mul.f16x2 %r2121,%r1879,%r2112;
}

	
	{xor.b32 %r2124,%r2121,0x80008000;
}

	
	{fma.rn.f16x2 %r2126,%r1882,%r2110,%r2124;
}

	and.b32 %r2148, %r206, 511;
add.s32 %r156, %r72, %r2148;
barrier.sync 0;
shl.b32 %r2149, %r141, 3;
add.s32 %r2150, %r2149, %r156;
shl.b32 %r2151, %r2150, 2;
add.s32 %r157, %r751, %r2151;
st.shared.u32 [%r157], %r1837;
st.shared.u32 [%r157+2048], %r1895;
st.shared.u32 [%r157+4096], %r1932;
st.shared.u32 [%r157+6144], %r1969;
st.shared.u32 [%r157+8192], %r2006;
st.shared.u32 [%r157+10240], %r2043;
st.shared.u32 [%r157+12288], %r2080;
st.shared.u32 [%r157+14336], %r2117;
barrier.sync 0;
add.s32 %r2153, %r141, %r156;
shl.b32 %r2154, %r2153, 2;
add.s32 %r158, %r751, %r2154;
ld.shared.u32 %r159, [%r158];
ld.shared.u32 %r160, [%r158+4096];
ld.shared.u32 %r161, [%r158+8192];
ld.shared.u32 %r162, [%r158+12288];
ld.shared.u32 %r163, [%r158+16384];
ld.shared.u32 %r164, [%r158+20480];
ld.shared.u32 %r165, [%r158+24576];
ld.shared.u32 %r166, [%r158+28672];
barrier.sync 0;
st.shared.u32 [%r157], %r1840;
st.shared.u32 [%r157+2048], %r1904;
st.shared.u32 [%r157+4096], %r1941;
st.shared.u32 [%r157+6144], %r1978;
st.shared.u32 [%r157+8192], %r2015;
st.shared.u32 [%r157+10240], %r2052;
st.shared.u32 [%r157+12288], %r2089;
st.shared.u32 [%r157+14336], %r2126;
barrier.sync 0;

	{add.f16x2 %r2156,%r159,%r163;
}

	
	{sub.f16x2 %r2162,%r159,%r163;
}

	
	{add.f16x2 %r2168,%r160,%r164;
}

	
	{sub.f16x2 %r2174,%r160,%r164;
}

	
	{add.f16x2 %r2180,%r161,%r165;
}

	
	{sub.f16x2 %r2186,%r161,%r165;
}

	
	{add.f16x2 %r2192,%r162,%r166;
}

	
	{sub.f16x2 %r2198,%r162,%r166;
}

	barrier.sync 0;
shl.b32 %r2204, %r206, 2;
add.s32 %r2206, %r751, %r2204;
st.shared.u32 [%r2206], %r2156;
st.shared.u32 [%r2206+4096], %r2168;
st.shared.u32 [%r2206+8192], %r2180;
st.shared.u32 [%r2206+12288], %r2192;
st.shared.u32 [%r2206+16384], %r2162;
st.shared.u32 [%r2206+20480], %r2174;
st.shared.u32 [%r2206+24576], %r2186;
st.shared.u32 [%r2206+28672], %r2198;
barrier.sync 0;
add.s32 %r2233, %r751, %r747;
ld.shared.u32 %r2208, [%r2233];
ld.shared.u32 %r2209, [%r2233+4];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2208;
mov.b32 {blow,bhigh}, %r2209;
mov.b32 %r2207, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2208;
mov.b32 {blow,bhigh}, %r2209;
mov.b32 %r2210, {ahigh,bhigh};}


	ld.shared.u32 %r2214, [%r2233+8192];
ld.shared.u32 %r2215, [%r2233+8196];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2214;
mov.b32 {blow,bhigh}, %r2215;
mov.b32 %r2213, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2214;
mov.b32 {blow,bhigh}, %r2215;
mov.b32 %r2216, {ahigh,bhigh};}


	ld.shared.u32 %r2220, [%r2233+16384];
ld.shared.u32 %r2221, [%r2233+16388];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2220;
mov.b32 {blow,bhigh}, %r2221;
mov.b32 %r2219, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2220;
mov.b32 {blow,bhigh}, %r2221;
mov.b32 %r2222, {ahigh,bhigh};}


	ld.shared.u32 %r2226, [%r2233+24576];
ld.shared.u32 %r2227, [%r2233+24580];

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2226;
mov.b32 {blow,bhigh}, %r2227;
mov.b32 %r2225, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2226;
mov.b32 {blow,bhigh}, %r2227;
mov.b32 %r2228, {ahigh,bhigh};}


	setp.eq.s16	%p4, %rs1, 0;
selp.b32	%r2234, 4096, 4097, %p4;
shl.b32 %r2235, %r204, 1;
add.s32 %r2236, %r2235, 1;
mad.lo.s32 %r2237, %r2235, %r2234, %r206;
cvta.to.global.u64 %rd16, %rd7;
mul.wide.u32 %rd17, %r2237, 4;
add.s64 %rd4, %rd16, %rd17;
mad.lo.s32 %r2238, %r2236, %r2234, %r206;
mul.wide.u32 %rd18, %r2238, 4;
add.s64 %rd5, %rd16, %rd18;
@%p2 bra BB12_6;
bra.uni BB12_5;

BB12_6:
st.global.u32 [%rd4], %r2207;
st.global.u32 [%rd4+4096], %r2213;
st.global.u32 [%rd4+8192], %r2219;
st.global.u32 [%rd4+12288], %r2225;
bra.uni BB12_7;

BB12_5:
shl.b32 %r2240, %r3, 1;
add.s32 %r2241, %r2240, -1;
st.global.u32 [%rd4], %r2207;
st.global.u32 [%rd4+4096], %r2213;
st.global.u32 [%rd4+8192], %r2219;
st.global.u32 [%rd4+12288], %r2225;
setp.ge.u32	%p5, %r2241, %r1;
@%p5 bra BB12_8;

BB12_7:
st.global.u32 [%rd5], %r2210;
st.global.u32 [%rd5+4096], %r2216;
st.global.u32 [%rd5+8192], %r2222;
st.global.u32 [%rd5+12288], %r2228;

BB12_8:
ret;
}


