







.version 7.0
.target sm_80
.address_size 64


.global .align 4 .u32 _ZZN71_INTERNAL_49_half_32bit_vector_RT_SM53_plus_compute_80_cpp1_ii_5637c3ce18cooperative_groups4__v17details17_binary_partitionINS1_15coalesced_groupEEES4_RKT_bE8fullMask = -1;
.extern .shared .align 4 .b8 smem_full[];

.weak .entry _Z10vector_fftILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<9>;
.reg .b16 %rs<9>;
.reg .f32 %f<2>;
.reg .b32 %r<127>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u32 %r1, [_Z10vector_fftILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd6, [_Z10vector_fftILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z10vector_fftILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r5, [_Z10vector_fftILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
mov.u32 %r2, %ctaid.x;
shl.b32 %r60, %r2, 5;
mov.u32 %r3, %tid.y;
add.s32 %r61, %r60, %r3;
shl.b32 %r4, %r61, 1;
setp.ge.u32	%p1, %r4, %r5;
@%p1 bra BB0_6;

shl.b32 %r6, %r3, 1;
shl.b32 %r62, %r2, 6;
add.s32 %r63, %r6, %r62;
shl.b32 %r64, %r63, 1;
mov.u32 %r65, %tid.x;
add.s32 %r66, %r65, %r64;
mov.u32 %r7, %nctaid.x;
add.s32 %r67, %r7, -1;
setp.lt.u32	%p2, %r2, %r67;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r66, 4;
add.s64 %rd1, %rd7, %rd8;
add.s32 %r68, %r66, 2;
mul.wide.u32 %rd9, %r68, 4;
add.s64 %rd2, %rd7, %rd9;
@%p2 bra BB0_3;
bra.uni BB0_2;

BB0_3:
ld.global.u32 %r122, [%rd1];
ld.global.u32 %r120, [%rd1+4];
bra.uni BB0_4;

BB0_2:
shl.b32 %r70, %r7, 6;
add.s32 %r71, %r6, %r70;
add.s32 %r72, %r71, -63;
ld.global.u32 %r122, [%rd1];
ld.global.u32 %r120, [%rd1+4];
setp.ge.u32	%p3, %r72, %r5;
@%p3 bra BB0_5;

BB0_4:
ld.global.u32 %r121, [%rd2];
ld.global.u32 %r119, [%rd2+4];

BB0_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r122;
mov.b32 {blow,bhigh}, %r121;
mov.b32 %r73, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r122;
mov.b32 {blow,bhigh}, %r121;
mov.b32 %r76, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r120;
mov.b32 {blow,bhigh}, %r119;
mov.b32 %r79, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r120;
mov.b32 {blow,bhigh}, %r119;
mov.b32 %r82, {ahigh,bhigh};}


	setp.eq.s32	%p4, %r1, 1;
selp.b32	%r123, %r79, %r82, %p4;
selp.b32	%r124, %r82, %r79, %p4;
selp.b32	%r125, %r73, %r76, %p4;
selp.b32	%r126, %r76, %r73, %p4;

BB0_6:

	{add.f16x2 %r85,%r126,%r124;
}

	
	{add.f16x2 %r88,%r125,%r123;
}

	
	{sub.f16x2 %r91,%r126,%r124;
}

	
	{sub.f16x2 %r94,%r125,%r123;
}

	@%p1 bra BB0_11;

mov.u32 %r109, %tid.x;
setp.eq.s32	%p6, %r1, 1;
selp.b32	%r108, %r91, %r94, %p6;
selp.b32	%r107, %r94, %r91, %p6;
selp.b32	%r102, %r85, %r88, %p6;
selp.b32	%r101, %r88, %r85, %p6;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r101;
mov.b32 {blow,bhigh}, %r102;
mov.b32 %r97, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r101;
mov.b32 {blow,bhigh}, %r102;
mov.b32 %r100, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r107;
mov.b32 {blow,bhigh}, %r108;
mov.b32 %r103, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r107;
mov.b32 {blow,bhigh}, %r108;
mov.b32 %r106, {ahigh,bhigh};}


	shl.b32 %r36, %r3, 1;
shl.b32 %r110, %r2, 6;
add.s32 %r111, %r36, %r110;
shl.b32 %r112, %r111, 1;
add.s32 %r113, %r109, %r112;
mov.u32 %r37, %nctaid.x;
add.s32 %r114, %r37, -1;
setp.lt.u32	%p7, %r2, %r114;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r113, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r115, %r113, 2;
mul.wide.u32 %rd12, %r115, 4;
add.s64 %rd4, %rd10, %rd12;
@%p7 bra BB0_9;
bra.uni BB0_8;

BB0_9:
st.global.u32 [%rd3], %r97;
st.global.u32 [%rd3+4], %r103;
bra.uni BB0_10;

BB0_8:
shl.b32 %r116, %r37, 6;
add.s32 %r117, %r36, %r116;
add.s32 %r118, %r117, -63;
st.global.u32 [%rd3], %r97;
st.global.u32 [%rd3+4], %r103;
setp.ge.u32	%p8, %r118, %r5;
@%p8 bra BB0_11;

BB0_10:
st.global.u32 [%rd4], %r100;
st.global.u32 [%rd4+4], %r106;

BB0_11:
ret;
}


.weak .entry _Z10vector_fftILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<9>;
.reg .b16 %rs<9>;
.reg .f32 %f<11>;
.reg .b32 %r<198>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u32 %r1, [_Z10vector_fftILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd6, [_Z10vector_fftILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z10vector_fftILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r5, [_Z10vector_fftILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
mov.u32 %r2, %ctaid.x;
shl.b32 %r71, %r2, 4;
mov.u32 %r3, %tid.y;
add.s32 %r72, %r71, %r3;
shl.b32 %r4, %r72, 1;
mov.u32 %r6, %tid.x;
setp.ge.u32	%p1, %r4, %r5;
@%p1 bra BB1_6;

shl.b32 %r7, %r3, 1;
shl.b32 %r73, %r2, 5;
add.s32 %r74, %r7, %r73;
shl.b32 %r75, %r74, 2;
mov.u32 %r8, %nctaid.x;
add.s32 %r76, %r8, -1;
setp.lt.u32	%p2, %r2, %r76;
add.s32 %r77, %r75, %r6;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r77, 4;
add.s64 %rd1, %rd7, %rd8;
add.s32 %r78, %r77, 4;
mul.wide.u32 %rd9, %r78, 4;
add.s64 %rd2, %rd7, %rd9;
@%p2 bra BB1_3;
bra.uni BB1_2;

BB1_3:
ld.global.u32 %r193, [%rd1];
ld.global.u32 %r191, [%rd1+8];
bra.uni BB1_4;

BB1_2:
shl.b32 %r80, %r8, 5;
add.s32 %r81, %r7, %r80;
add.s32 %r82, %r81, -31;
ld.global.u32 %r193, [%rd1];
ld.global.u32 %r191, [%rd1+8];
setp.ge.u32	%p3, %r82, %r5;
@%p3 bra BB1_5;

BB1_4:
ld.global.u32 %r192, [%rd2];
ld.global.u32 %r190, [%rd2+8];

BB1_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r193;
mov.b32 {blow,bhigh}, %r192;
mov.b32 %r83, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r193;
mov.b32 {blow,bhigh}, %r192;
mov.b32 %r86, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r191;
mov.b32 {blow,bhigh}, %r190;
mov.b32 %r89, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r191;
mov.b32 {blow,bhigh}, %r190;
mov.b32 %r92, {ahigh,bhigh};}


	setp.eq.s32	%p4, %r1, 1;
selp.b32	%r194, %r89, %r92, %p4;
selp.b32	%r195, %r92, %r89, %p4;
selp.b32	%r196, %r83, %r86, %p4;
selp.b32	%r197, %r86, %r83, %p4;

BB1_6:

	{add.f16x2 %r95,%r197,%r195;
}

	
	{add.f16x2 %r98,%r196,%r194;
}

	
	{sub.f16x2 %r101,%r197,%r195;
}

	
	{sub.f16x2 %r104,%r196,%r194;
}

	and.b32 %r31, %r6, 1;
cvt.rn.f32.u32	%f8, %r31;
mul.f32 %f9, %f8, 0f3FC90FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r107, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r107;
mov.b32 %r110, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r107;
mov.b32 %r112, {high,high};}


	
	{mul.f16x2 %r114,%r104,%r112;
}

	
	{xor.b32 %r117,%r114,0x80008000;
}

	
	{fma.rn.f16x2 %r119,%r101,%r110,%r117;
}

	
	{mul.f16x2 %r123,%r101,%r112;
}

	
	{fma.rn.f16x2 %r126,%r104,%r110,%r123;
}

	shl.b32 %r147, %r6, 1;
and.b32 %r148, %r147, -4;
shl.b32 %r149, %r3, 2;
add.s32 %r34, %r148, %r149;
barrier.sync 0;
shl.b32 %r150, %r31, 1;
add.s32 %r151, %r150, %r34;
shl.b32 %r152, %r151, 2;
mov.u32 %r153, smem_full;
add.s32 %r35, %r153, %r152;
st.shared.u32 [%r35], %r95;
st.shared.u32 [%r35+4], %r119;
barrier.sync 0;
add.s32 %r154, %r31, %r34;
shl.b32 %r155, %r154, 2;
add.s32 %r36, %r153, %r155;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+8];
barrier.sync 0;
st.shared.u32 [%r35], %r98;
st.shared.u32 [%r35+4], %r126;
barrier.sync 0;
ld.shared.u32 %r161, [%r36];
ld.shared.u32 %r162, [%r36+8];

	{add.f16x2 %r157,%r37,%r38;
}

	
	{add.f16x2 %r160,%r161,%r162;
}

	
	{sub.f16x2 %r163,%r37,%r38;
}

	
	{sub.f16x2 %r166,%r161,%r162;
}

	@%p1 bra BB1_11;

setp.eq.s32	%p6, %r1, 1;
selp.b32	%r180, %r163, %r166, %p6;
selp.b32	%r179, %r166, %r163, %p6;
selp.b32	%r174, %r157, %r160, %p6;
selp.b32	%r173, %r160, %r157, %p6;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r173;
mov.b32 {blow,bhigh}, %r174;
mov.b32 %r169, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r173;
mov.b32 {blow,bhigh}, %r174;
mov.b32 %r172, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r179;
mov.b32 {blow,bhigh}, %r180;
mov.b32 %r175, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r179;
mov.b32 {blow,bhigh}, %r180;
mov.b32 %r178, {ahigh,bhigh};}


	shl.b32 %r47, %r3, 1;
shl.b32 %r181, %r2, 5;
add.s32 %r182, %r47, %r181;
shl.b32 %r183, %r182, 2;
add.s32 %r184, %r6, %r183;
mov.u32 %r48, %nctaid.x;
add.s32 %r185, %r48, -1;
setp.lt.u32	%p7, %r2, %r185;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r184, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r186, %r184, 4;
mul.wide.u32 %rd12, %r186, 4;
add.s64 %rd4, %rd10, %rd12;
@%p7 bra BB1_9;
bra.uni BB1_8;

BB1_9:
st.global.u32 [%rd3], %r169;
st.global.u32 [%rd3+8], %r175;
bra.uni BB1_10;

BB1_8:
shl.b32 %r187, %r48, 5;
add.s32 %r188, %r47, %r187;
add.s32 %r189, %r188, -31;
st.global.u32 [%rd3], %r169;
st.global.u32 [%rd3+8], %r175;
setp.ge.u32	%p8, %r189, %r5;
@%p8 bra BB1_11;

BB1_10:
st.global.u32 [%rd4], %r172;
st.global.u32 [%rd4+8], %r178;

BB1_11:
ret;
}


.weak .entry _Z10vector_fftILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<9>;
.reg .b16 %rs<9>;
.reg .f32 %f<20>;
.reg .b32 %r<269>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u32 %r1, [_Z10vector_fftILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd6, [_Z10vector_fftILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z10vector_fftILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r5, [_Z10vector_fftILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
mov.u32 %r2, %ctaid.x;
shl.b32 %r81, %r2, 3;
mov.u32 %r3, %tid.y;
add.s32 %r82, %r81, %r3;
shl.b32 %r4, %r82, 1;
mov.u32 %r6, %tid.x;
setp.ge.u32	%p1, %r4, %r5;
@%p1 bra BB2_6;

shl.b32 %r7, %r3, 1;
shl.b32 %r83, %r2, 4;
add.s32 %r84, %r7, %r83;
shl.b32 %r85, %r84, 3;
mov.u32 %r8, %nctaid.x;
add.s32 %r86, %r8, -1;
setp.lt.u32	%p2, %r2, %r86;
add.s32 %r87, %r85, %r6;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r87, 4;
add.s64 %rd1, %rd7, %rd8;
add.s32 %r88, %r87, 8;
mul.wide.u32 %rd9, %r88, 4;
add.s64 %rd2, %rd7, %rd9;
@%p2 bra BB2_3;
bra.uni BB2_2;

BB2_3:
ld.global.u32 %r264, [%rd1];
ld.global.u32 %r262, [%rd1+16];
bra.uni BB2_4;

BB2_2:
shl.b32 %r90, %r8, 4;
add.s32 %r91, %r7, %r90;
add.s32 %r92, %r91, -15;
ld.global.u32 %r264, [%rd1];
ld.global.u32 %r262, [%rd1+16];
setp.ge.u32	%p3, %r92, %r5;
@%p3 bra BB2_5;

BB2_4:
ld.global.u32 %r263, [%rd2];
ld.global.u32 %r261, [%rd2+16];

BB2_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r264;
mov.b32 {blow,bhigh}, %r263;
mov.b32 %r93, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r264;
mov.b32 {blow,bhigh}, %r263;
mov.b32 %r96, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r262;
mov.b32 {blow,bhigh}, %r261;
mov.b32 %r99, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r262;
mov.b32 {blow,bhigh}, %r261;
mov.b32 %r102, {ahigh,bhigh};}


	setp.eq.s32	%p4, %r1, 1;
selp.b32	%r265, %r99, %r102, %p4;
selp.b32	%r266, %r102, %r99, %p4;
selp.b32	%r267, %r93, %r96, %p4;
selp.b32	%r268, %r96, %r93, %p4;

BB2_6:

	{add.f16x2 %r105,%r268,%r266;
}

	
	{add.f16x2 %r108,%r267,%r265;
}

	
	{sub.f16x2 %r111,%r268,%r266;
}

	
	{sub.f16x2 %r114,%r267,%r265;
}

	and.b32 %r31, %r6, 3;
cvt.rn.f32.u32	%f8, %r31;
mul.f32 %f9, %f8, 0f3F490FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r117, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r117;
mov.b32 %r120, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r117;
mov.b32 %r122, {high,high};}


	
	{mul.f16x2 %r124,%r114,%r122;
}

	
	{xor.b32 %r127,%r124,0x80008000;
}

	
	{fma.rn.f16x2 %r129,%r111,%r120,%r127;
}

	
	{mul.f16x2 %r133,%r111,%r122;
}

	
	{fma.rn.f16x2 %r136,%r114,%r120,%r133;
}

	shl.b32 %r157, %r6, 1;
and.b32 %r158, %r157, -8;
shl.b32 %r159, %r3, 3;
add.s32 %r34, %r158, %r159;
barrier.sync 0;
shl.b32 %r160, %r31, 1;
add.s32 %r161, %r160, %r34;
shl.b32 %r162, %r161, 2;
mov.u32 %r163, smem_full;
add.s32 %r35, %r163, %r162;
st.shared.u32 [%r35], %r105;
st.shared.u32 [%r35+4], %r129;
barrier.sync 0;
add.s32 %r164, %r31, %r34;
shl.b32 %r165, %r164, 2;
add.s32 %r36, %r163, %r165;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+16];
barrier.sync 0;
st.shared.u32 [%r35], %r108;
st.shared.u32 [%r35+4], %r136;
barrier.sync 0;
ld.shared.u32 %r171, [%r36];
ld.shared.u32 %r172, [%r36+16];

	{add.f16x2 %r167,%r37,%r38;
}

	
	{add.f16x2 %r170,%r171,%r172;
}

	
	{sub.f16x2 %r173,%r37,%r38;
}

	
	{sub.f16x2 %r176,%r171,%r172;
}

	and.b32 %r41, %r6, 2;
bfe.u32 %r219, %r6, 1, 1;
cvt.rn.f32.u32	%f17, %r219;
mul.f32 %f18, %f17, 0f3FC90FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r179, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r179;
mov.b32 %r182, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r179;
mov.b32 %r184, {high,high};}


	
	{mul.f16x2 %r186,%r176,%r184;
}

	
	{xor.b32 %r189,%r186,0x80008000;
}

	
	{fma.rn.f16x2 %r191,%r173,%r182,%r189;
}

	
	{mul.f16x2 %r195,%r173,%r184;
}

	
	{fma.rn.f16x2 %r198,%r176,%r182,%r195;
}

	and.b32 %r220, %r6, 1;
add.s32 %r44, %r34, %r220;
barrier.sync 0;
shl.b32 %r221, %r41, 1;
add.s32 %r222, %r221, %r44;
shl.b32 %r223, %r222, 2;
add.s32 %r45, %r163, %r223;
st.shared.u32 [%r45], %r167;
st.shared.u32 [%r45+8], %r191;
barrier.sync 0;
add.s32 %r225, %r41, %r44;
shl.b32 %r226, %r225, 2;
add.s32 %r46, %r163, %r226;
ld.shared.u32 %r47, [%r46];
ld.shared.u32 %r48, [%r46+16];
barrier.sync 0;
st.shared.u32 [%r45], %r170;
st.shared.u32 [%r45+8], %r198;
barrier.sync 0;
ld.shared.u32 %r232, [%r46];
ld.shared.u32 %r233, [%r46+16];

	{add.f16x2 %r228,%r47,%r48;
}

	
	{add.f16x2 %r231,%r232,%r233;
}

	
	{sub.f16x2 %r234,%r47,%r48;
}

	
	{sub.f16x2 %r237,%r232,%r233;
}

	@%p1 bra BB2_11;

setp.eq.s32	%p6, %r1, 1;
selp.b32	%r251, %r234, %r237, %p6;
selp.b32	%r250, %r237, %r234, %p6;
selp.b32	%r245, %r228, %r231, %p6;
selp.b32	%r244, %r231, %r228, %p6;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r244;
mov.b32 {blow,bhigh}, %r245;
mov.b32 %r240, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r244;
mov.b32 {blow,bhigh}, %r245;
mov.b32 %r243, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r250;
mov.b32 {blow,bhigh}, %r251;
mov.b32 %r246, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r250;
mov.b32 {blow,bhigh}, %r251;
mov.b32 %r249, {ahigh,bhigh};}


	shl.b32 %r57, %r3, 1;
shl.b32 %r252, %r2, 4;
add.s32 %r253, %r57, %r252;
shl.b32 %r254, %r253, 3;
add.s32 %r255, %r6, %r254;
mov.u32 %r58, %nctaid.x;
add.s32 %r256, %r58, -1;
setp.lt.u32	%p7, %r2, %r256;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r255, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r257, %r255, 8;
mul.wide.u32 %rd12, %r257, 4;
add.s64 %rd4, %rd10, %rd12;
@%p7 bra BB2_9;
bra.uni BB2_8;

BB2_9:
st.global.u32 [%rd3], %r240;
st.global.u32 [%rd3+16], %r246;
bra.uni BB2_10;

BB2_8:
shl.b32 %r258, %r58, 4;
add.s32 %r259, %r57, %r258;
add.s32 %r260, %r259, -15;
st.global.u32 [%rd3], %r240;
st.global.u32 [%rd3+16], %r246;
setp.ge.u32	%p8, %r260, %r5;
@%p8 bra BB2_11;

BB2_10:
st.global.u32 [%rd4], %r243;
st.global.u32 [%rd4+16], %r249;

BB2_11:
ret;
}


.weak .entry _Z10vector_fftILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<9>;
.reg .b16 %rs<9>;
.reg .f32 %f<29>;
.reg .b32 %r<340>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u32 %r1, [_Z10vector_fftILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd6, [_Z10vector_fftILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z10vector_fftILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r5, [_Z10vector_fftILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
mov.u32 %r2, %ctaid.x;
shl.b32 %r91, %r2, 2;
mov.u32 %r3, %tid.y;
add.s32 %r92, %r91, %r3;
shl.b32 %r4, %r92, 1;
mov.u32 %r6, %tid.x;
setp.ge.u32	%p1, %r4, %r5;
@%p1 bra BB3_6;

shl.b32 %r7, %r3, 1;
shl.b32 %r93, %r2, 3;
add.s32 %r94, %r7, %r93;
shl.b32 %r95, %r94, 4;
mov.u32 %r8, %nctaid.x;
add.s32 %r96, %r8, -1;
setp.lt.u32	%p2, %r2, %r96;
add.s32 %r97, %r95, %r6;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r97, 4;
add.s64 %rd1, %rd7, %rd8;
add.s32 %r98, %r97, 16;
mul.wide.u32 %rd9, %r98, 4;
add.s64 %rd2, %rd7, %rd9;
@%p2 bra BB3_3;
bra.uni BB3_2;

BB3_3:
ld.global.u32 %r335, [%rd1];
ld.global.u32 %r333, [%rd1+32];
bra.uni BB3_4;

BB3_2:
shl.b32 %r100, %r8, 3;
add.s32 %r101, %r7, %r100;
add.s32 %r102, %r101, -7;
ld.global.u32 %r335, [%rd1];
ld.global.u32 %r333, [%rd1+32];
setp.ge.u32	%p3, %r102, %r5;
@%p3 bra BB3_5;

BB3_4:
ld.global.u32 %r334, [%rd2];
ld.global.u32 %r332, [%rd2+32];

BB3_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r335;
mov.b32 {blow,bhigh}, %r334;
mov.b32 %r103, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r335;
mov.b32 {blow,bhigh}, %r334;
mov.b32 %r106, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r333;
mov.b32 {blow,bhigh}, %r332;
mov.b32 %r109, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r333;
mov.b32 {blow,bhigh}, %r332;
mov.b32 %r112, {ahigh,bhigh};}


	setp.eq.s32	%p4, %r1, 1;
selp.b32	%r336, %r109, %r112, %p4;
selp.b32	%r337, %r112, %r109, %p4;
selp.b32	%r338, %r103, %r106, %p4;
selp.b32	%r339, %r106, %r103, %p4;

BB3_6:

	{add.f16x2 %r115,%r339,%r337;
}

	
	{add.f16x2 %r118,%r338,%r336;
}

	
	{sub.f16x2 %r121,%r339,%r337;
}

	
	{sub.f16x2 %r124,%r338,%r336;
}

	and.b32 %r31, %r6, 7;
cvt.rn.f32.u32	%f8, %r31;
mul.f32 %f9, %f8, 0f3EC90FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r127, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r127;
mov.b32 %r130, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r127;
mov.b32 %r132, {high,high};}


	
	{mul.f16x2 %r134,%r124,%r132;
}

	
	{xor.b32 %r137,%r134,0x80008000;
}

	
	{fma.rn.f16x2 %r139,%r121,%r130,%r137;
}

	
	{mul.f16x2 %r143,%r121,%r132;
}

	
	{fma.rn.f16x2 %r146,%r124,%r130,%r143;
}

	shl.b32 %r167, %r6, 1;
and.b32 %r168, %r167, -16;
shl.b32 %r169, %r3, 4;
add.s32 %r34, %r168, %r169;
barrier.sync 0;
shl.b32 %r170, %r31, 1;
add.s32 %r171, %r170, %r34;
shl.b32 %r172, %r171, 2;
mov.u32 %r173, smem_full;
add.s32 %r35, %r173, %r172;
st.shared.u32 [%r35], %r115;
st.shared.u32 [%r35+4], %r139;
barrier.sync 0;
add.s32 %r174, %r31, %r34;
shl.b32 %r175, %r174, 2;
add.s32 %r36, %r173, %r175;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+32];
barrier.sync 0;
st.shared.u32 [%r35], %r118;
st.shared.u32 [%r35+4], %r146;
barrier.sync 0;
ld.shared.u32 %r181, [%r36];
ld.shared.u32 %r182, [%r36+32];

	{add.f16x2 %r177,%r37,%r38;
}

	
	{add.f16x2 %r180,%r181,%r182;
}

	
	{sub.f16x2 %r183,%r37,%r38;
}

	
	{sub.f16x2 %r186,%r181,%r182;
}

	and.b32 %r41, %r6, 6;
bfe.u32 %r229, %r6, 1, 2;
cvt.rn.f32.u32	%f17, %r229;
mul.f32 %f18, %f17, 0f3F490FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r189, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r192, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r194, {high,high};}


	
	{mul.f16x2 %r196,%r186,%r194;
}

	
	{xor.b32 %r199,%r196,0x80008000;
}

	
	{fma.rn.f16x2 %r201,%r183,%r192,%r199;
}

	
	{mul.f16x2 %r205,%r183,%r194;
}

	
	{fma.rn.f16x2 %r208,%r186,%r192,%r205;
}

	and.b32 %r230, %r6, 1;
add.s32 %r44, %r34, %r230;
barrier.sync 0;
shl.b32 %r231, %r41, 1;
add.s32 %r232, %r231, %r44;
shl.b32 %r233, %r232, 2;
add.s32 %r45, %r173, %r233;
st.shared.u32 [%r45], %r177;
st.shared.u32 [%r45+8], %r201;
barrier.sync 0;
add.s32 %r235, %r41, %r44;
shl.b32 %r236, %r235, 2;
add.s32 %r46, %r173, %r236;
ld.shared.u32 %r47, [%r46];
ld.shared.u32 %r48, [%r46+32];
barrier.sync 0;
st.shared.u32 [%r45], %r180;
st.shared.u32 [%r45+8], %r208;
barrier.sync 0;
ld.shared.u32 %r242, [%r46];
ld.shared.u32 %r243, [%r46+32];

	{add.f16x2 %r238,%r47,%r48;
}

	
	{add.f16x2 %r241,%r242,%r243;
}

	
	{sub.f16x2 %r244,%r47,%r48;
}

	
	{sub.f16x2 %r247,%r242,%r243;
}

	and.b32 %r51, %r6, 4;
bfe.u32 %r290, %r6, 2, 1;
cvt.rn.f32.u32	%f26, %r290;
mul.f32 %f27, %f26, 0f3FC90FDB;
cos.approx.f32 %f20, %f27;
sin.approx.f32 %f28, %f27;
neg.f32 %f21, %f28;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f20;
cvt.rn.f16.f32 high, %f21;
mov.b32 %r250, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r250;
mov.b32 %r253, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r250;
mov.b32 %r255, {high,high};}


	
	{mul.f16x2 %r257,%r247,%r255;
}

	
	{xor.b32 %r260,%r257,0x80008000;
}

	
	{fma.rn.f16x2 %r262,%r244,%r253,%r260;
}

	
	{mul.f16x2 %r266,%r244,%r255;
}

	
	{fma.rn.f16x2 %r269,%r247,%r253,%r266;
}

	and.b32 %r291, %r6, 3;
add.s32 %r54, %r34, %r291;
barrier.sync 0;
shl.b32 %r292, %r51, 1;
add.s32 %r293, %r292, %r54;
shl.b32 %r294, %r293, 2;
add.s32 %r55, %r173, %r294;
st.shared.u32 [%r55], %r238;
st.shared.u32 [%r55+16], %r262;
barrier.sync 0;
add.s32 %r296, %r51, %r54;
shl.b32 %r297, %r296, 2;
add.s32 %r56, %r173, %r297;
ld.shared.u32 %r57, [%r56];
ld.shared.u32 %r58, [%r56+32];
barrier.sync 0;
st.shared.u32 [%r55], %r241;
st.shared.u32 [%r55+16], %r269;
barrier.sync 0;
ld.shared.u32 %r303, [%r56];
ld.shared.u32 %r304, [%r56+32];

	{add.f16x2 %r299,%r57,%r58;
}

	
	{add.f16x2 %r302,%r303,%r304;
}

	
	{sub.f16x2 %r305,%r57,%r58;
}

	
	{sub.f16x2 %r308,%r303,%r304;
}

	@%p1 bra BB3_11;

setp.eq.s32	%p6, %r1, 1;
selp.b32	%r322, %r305, %r308, %p6;
selp.b32	%r321, %r308, %r305, %p6;
selp.b32	%r316, %r299, %r302, %p6;
selp.b32	%r315, %r302, %r299, %p6;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r315;
mov.b32 {blow,bhigh}, %r316;
mov.b32 %r311, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r315;
mov.b32 {blow,bhigh}, %r316;
mov.b32 %r314, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r321;
mov.b32 {blow,bhigh}, %r322;
mov.b32 %r317, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r321;
mov.b32 {blow,bhigh}, %r322;
mov.b32 %r320, {ahigh,bhigh};}


	shl.b32 %r67, %r3, 1;
shl.b32 %r323, %r2, 3;
add.s32 %r324, %r67, %r323;
shl.b32 %r325, %r324, 4;
add.s32 %r326, %r6, %r325;
mov.u32 %r68, %nctaid.x;
add.s32 %r327, %r68, -1;
setp.lt.u32	%p7, %r2, %r327;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r326, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r328, %r326, 16;
mul.wide.u32 %rd12, %r328, 4;
add.s64 %rd4, %rd10, %rd12;
@%p7 bra BB3_9;
bra.uni BB3_8;

BB3_9:
st.global.u32 [%rd3], %r311;
st.global.u32 [%rd3+32], %r317;
bra.uni BB3_10;

BB3_8:
shl.b32 %r329, %r68, 3;
add.s32 %r330, %r67, %r329;
add.s32 %r331, %r330, -7;
st.global.u32 [%rd3], %r311;
st.global.u32 [%rd3+32], %r317;
setp.ge.u32	%p8, %r331, %r5;
@%p8 bra BB3_11;

BB3_10:
st.global.u32 [%rd4], %r314;
st.global.u32 [%rd4+32], %r320;

BB3_11:
ret;
}


.weak .entry _Z10vector_fftILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 128, 1, 1
{
.reg .pred %p<9>;
.reg .b16 %rs<9>;
.reg .f32 %f<38>;
.reg .b32 %r<412>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u32 %r1, [_Z10vector_fftILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd6, [_Z10vector_fftILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z10vector_fftILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r5, [_Z10vector_fftILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
mov.u32 %r2, %ctaid.x;
shl.b32 %r101, %r2, 3;
mov.u32 %r3, %tid.y;
add.s32 %r102, %r101, %r3;
shl.b32 %r4, %r102, 1;
mov.u32 %r6, %tid.x;
setp.ge.u32	%p1, %r4, %r5;
@%p1 bra BB4_6;

shl.b32 %r7, %r3, 1;
shl.b32 %r103, %r2, 4;
add.s32 %r104, %r7, %r103;
shl.b32 %r105, %r104, 5;
mov.u32 %r8, %nctaid.x;
add.s32 %r106, %r8, -1;
setp.lt.u32	%p2, %r2, %r106;
add.s32 %r107, %r105, %r6;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r107, 4;
add.s64 %rd1, %rd7, %rd8;
add.s32 %r108, %r107, 32;
mul.wide.u32 %rd9, %r108, 4;
add.s64 %rd2, %rd7, %rd9;
@%p2 bra BB4_3;
bra.uni BB4_2;

BB4_3:
ld.global.u32 %r407, [%rd1];
ld.global.u32 %r405, [%rd1+64];
bra.uni BB4_4;

BB4_2:
shl.b32 %r110, %r8, 4;
add.s32 %r111, %r7, %r110;
add.s32 %r112, %r111, -15;
ld.global.u32 %r407, [%rd1];
ld.global.u32 %r405, [%rd1+64];
setp.ge.u32	%p3, %r112, %r5;
@%p3 bra BB4_5;

BB4_4:
ld.global.u32 %r406, [%rd2];
ld.global.u32 %r404, [%rd2+64];

BB4_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r407;
mov.b32 {blow,bhigh}, %r406;
mov.b32 %r113, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r407;
mov.b32 {blow,bhigh}, %r406;
mov.b32 %r116, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r405;
mov.b32 {blow,bhigh}, %r404;
mov.b32 %r119, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r405;
mov.b32 {blow,bhigh}, %r404;
mov.b32 %r122, {ahigh,bhigh};}


	setp.eq.s32	%p4, %r1, 1;
selp.b32	%r408, %r119, %r122, %p4;
selp.b32	%r409, %r122, %r119, %p4;
selp.b32	%r410, %r113, %r116, %p4;
selp.b32	%r411, %r116, %r113, %p4;

BB4_6:

	{add.f16x2 %r125,%r411,%r409;
}

	
	{add.f16x2 %r128,%r410,%r408;
}

	
	{sub.f16x2 %r131,%r411,%r409;
}

	
	{sub.f16x2 %r134,%r410,%r408;
}

	and.b32 %r31, %r6, 15;
cvt.rn.f32.u32	%f8, %r31;
mul.f32 %f9, %f8, 0f3E490FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r137, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r137;
mov.b32 %r140, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r137;
mov.b32 %r142, {high,high};}


	
	{mul.f16x2 %r144,%r134,%r142;
}

	
	{xor.b32 %r147,%r144,0x80008000;
}

	
	{fma.rn.f16x2 %r149,%r131,%r140,%r147;
}

	
	{mul.f16x2 %r153,%r131,%r142;
}

	
	{fma.rn.f16x2 %r156,%r134,%r140,%r153;
}

	shl.b32 %r177, %r6, 1;
and.b32 %r178, %r177, -32;
shl.b32 %r179, %r3, 5;
add.s32 %r34, %r178, %r179;
barrier.sync 0;
shl.b32 %r180, %r31, 1;
add.s32 %r181, %r180, %r34;
shl.b32 %r182, %r181, 2;
mov.u32 %r183, smem_full;
add.s32 %r35, %r183, %r182;
st.shared.u32 [%r35], %r125;
st.shared.u32 [%r35+4], %r149;
barrier.sync 0;
add.s32 %r184, %r31, %r34;
shl.b32 %r185, %r184, 2;
add.s32 %r36, %r183, %r185;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+64];
barrier.sync 0;
st.shared.u32 [%r35], %r128;
st.shared.u32 [%r35+4], %r156;
barrier.sync 0;
ld.shared.u32 %r191, [%r36];
ld.shared.u32 %r192, [%r36+64];

	{add.f16x2 %r187,%r37,%r38;
}

	
	{add.f16x2 %r190,%r191,%r192;
}

	
	{sub.f16x2 %r193,%r37,%r38;
}

	
	{sub.f16x2 %r196,%r191,%r192;
}

	and.b32 %r41, %r6, 14;
bfe.u32 %r239, %r6, 1, 3;
cvt.rn.f32.u32	%f17, %r239;
mul.f32 %f18, %f17, 0f3EC90FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r199, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r199;
mov.b32 %r202, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r199;
mov.b32 %r204, {high,high};}


	
	{mul.f16x2 %r206,%r196,%r204;
}

	
	{xor.b32 %r209,%r206,0x80008000;
}

	
	{fma.rn.f16x2 %r211,%r193,%r202,%r209;
}

	
	{mul.f16x2 %r215,%r193,%r204;
}

	
	{fma.rn.f16x2 %r218,%r196,%r202,%r215;
}

	neg.s32 %r240, %r6;
and.b32 %r241, %r240, 1;
add.s32 %r44, %r34, %r241;
barrier.sync 0;
shl.b32 %r242, %r41, 1;
add.s32 %r243, %r242, %r44;
shl.b32 %r244, %r243, 2;
add.s32 %r45, %r183, %r244;
st.shared.u32 [%r45], %r187;
st.shared.u32 [%r45+8], %r211;
barrier.sync 0;
add.s32 %r246, %r41, %r44;
shl.b32 %r247, %r246, 2;
add.s32 %r46, %r183, %r247;
ld.shared.u32 %r47, [%r46];
ld.shared.u32 %r48, [%r46+64];
barrier.sync 0;
st.shared.u32 [%r45], %r190;
st.shared.u32 [%r45+8], %r218;
barrier.sync 0;
ld.shared.u32 %r253, [%r46];
ld.shared.u32 %r254, [%r46+64];

	{add.f16x2 %r249,%r47,%r48;
}

	
	{add.f16x2 %r252,%r253,%r254;
}

	
	{sub.f16x2 %r255,%r47,%r48;
}

	
	{sub.f16x2 %r258,%r253,%r254;
}

	and.b32 %r51, %r6, 12;
bfe.u32 %r301, %r6, 2, 2;
cvt.rn.f32.u32	%f26, %r301;
mul.f32 %f27, %f26, 0f3F490FDB;
cos.approx.f32 %f20, %f27;
sin.approx.f32 %f28, %f27;
neg.f32 %f21, %f28;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f20;
cvt.rn.f16.f32 high, %f21;
mov.b32 %r261, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r261;
mov.b32 %r264, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r261;
mov.b32 %r266, {high,high};}


	
	{mul.f16x2 %r268,%r258,%r266;
}

	
	{xor.b32 %r271,%r268,0x80008000;
}

	
	{fma.rn.f16x2 %r273,%r255,%r264,%r271;
}

	
	{mul.f16x2 %r277,%r255,%r266;
}

	
	{fma.rn.f16x2 %r280,%r258,%r264,%r277;
}

	and.b32 %r302, %r6, 3;
add.s32 %r54, %r34, %r302;
barrier.sync 0;
shl.b32 %r303, %r51, 1;
add.s32 %r304, %r303, %r54;
shl.b32 %r305, %r304, 2;
add.s32 %r55, %r183, %r305;
st.shared.u32 [%r55], %r249;
st.shared.u32 [%r55+16], %r273;
barrier.sync 0;
add.s32 %r307, %r51, %r54;
shl.b32 %r308, %r307, 2;
add.s32 %r56, %r183, %r308;
ld.shared.u32 %r57, [%r56];
ld.shared.u32 %r58, [%r56+64];
barrier.sync 0;
st.shared.u32 [%r55], %r252;
st.shared.u32 [%r55+16], %r280;
barrier.sync 0;
ld.shared.u32 %r314, [%r56];
ld.shared.u32 %r315, [%r56+64];

	{add.f16x2 %r310,%r57,%r58;
}

	
	{add.f16x2 %r313,%r314,%r315;
}

	
	{sub.f16x2 %r316,%r57,%r58;
}

	
	{sub.f16x2 %r319,%r314,%r315;
}

	and.b32 %r61, %r6, 8;
bfe.u32 %r362, %r6, 3, 1;
cvt.rn.f32.u32	%f35, %r362;
mul.f32 %f36, %f35, 0f3FC90FDB;
cos.approx.f32 %f29, %f36;
sin.approx.f32 %f37, %f36;
neg.f32 %f30, %f37;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f29;
cvt.rn.f16.f32 high, %f30;
mov.b32 %r322, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r322;
mov.b32 %r325, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r322;
mov.b32 %r327, {high,high};}


	
	{mul.f16x2 %r329,%r319,%r327;
}

	
	{xor.b32 %r332,%r329,0x80008000;
}

	
	{fma.rn.f16x2 %r334,%r316,%r325,%r332;
}

	
	{mul.f16x2 %r338,%r316,%r327;
}

	
	{fma.rn.f16x2 %r341,%r319,%r325,%r338;
}

	and.b32 %r363, %r6, 7;
add.s32 %r64, %r34, %r363;
barrier.sync 0;
shl.b32 %r364, %r61, 1;
add.s32 %r365, %r364, %r64;
shl.b32 %r366, %r365, 2;
add.s32 %r65, %r183, %r366;
st.shared.u32 [%r65], %r310;
st.shared.u32 [%r65+32], %r334;
barrier.sync 0;
add.s32 %r368, %r61, %r64;
shl.b32 %r369, %r368, 2;
add.s32 %r66, %r183, %r369;
ld.shared.u32 %r67, [%r66];
ld.shared.u32 %r68, [%r66+64];
barrier.sync 0;
st.shared.u32 [%r65], %r313;
st.shared.u32 [%r65+32], %r341;
barrier.sync 0;
ld.shared.u32 %r375, [%r66];
ld.shared.u32 %r376, [%r66+64];

	{add.f16x2 %r371,%r67,%r68;
}

	
	{add.f16x2 %r374,%r375,%r376;
}

	
	{sub.f16x2 %r377,%r67,%r68;
}

	
	{sub.f16x2 %r380,%r375,%r376;
}

	@%p1 bra BB4_11;

setp.eq.s32	%p6, %r1, 1;
selp.b32	%r394, %r377, %r380, %p6;
selp.b32	%r393, %r380, %r377, %p6;
selp.b32	%r388, %r371, %r374, %p6;
selp.b32	%r387, %r374, %r371, %p6;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r387;
mov.b32 {blow,bhigh}, %r388;
mov.b32 %r383, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r387;
mov.b32 {blow,bhigh}, %r388;
mov.b32 %r386, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r393;
mov.b32 {blow,bhigh}, %r394;
mov.b32 %r389, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r393;
mov.b32 {blow,bhigh}, %r394;
mov.b32 %r392, {ahigh,bhigh};}


	shl.b32 %r77, %r3, 1;
shl.b32 %r395, %r2, 4;
add.s32 %r396, %r77, %r395;
shl.b32 %r397, %r396, 5;
add.s32 %r398, %r6, %r397;
mov.u32 %r78, %nctaid.x;
add.s32 %r399, %r78, -1;
setp.lt.u32	%p7, %r2, %r399;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r398, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r400, %r398, 32;
mul.wide.u32 %rd12, %r400, 4;
add.s64 %rd4, %rd10, %rd12;
@%p7 bra BB4_9;
bra.uni BB4_8;

BB4_9:
st.global.u32 [%rd3], %r383;
st.global.u32 [%rd3+64], %r389;
bra.uni BB4_10;

BB4_8:
shl.b32 %r401, %r78, 4;
add.s32 %r402, %r77, %r401;
add.s32 %r403, %r402, -15;
st.global.u32 [%rd3], %r383;
st.global.u32 [%rd3+64], %r389;
setp.ge.u32	%p8, %r403, %r5;
@%p8 bra BB4_11;

BB4_10:
st.global.u32 [%rd4], %r386;
st.global.u32 [%rd4+64], %r392;

BB4_11:
ret;
}


.weak .entry _Z10vector_fftILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 16, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<9>;
.reg .f32 %f<28>;
.reg .b32 %r<549>;
.reg .f64 %fd<2>;
.reg .b64 %rd<10>;


ld.param.u32 %r1, [_Z10vector_fftILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u32 %r70, [_Z10vector_fftILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd5, [_Z10vector_fftILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z10vector_fftILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
mov.u32 %r2, %ctaid.x;
shl.b32 %r91, %r2, 7;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r92, %r91, %r5;
cvt.u64.u32	%rd1, %r92;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r92, 4;
add.s64 %rd2, %rd6, %rd7;
@%p2 bra BB5_2;
bra.uni BB5_1;

BB5_2:
ld.global.u32 %r548, [%rd2];
ld.global.u32 %r546, [%rd2+64];
ld.global.u32 %r544, [%rd2+128];
ld.global.u32 %r542, [%rd2+192];
bra.uni BB5_3;

BB5_1:
shl.b32 %r94, %r3, 1;
add.s32 %r95, %r94, -1;
ld.global.u32 %r548, [%rd2];
ld.global.u32 %r546, [%rd2+64];
ld.global.u32 %r544, [%rd2+128];
ld.global.u32 %r542, [%rd2+192];
setp.ge.u32	%p3, %r95, %r1;
@%p3 bra BB5_4;

BB5_3:
ld.global.u32 %r547, [%rd2+256];
ld.global.u32 %r545, [%rd2+320];
ld.global.u32 %r543, [%rd2+384];
ld.global.u32 %r541, [%rd2+448];

BB5_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r548;
mov.b32 {blow,bhigh}, %r547;
mov.b32 %r96, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r548;
mov.b32 {blow,bhigh}, %r547;
mov.b32 %r99, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r546;
mov.b32 {blow,bhigh}, %r545;
mov.b32 %r102, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r546;
mov.b32 {blow,bhigh}, %r545;
mov.b32 %r105, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r544;
mov.b32 {blow,bhigh}, %r543;
mov.b32 %r108, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r544;
mov.b32 {blow,bhigh}, %r543;
mov.b32 %r111, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r542;
mov.b32 {blow,bhigh}, %r541;
mov.b32 %r114, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r542;
mov.b32 {blow,bhigh}, %r541;
mov.b32 %r117, {ahigh,bhigh};}


	setp.eq.s32	%p1, %r70, 1;
selp.b32	%r143, %r114, %r117, %p1;
selp.b32	%r140, %r117, %r114, %p1;
selp.b32	%r131, %r108, %r111, %p1;
selp.b32	%r128, %r111, %r108, %p1;
selp.b32	%r142, %r102, %r105, %p1;
selp.b32	%r139, %r105, %r102, %p1;
selp.b32	%r130, %r96, %r99, %p1;
selp.b32	%r127, %r99, %r96, %p1;

	{add.f16x2 %r120,%r127,%r128;
}

	
	{add.f16x2 %r123,%r130,%r131;
}

	
	{sub.f16x2 %r126,%r127,%r128;
}

	
	{sub.f16x2 %r129,%r130,%r131;
}

	
	{add.f16x2 %r132,%r139,%r140;
}

	
	{add.f16x2 %r135,%r142,%r143;
}

	
	{sub.f16x2 %r138,%r139,%r140;
}

	
	{sub.f16x2 %r141,%r142,%r143;
}

	
	{xor.b32 %r144,%r138,0x80008000;
}

	
	{add.f16x2 %r146,%r120,%r132;
}

	
	{add.f16x2 %r149,%r123,%r135;
}

	
	{sub.f16x2 %r152,%r120,%r132;
}

	
	{sub.f16x2 %r155,%r123,%r135;
}

	
	{add.f16x2 %r158,%r126,%r141;
}

	
	{add.f16x2 %r161,%r129,%r144;
}

	
	{sub.f16x2 %r164,%r126,%r141;
}

	
	{sub.f16x2 %r167,%r129,%r144;
}

	and.b32 %r32, %r5, 15;
cvt.rn.f32.u32	%f12, %r32;
mul.f32 %f13, %f12, 0f3DC90FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r170, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r170;
mov.b32 %r173, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r170;
mov.b32 %r175, {high,high};}


	
	{mul.f16x2 %r177,%r161,%r175;
}

	
	{xor.b32 %r180,%r177,0x80008000;
}

	
	{fma.rn.f16x2 %r182,%r158,%r173,%r180;
}

	
	{mul.f16x2 %r186,%r158,%r175;
}

	
	{fma.rn.f16x2 %r189,%r161,%r173,%r186;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r170;
mov.b32 %r193, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r170;
mov.b32 %r195, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r197, {low,high};}


	
	{mul.f16x2 %r198,%r195,%r197;
}

	
	{mul.f16x2 %r201,%r170,%r193;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r170;
mov.b32 %r204, {high,low};}


	
	{fma.rn.f16x2 %r206,%r198,%r204,%r201;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r206;
mov.b32 %r210, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r206;
mov.b32 %r212, {high,high};}


	
	{mul.f16x2 %r214,%r155,%r212;
}

	
	{xor.b32 %r217,%r214,0x80008000;
}

	
	{fma.rn.f16x2 %r219,%r152,%r210,%r217;
}

	
	{mul.f16x2 %r223,%r152,%r212;
}

	
	{fma.rn.f16x2 %r226,%r155,%r210,%r223;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r170;
mov.b32 %r230, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r170;
mov.b32 %r232, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r234, {low,high};}


	
	{mul.f16x2 %r235,%r232,%r234;
}

	
	{mul.f16x2 %r238,%r206,%r230;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r206;
mov.b32 %r241, {high,low};}


	
	{fma.rn.f16x2 %r243,%r235,%r241,%r238;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r243;
mov.b32 %r247, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r243;
mov.b32 %r249, {high,high};}


	
	{mul.f16x2 %r251,%r167,%r249;
}

	
	{xor.b32 %r254,%r251,0x80008000;
}

	
	{fma.rn.f16x2 %r256,%r164,%r247,%r254;
}

	
	{mul.f16x2 %r260,%r164,%r249;
}

	
	{fma.rn.f16x2 %r263,%r167,%r247,%r260;
}

	shl.b32 %r284, %r5, 2;
and.b32 %r39, %r284, -64;
barrier.sync 0;
shl.b32 %r285, %r32, 2;
add.s32 %r286, %r285, %r39;
shl.b32 %r287, %r286, 2;
mov.u32 %r288, smem_full;
add.s32 %r40, %r288, %r287;
st.shared.u32 [%r40], %r146;
st.shared.u32 [%r40+4], %r182;
st.shared.u32 [%r40+8], %r219;
st.shared.u32 [%r40+12], %r256;
barrier.sync 0;
add.s32 %r289, %r32, %r39;
shl.b32 %r290, %r289, 2;
add.s32 %r41, %r288, %r290;
ld.shared.u32 %r42, [%r41];
ld.shared.u32 %r43, [%r41+64];
ld.shared.u32 %r44, [%r41+128];
ld.shared.u32 %r45, [%r41+192];
barrier.sync 0;
st.shared.u32 [%r40], %r149;
st.shared.u32 [%r40+4], %r189;
st.shared.u32 [%r40+8], %r226;
st.shared.u32 [%r40+12], %r263;
barrier.sync 0;
ld.shared.u32 %r296, [%r41];
ld.shared.u32 %r308, [%r41+64];
ld.shared.u32 %r297, [%r41+128];
ld.shared.u32 %r309, [%r41+192];

	{add.f16x2 %r292,%r42,%r44;
}

	
	{add.f16x2 %r295,%r296,%r297;
}

	
	{sub.f16x2 %r298,%r42,%r44;
}

	
	{sub.f16x2 %r301,%r296,%r297;
}

	
	{add.f16x2 %r304,%r43,%r45;
}

	
	{add.f16x2 %r307,%r308,%r309;
}

	
	{sub.f16x2 %r310,%r43,%r45;
}

	
	{sub.f16x2 %r313,%r308,%r309;
}

	
	{xor.b32 %r316,%r310,0x80008000;
}

	
	{add.f16x2 %r318,%r292,%r304;
}

	
	{add.f16x2 %r321,%r295,%r307;
}

	
	{sub.f16x2 %r324,%r292,%r304;
}

	
	{sub.f16x2 %r327,%r295,%r307;
}

	
	{add.f16x2 %r330,%r298,%r313;
}

	
	{add.f16x2 %r333,%r301,%r316;
}

	
	{sub.f16x2 %r336,%r298,%r313;
}

	
	{sub.f16x2 %r339,%r301,%r316;
}

	and.b32 %r48, %r5, 12;
bfe.u32 %r456, %r5, 2, 2;
cvt.rn.f32.u32	%f25, %r456;
mul.f32 %f26, %f25, 0f3EC90FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r342, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r342;
mov.b32 %r345, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r342;
mov.b32 %r347, {high,high};}


	
	{mul.f16x2 %r349,%r333,%r347;
}

	
	{xor.b32 %r352,%r349,0x80008000;
}

	
	{fma.rn.f16x2 %r354,%r330,%r345,%r352;
}

	
	{mul.f16x2 %r358,%r330,%r347;
}

	
	{fma.rn.f16x2 %r361,%r333,%r345,%r358;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r342;
mov.b32 %r365, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r342;
mov.b32 %r367, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r369, {low,high};}


	
	{mul.f16x2 %r370,%r367,%r369;
}

	
	{mul.f16x2 %r373,%r342,%r365;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r342;
mov.b32 %r376, {high,low};}


	
	{fma.rn.f16x2 %r378,%r370,%r376,%r373;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r378;
mov.b32 %r382, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r378;
mov.b32 %r384, {high,high};}


	
	{mul.f16x2 %r386,%r327,%r384;
}

	
	{xor.b32 %r389,%r386,0x80008000;
}

	
	{fma.rn.f16x2 %r391,%r324,%r382,%r389;
}

	
	{mul.f16x2 %r395,%r324,%r384;
}

	
	{fma.rn.f16x2 %r398,%r327,%r382,%r395;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r342;
mov.b32 %r402, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r342;
mov.b32 %r404, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r406, {low,high};}


	
	{mul.f16x2 %r407,%r404,%r406;
}

	
	{mul.f16x2 %r410,%r378,%r402;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r378;
mov.b32 %r413, {high,low};}


	
	{fma.rn.f16x2 %r415,%r407,%r413,%r410;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r415;
mov.b32 %r419, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r415;
mov.b32 %r421, {high,high};}


	
	{mul.f16x2 %r423,%r339,%r421;
}

	
	{xor.b32 %r426,%r423,0x80008000;
}

	
	{fma.rn.f16x2 %r428,%r336,%r419,%r426;
}

	
	{mul.f16x2 %r432,%r336,%r421;
}

	
	{fma.rn.f16x2 %r435,%r339,%r419,%r432;
}

	and.b32 %r457, %r5, 3;
add.s32 %r55, %r39, %r457;
barrier.sync 0;
shl.b32 %r458, %r48, 2;
add.s32 %r459, %r458, %r55;
shl.b32 %r460, %r459, 2;
add.s32 %r56, %r288, %r460;
st.shared.u32 [%r56], %r318;
st.shared.u32 [%r56+16], %r354;
st.shared.u32 [%r56+32], %r391;
st.shared.u32 [%r56+48], %r428;
barrier.sync 0;
add.s32 %r462, %r48, %r55;
shl.b32 %r463, %r462, 2;
add.s32 %r57, %r288, %r463;
ld.shared.u32 %r58, [%r57];
ld.shared.u32 %r59, [%r57+64];
ld.shared.u32 %r60, [%r57+128];
ld.shared.u32 %r61, [%r57+192];
barrier.sync 0;
st.shared.u32 [%r56], %r321;
st.shared.u32 [%r56+16], %r361;
st.shared.u32 [%r56+32], %r398;
st.shared.u32 [%r56+48], %r435;
barrier.sync 0;
ld.shared.u32 %r469, [%r57];
ld.shared.u32 %r481, [%r57+64];
ld.shared.u32 %r470, [%r57+128];
ld.shared.u32 %r482, [%r57+192];

	{add.f16x2 %r465,%r58,%r60;
}

	
	{add.f16x2 %r468,%r469,%r470;
}

	
	{sub.f16x2 %r471,%r58,%r60;
}

	
	{sub.f16x2 %r474,%r469,%r470;
}

	
	{add.f16x2 %r477,%r59,%r61;
}

	
	{add.f16x2 %r480,%r481,%r482;
}

	
	{sub.f16x2 %r483,%r59,%r61;
}

	
	{sub.f16x2 %r486,%r481,%r482;
}

	
	{xor.b32 %r489,%r483,0x80008000;
}

	
	{add.f16x2 %r491,%r465,%r477;
}

	
	{add.f16x2 %r494,%r468,%r480;
}

	
	{sub.f16x2 %r497,%r465,%r477;
}

	
	{sub.f16x2 %r500,%r468,%r480;
}

	
	{add.f16x2 %r503,%r471,%r486;
}

	
	{add.f16x2 %r506,%r474,%r489;
}

	
	{sub.f16x2 %r509,%r471,%r486;
}

	
	{sub.f16x2 %r512,%r474,%r489;
}

	selp.b32	%r538, %r509, %r512, %p1;
selp.b32	%r537, %r512, %r509, %p1;
selp.b32	%r532, %r497, %r500, %p1;
selp.b32	%r531, %r500, %r497, %p1;
selp.b32	%r526, %r503, %r506, %p1;
selp.b32	%r525, %r506, %r503, %p1;
selp.b32	%r520, %r491, %r494, %p1;
selp.b32	%r519, %r494, %r491, %p1;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r519;
mov.b32 {blow,bhigh}, %r520;
mov.b32 %r515, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r519;
mov.b32 {blow,bhigh}, %r520;
mov.b32 %r518, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r525;
mov.b32 {blow,bhigh}, %r526;
mov.b32 %r521, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r525;
mov.b32 {blow,bhigh}, %r526;
mov.b32 %r524, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r531;
mov.b32 {blow,bhigh}, %r532;
mov.b32 %r527, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r531;
mov.b32 {blow,bhigh}, %r532;
mov.b32 %r530, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r537;
mov.b32 {blow,bhigh}, %r538;
mov.b32 %r533, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r537;
mov.b32 {blow,bhigh}, %r538;
mov.b32 %r536, {ahigh,bhigh};}


	cvta.to.global.u64 %rd8, %rd5;
shl.b64 %rd9, %rd1, 2;
add.s64 %rd3, %rd8, %rd9;
@%p2 bra BB5_6;
bra.uni BB5_5;

BB5_6:
st.global.u32 [%rd3], %r515;
st.global.u32 [%rd3+64], %r521;
st.global.u32 [%rd3+128], %r527;
st.global.u32 [%rd3+192], %r533;
bra.uni BB5_7;

BB5_5:
shl.b32 %r539, %r3, 1;
add.s32 %r540, %r539, -1;
st.global.u32 [%rd3], %r515;
st.global.u32 [%rd3+64], %r521;
st.global.u32 [%rd3+128], %r527;
st.global.u32 [%rd3+192], %r533;
setp.ge.u32	%p5, %r540, %r1;
@%p5 bra BB5_8;

BB5_7:
st.global.u32 [%rd3+256], %r518;
st.global.u32 [%rd3+320], %r524;
st.global.u32 [%rd3+384], %r530;
st.global.u32 [%rd3+448], %r536;

BB5_8:
ret;
}


.weak .entry _Z10vector_fftILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<9>;
.reg .f32 %f<41>;
.reg .b32 %r<715>;
.reg .f64 %fd<2>;
.reg .b64 %rd<10>;


ld.param.u32 %r1, [_Z10vector_fftILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u32 %r85, [_Z10vector_fftILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd5, [_Z10vector_fftILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z10vector_fftILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
mov.u32 %r106, %ctaid.x;
shl.b32 %r107, %r106, 8;
mov.u32 %r2, %nctaid.x;
add.s32 %r3, %r2, -1;
setp.lt.u32	%p3, %r106, %r3;
mov.u32 %r108, %tid.x;
add.s32 %r109, %r107, %r108;
cvt.u64.u32	%rd1, %r109;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r109, 4;
add.s64 %rd2, %rd6, %rd7;
@%p3 bra BB6_2;
bra.uni BB6_1;

BB6_2:
ld.global.u32 %r714, [%rd2];
ld.global.u32 %r712, [%rd2+128];
ld.global.u32 %r710, [%rd2+256];
ld.global.u32 %r708, [%rd2+384];
bra.uni BB6_3;

BB6_1:
shl.b32 %r111, %r2, 1;
add.s32 %r112, %r111, -1;
ld.global.u32 %r714, [%rd2];
ld.global.u32 %r712, [%rd2+128];
ld.global.u32 %r710, [%rd2+256];
ld.global.u32 %r708, [%rd2+384];
setp.ge.u32	%p4, %r112, %r1;
@%p4 bra BB6_4;

BB6_3:
ld.global.u32 %r713, [%rd2+512];
ld.global.u32 %r711, [%rd2+640];
ld.global.u32 %r709, [%rd2+768];
ld.global.u32 %r707, [%rd2+896];

BB6_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r714;
mov.b32 {blow,bhigh}, %r713;
mov.b32 %r113, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r714;
mov.b32 {blow,bhigh}, %r713;
mov.b32 %r116, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r712;
mov.b32 {blow,bhigh}, %r711;
mov.b32 %r119, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r712;
mov.b32 {blow,bhigh}, %r711;
mov.b32 %r122, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r710;
mov.b32 {blow,bhigh}, %r709;
mov.b32 %r125, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r710;
mov.b32 {blow,bhigh}, %r709;
mov.b32 %r128, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r708;
mov.b32 {blow,bhigh}, %r707;
mov.b32 %r131, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r708;
mov.b32 {blow,bhigh}, %r707;
mov.b32 %r134, {ahigh,bhigh};}


	setp.eq.s32	%p2, %r85, 1;
selp.b32	%r160, %r131, %r134, %p2;
selp.b32	%r157, %r134, %r131, %p2;
selp.b32	%r148, %r125, %r128, %p2;
selp.b32	%r145, %r128, %r125, %p2;
selp.b32	%r159, %r119, %r122, %p2;
selp.b32	%r156, %r122, %r119, %p2;
selp.b32	%r147, %r113, %r116, %p2;
selp.b32	%r144, %r116, %r113, %p2;

	{add.f16x2 %r137,%r144,%r145;
}

	
	{add.f16x2 %r140,%r147,%r148;
}

	
	{sub.f16x2 %r143,%r144,%r145;
}

	
	{sub.f16x2 %r146,%r147,%r148;
}

	
	{add.f16x2 %r149,%r156,%r157;
}

	
	{add.f16x2 %r152,%r159,%r160;
}

	
	{sub.f16x2 %r155,%r156,%r157;
}

	
	{sub.f16x2 %r158,%r159,%r160;
}

	
	{xor.b32 %r161,%r155,0x80008000;
}

	
	{add.f16x2 %r163,%r137,%r149;
}

	
	{add.f16x2 %r166,%r140,%r152;
}

	
	{sub.f16x2 %r169,%r137,%r149;
}

	
	{sub.f16x2 %r172,%r140,%r152;
}

	
	{add.f16x2 %r175,%r143,%r158;
}

	
	{add.f16x2 %r178,%r146,%r161;
}

	
	{sub.f16x2 %r181,%r143,%r158;
}

	
	{sub.f16x2 %r184,%r146,%r161;
}

	and.b32 %r31, %r108, 31;
cvt.rn.f32.u32	%f12, %r31;
mul.f32 %f13, %f12, 0f3D490FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r187, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r187;
mov.b32 %r190, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r187;
mov.b32 %r192, {high,high};}


	
	{mul.f16x2 %r194,%r178,%r192;
}

	
	{xor.b32 %r197,%r194,0x80008000;
}

	
	{fma.rn.f16x2 %r199,%r175,%r190,%r197;
}

	
	{mul.f16x2 %r203,%r175,%r192;
}

	
	{fma.rn.f16x2 %r206,%r178,%r190,%r203;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r187;
mov.b32 %r210, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r187;
mov.b32 %r212, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r214, {low,high};}


	
	{mul.f16x2 %r215,%r212,%r214;
}

	
	{mul.f16x2 %r218,%r187,%r210;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r187;
mov.b32 %r221, {high,low};}


	
	{fma.rn.f16x2 %r223,%r215,%r221,%r218;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r223;
mov.b32 %r227, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r223;
mov.b32 %r229, {high,high};}


	
	{mul.f16x2 %r231,%r172,%r229;
}

	
	{xor.b32 %r234,%r231,0x80008000;
}

	
	{fma.rn.f16x2 %r236,%r169,%r227,%r234;
}

	
	{mul.f16x2 %r240,%r169,%r229;
}

	
	{fma.rn.f16x2 %r243,%r172,%r227,%r240;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r187;
mov.b32 %r247, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r187;
mov.b32 %r249, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r251, {low,high};}


	
	{mul.f16x2 %r252,%r249,%r251;
}

	
	{mul.f16x2 %r255,%r223,%r247;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r223;
mov.b32 %r258, {high,low};}


	
	{fma.rn.f16x2 %r260,%r252,%r258,%r255;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r260;
mov.b32 %r264, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r260;
mov.b32 %r266, {high,high};}


	
	{mul.f16x2 %r268,%r184,%r266;
}

	
	{xor.b32 %r271,%r268,0x80008000;
}

	
	{fma.rn.f16x2 %r273,%r181,%r264,%r271;
}

	
	{mul.f16x2 %r277,%r181,%r266;
}

	
	{fma.rn.f16x2 %r280,%r184,%r264,%r277;
}

	shl.b32 %r302, %r108, 2;
and.b32 %r38, %r302, -128;
barrier.sync 0;
shl.b32 %r303, %r31, 2;
add.s32 %r304, %r303, %r38;
shl.b32 %r305, %r304, 2;
mov.u32 %r306, smem_full;
add.s32 %r39, %r306, %r305;
st.shared.u32 [%r39], %r163;
st.shared.u32 [%r39+4], %r199;
st.shared.u32 [%r39+8], %r236;
st.shared.u32 [%r39+12], %r273;
barrier.sync 0;
add.s32 %r307, %r31, %r38;
shl.b32 %r308, %r307, 2;
add.s32 %r40, %r306, %r308;
ld.shared.u32 %r41, [%r40];
ld.shared.u32 %r42, [%r40+128];
ld.shared.u32 %r43, [%r40+256];
ld.shared.u32 %r44, [%r40+384];
barrier.sync 0;
st.shared.u32 [%r39], %r166;
st.shared.u32 [%r39+4], %r206;
st.shared.u32 [%r39+8], %r243;
st.shared.u32 [%r39+12], %r280;
barrier.sync 0;
ld.shared.u32 %r314, [%r40];
ld.shared.u32 %r326, [%r40+128];
ld.shared.u32 %r315, [%r40+256];
ld.shared.u32 %r327, [%r40+384];

	{add.f16x2 %r310,%r41,%r43;
}

	
	{add.f16x2 %r313,%r314,%r315;
}

	
	{sub.f16x2 %r316,%r41,%r43;
}

	
	{sub.f16x2 %r319,%r314,%r315;
}

	
	{add.f16x2 %r322,%r42,%r44;
}

	
	{add.f16x2 %r325,%r326,%r327;
}

	
	{sub.f16x2 %r328,%r42,%r44;
}

	
	{sub.f16x2 %r331,%r326,%r327;
}

	
	{xor.b32 %r334,%r328,0x80008000;
}

	
	{add.f16x2 %r336,%r310,%r322;
}

	
	{add.f16x2 %r339,%r313,%r325;
}

	
	{sub.f16x2 %r342,%r310,%r322;
}

	
	{sub.f16x2 %r345,%r313,%r325;
}

	
	{add.f16x2 %r348,%r316,%r331;
}

	
	{add.f16x2 %r351,%r319,%r334;
}

	
	{sub.f16x2 %r354,%r316,%r331;
}

	
	{sub.f16x2 %r357,%r319,%r334;
}

	shr.u32 %r47, %r31, 2;
cvt.rn.f32.u32	%f25, %r47;
mul.f32 %f26, %f25, 0f3E490FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r360, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r360;
mov.b32 %r363, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r360;
mov.b32 %r365, {high,high};}


	
	{mul.f16x2 %r367,%r351,%r365;
}

	
	{xor.b32 %r370,%r367,0x80008000;
}

	
	{fma.rn.f16x2 %r372,%r348,%r363,%r370;
}

	
	{mul.f16x2 %r376,%r348,%r365;
}

	
	{fma.rn.f16x2 %r379,%r351,%r363,%r376;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r360;
mov.b32 %r383, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r360;
mov.b32 %r385, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r387, {low,high};}


	
	{mul.f16x2 %r388,%r385,%r387;
}

	
	{mul.f16x2 %r391,%r360,%r383;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r360;
mov.b32 %r394, {high,low};}


	
	{fma.rn.f16x2 %r396,%r388,%r394,%r391;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r396;
mov.b32 %r400, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r396;
mov.b32 %r402, {high,high};}


	
	{mul.f16x2 %r404,%r345,%r402;
}

	
	{xor.b32 %r407,%r404,0x80008000;
}

	
	{fma.rn.f16x2 %r409,%r342,%r400,%r407;
}

	
	{mul.f16x2 %r413,%r342,%r402;
}

	
	{fma.rn.f16x2 %r416,%r345,%r400,%r413;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r360;
mov.b32 %r420, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r360;
mov.b32 %r422, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r424, {low,high};}


	
	{mul.f16x2 %r425,%r422,%r424;
}

	
	{mul.f16x2 %r428,%r396,%r420;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r396;
mov.b32 %r431, {high,low};}


	
	{fma.rn.f16x2 %r433,%r425,%r431,%r428;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r433;
mov.b32 %r437, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r433;
mov.b32 %r439, {high,high};}


	
	{mul.f16x2 %r441,%r357,%r439;
}

	
	{xor.b32 %r444,%r441,0x80008000;
}

	
	{fma.rn.f16x2 %r446,%r354,%r437,%r444;
}

	
	{mul.f16x2 %r450,%r354,%r439;
}

	
	{fma.rn.f16x2 %r453,%r357,%r437,%r450;
}

	and.b32 %r474, %r108, 3;
add.s32 %r54, %r38, %r474;
barrier.sync 0;
shl.b32 %r475, %r47, 4;
add.s32 %r476, %r475, %r54;
shl.b32 %r477, %r476, 2;
add.s32 %r55, %r306, %r477;
st.shared.u32 [%r55], %r336;
st.shared.u32 [%r55+16], %r372;
st.shared.u32 [%r55+32], %r409;
st.shared.u32 [%r55+48], %r446;
barrier.sync 0;
shl.b32 %r479, %r47, 2;
add.s32 %r480, %r479, %r54;
shl.b32 %r481, %r480, 2;
add.s32 %r56, %r306, %r481;
ld.shared.u32 %r57, [%r56];
ld.shared.u32 %r58, [%r56+128];
ld.shared.u32 %r59, [%r56+256];
ld.shared.u32 %r60, [%r56+384];
barrier.sync 0;
st.shared.u32 [%r55], %r339;
st.shared.u32 [%r55+16], %r379;
st.shared.u32 [%r55+32], %r416;
st.shared.u32 [%r55+48], %r453;
barrier.sync 0;
ld.shared.u32 %r487, [%r56];
ld.shared.u32 %r499, [%r56+128];
ld.shared.u32 %r488, [%r56+256];
ld.shared.u32 %r500, [%r56+384];

	{add.f16x2 %r483,%r57,%r59;
}

	
	{add.f16x2 %r486,%r487,%r488;
}

	
	{sub.f16x2 %r489,%r57,%r59;
}

	
	{sub.f16x2 %r492,%r487,%r488;
}

	
	{add.f16x2 %r495,%r58,%r60;
}

	
	{add.f16x2 %r498,%r499,%r500;
}

	
	{sub.f16x2 %r501,%r58,%r60;
}

	
	{sub.f16x2 %r504,%r499,%r500;
}

	
	{xor.b32 %r507,%r501,0x80008000;
}

	
	{add.f16x2 %r509,%r483,%r495;
}

	
	{add.f16x2 %r512,%r486,%r498;
}

	
	{sub.f16x2 %r515,%r483,%r495;
}

	
	{sub.f16x2 %r518,%r486,%r498;
}

	
	{add.f16x2 %r521,%r489,%r504;
}

	
	{add.f16x2 %r524,%r492,%r507;
}

	
	{sub.f16x2 %r527,%r489,%r504;
}

	
	{sub.f16x2 %r530,%r492,%r507;
}

	shr.u32 %r63, %r31, 4;
cvt.rn.f32.u32	%f38, %r63;
mul.f32 %f39, %f38, 0f3F490FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r533, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r533;
mov.b32 %r536, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r533;
mov.b32 %r538, {high,high};}


	
	{mul.f16x2 %r540,%r524,%r538;
}

	
	{xor.b32 %r543,%r540,0x80008000;
}

	
	{fma.rn.f16x2 %r545,%r521,%r536,%r543;
}

	
	{mul.f16x2 %r549,%r521,%r538;
}

	
	{fma.rn.f16x2 %r552,%r524,%r536,%r549;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r533;
mov.b32 %r556, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r533;
mov.b32 %r558, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r560, {low,high};}


	
	{mul.f16x2 %r561,%r558,%r560;
}

	
	{mul.f16x2 %r564,%r533,%r556;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r533;
mov.b32 %r567, {high,low};}


	
	{fma.rn.f16x2 %r569,%r561,%r567,%r564;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r569;
mov.b32 %r573, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r569;
mov.b32 %r575, {high,high};}


	
	{mul.f16x2 %r577,%r518,%r575;
}

	
	{xor.b32 %r580,%r577,0x80008000;
}

	
	{fma.rn.f16x2 %r582,%r515,%r573,%r580;
}

	
	{mul.f16x2 %r586,%r515,%r575;
}

	
	{fma.rn.f16x2 %r589,%r518,%r573,%r586;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r533;
mov.b32 %r593, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r533;
mov.b32 %r595, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r597, {low,high};}


	
	{mul.f16x2 %r598,%r595,%r597;
}

	
	{mul.f16x2 %r601,%r569,%r593;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r569;
mov.b32 %r604, {high,low};}


	
	{fma.rn.f16x2 %r606,%r598,%r604,%r601;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r606;
mov.b32 %r610, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r606;
mov.b32 %r612, {high,high};}


	
	{mul.f16x2 %r614,%r530,%r612;
}

	
	{xor.b32 %r617,%r614,0x80008000;
}

	
	{fma.rn.f16x2 %r619,%r527,%r610,%r617;
}

	
	{mul.f16x2 %r623,%r527,%r612;
}

	
	{fma.rn.f16x2 %r626,%r530,%r610,%r623;
}

	and.b32 %r647, %r108, 15;
add.s32 %r70, %r38, %r647;
barrier.sync 0;
shl.b32 %r648, %r63, 6;
add.s32 %r649, %r648, %r70;
shl.b32 %r650, %r649, 2;
add.s32 %r71, %r306, %r650;
st.shared.u32 [%r71], %r509;
st.shared.u32 [%r71+64], %r545;
st.shared.u32 [%r71+128], %r582;
st.shared.u32 [%r71+192], %r619;
barrier.sync 0;
shl.b32 %r652, %r63, 4;
add.s32 %r653, %r652, %r70;
shl.b32 %r654, %r653, 2;
add.s32 %r72, %r306, %r654;
ld.shared.u32 %r73, [%r72];
ld.shared.u32 %r74, [%r72+128];
ld.shared.u32 %r75, [%r72+256];
ld.shared.u32 %r76, [%r72+384];
barrier.sync 0;
st.shared.u32 [%r71], %r512;
st.shared.u32 [%r71+64], %r552;
st.shared.u32 [%r71+128], %r589;
st.shared.u32 [%r71+192], %r626;
barrier.sync 0;
ld.shared.u32 %r660, [%r72];
ld.shared.u32 %r672, [%r72+128];
ld.shared.u32 %r661, [%r72+256];
ld.shared.u32 %r673, [%r72+384];

	{add.f16x2 %r656,%r73,%r75;
}

	
	{add.f16x2 %r659,%r660,%r661;
}

	
	{sub.f16x2 %r662,%r73,%r75;
}

	
	{sub.f16x2 %r665,%r660,%r661;
}

	
	{add.f16x2 %r668,%r74,%r76;
}

	
	{add.f16x2 %r671,%r672,%r673;
}

	
	{sub.f16x2 %r674,%r74,%r76;
}

	
	{sub.f16x2 %r677,%r672,%r673;
}

	selp.b32	%r703, %r674, %r677, %p2;
selp.b32	%r702, %r677, %r674, %p2;
selp.b32	%r697, %r662, %r665, %p2;
selp.b32	%r696, %r665, %r662, %p2;
selp.b32	%r691, %r668, %r671, %p2;
selp.b32	%r690, %r671, %r668, %p2;
selp.b32	%r685, %r656, %r659, %p2;
selp.b32	%r684, %r659, %r656, %p2;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r684;
mov.b32 {blow,bhigh}, %r685;
mov.b32 %r680, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r684;
mov.b32 {blow,bhigh}, %r685;
mov.b32 %r683, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r690;
mov.b32 {blow,bhigh}, %r691;
mov.b32 %r686, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r690;
mov.b32 {blow,bhigh}, %r691;
mov.b32 %r689, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r696;
mov.b32 {blow,bhigh}, %r697;
mov.b32 %r692, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r696;
mov.b32 {blow,bhigh}, %r697;
mov.b32 %r695, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r702;
mov.b32 {blow,bhigh}, %r703;
mov.b32 %r698, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r702;
mov.b32 {blow,bhigh}, %r703;
mov.b32 %r701, {ahigh,bhigh};}


	cvta.to.global.u64 %rd8, %rd5;
shl.b64 %rd9, %rd1, 2;
add.s64 %rd3, %rd8, %rd9;
@%p3 bra BB6_6;
bra.uni BB6_5;

BB6_6:
st.global.u32 [%rd3], %r680;
st.global.u32 [%rd3+128], %r686;
st.global.u32 [%rd3+256], %r692;
st.global.u32 [%rd3+384], %r698;
bra.uni BB6_7;

BB6_5:
shl.b32 %r705, %r2, 1;
add.s32 %r706, %r705, -1;
st.global.u32 [%rd3], %r680;
st.global.u32 [%rd3+128], %r686;
st.global.u32 [%rd3+256], %r692;
st.global.u32 [%rd3+384], %r698;
setp.ge.u32	%p5, %r706, %r1;
@%p5 bra BB6_8;

BB6_7:
st.global.u32 [%rd3+512], %r683;
st.global.u32 [%rd3+640], %r689;
st.global.u32 [%rd3+768], %r695;
st.global.u32 [%rd3+896], %r701;

BB6_8:
ret;
}


.weak .entry _Z10vector_fftILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 64, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<9>;
.reg .f32 %f<41>;
.reg .b32 %r<738>;
.reg .f64 %fd<2>;
.reg .b64 %rd<10>;


ld.param.u32 %r1, [_Z10vector_fftILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u32 %r86, [_Z10vector_fftILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd5, [_Z10vector_fftILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z10vector_fftILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
mov.u32 %r2, %ctaid.x;
shl.b32 %r107, %r2, 9;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r108, %r107, %r5;
cvt.u64.u32	%rd1, %r108;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r108, 4;
add.s64 %rd2, %rd6, %rd7;
@%p2 bra BB7_2;
bra.uni BB7_1;

BB7_2:
ld.global.u32 %r737, [%rd2];
ld.global.u32 %r735, [%rd2+256];
ld.global.u32 %r733, [%rd2+512];
ld.global.u32 %r731, [%rd2+768];
bra.uni BB7_3;

BB7_1:
shl.b32 %r110, %r3, 1;
add.s32 %r111, %r110, -1;
ld.global.u32 %r737, [%rd2];
ld.global.u32 %r735, [%rd2+256];
ld.global.u32 %r733, [%rd2+512];
ld.global.u32 %r731, [%rd2+768];
setp.ge.u32	%p3, %r111, %r1;
@%p3 bra BB7_4;

BB7_3:
ld.global.u32 %r736, [%rd2+1024];
ld.global.u32 %r734, [%rd2+1280];
ld.global.u32 %r732, [%rd2+1536];
ld.global.u32 %r730, [%rd2+1792];

BB7_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r737;
mov.b32 {blow,bhigh}, %r736;
mov.b32 %r112, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r737;
mov.b32 {blow,bhigh}, %r736;
mov.b32 %r115, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r735;
mov.b32 {blow,bhigh}, %r734;
mov.b32 %r118, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r735;
mov.b32 {blow,bhigh}, %r734;
mov.b32 %r121, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r733;
mov.b32 {blow,bhigh}, %r732;
mov.b32 %r124, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r733;
mov.b32 {blow,bhigh}, %r732;
mov.b32 %r127, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r731;
mov.b32 {blow,bhigh}, %r730;
mov.b32 %r130, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r731;
mov.b32 {blow,bhigh}, %r730;
mov.b32 %r133, {ahigh,bhigh};}


	setp.eq.s32	%p1, %r86, 1;
selp.b32	%r159, %r130, %r133, %p1;
selp.b32	%r156, %r133, %r130, %p1;
selp.b32	%r147, %r124, %r127, %p1;
selp.b32	%r144, %r127, %r124, %p1;
selp.b32	%r158, %r118, %r121, %p1;
selp.b32	%r155, %r121, %r118, %p1;
selp.b32	%r146, %r112, %r115, %p1;
selp.b32	%r143, %r115, %r112, %p1;

	{add.f16x2 %r136,%r143,%r144;
}

	
	{add.f16x2 %r139,%r146,%r147;
}

	
	{sub.f16x2 %r142,%r143,%r144;
}

	
	{sub.f16x2 %r145,%r146,%r147;
}

	
	{add.f16x2 %r148,%r155,%r156;
}

	
	{add.f16x2 %r151,%r158,%r159;
}

	
	{sub.f16x2 %r154,%r155,%r156;
}

	
	{sub.f16x2 %r157,%r158,%r159;
}

	
	{xor.b32 %r160,%r154,0x80008000;
}

	
	{add.f16x2 %r162,%r136,%r148;
}

	
	{add.f16x2 %r165,%r139,%r151;
}

	
	{sub.f16x2 %r168,%r136,%r148;
}

	
	{sub.f16x2 %r171,%r139,%r151;
}

	
	{add.f16x2 %r174,%r142,%r157;
}

	
	{add.f16x2 %r177,%r145,%r160;
}

	
	{sub.f16x2 %r180,%r142,%r157;
}

	
	{sub.f16x2 %r183,%r145,%r160;
}

	and.b32 %r32, %r5, 63;
cvt.rn.f32.u32	%f12, %r32;
mul.f32 %f13, %f12, 0f3CC90FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r186, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r186;
mov.b32 %r189, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r186;
mov.b32 %r191, {high,high};}


	
	{mul.f16x2 %r193,%r177,%r191;
}

	
	{xor.b32 %r196,%r193,0x80008000;
}

	
	{fma.rn.f16x2 %r198,%r174,%r189,%r196;
}

	
	{mul.f16x2 %r202,%r174,%r191;
}

	
	{fma.rn.f16x2 %r205,%r177,%r189,%r202;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r186;
mov.b32 %r209, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r186;
mov.b32 %r211, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r213, {low,high};}


	
	{mul.f16x2 %r214,%r211,%r213;
}

	
	{mul.f16x2 %r217,%r186,%r209;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r186;
mov.b32 %r220, {high,low};}


	
	{fma.rn.f16x2 %r222,%r214,%r220,%r217;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r222;
mov.b32 %r226, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r222;
mov.b32 %r228, {high,high};}


	
	{mul.f16x2 %r230,%r171,%r228;
}

	
	{xor.b32 %r233,%r230,0x80008000;
}

	
	{fma.rn.f16x2 %r235,%r168,%r226,%r233;
}

	
	{mul.f16x2 %r239,%r168,%r228;
}

	
	{fma.rn.f16x2 %r242,%r171,%r226,%r239;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r186;
mov.b32 %r246, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r186;
mov.b32 %r248, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r250, {low,high};}


	
	{mul.f16x2 %r251,%r248,%r250;
}

	
	{mul.f16x2 %r254,%r222,%r246;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r222;
mov.b32 %r257, {high,low};}


	
	{fma.rn.f16x2 %r259,%r251,%r257,%r254;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r259;
mov.b32 %r263, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r259;
mov.b32 %r265, {high,high};}


	
	{mul.f16x2 %r267,%r183,%r265;
}

	
	{xor.b32 %r270,%r267,0x80008000;
}

	
	{fma.rn.f16x2 %r272,%r180,%r263,%r270;
}

	
	{mul.f16x2 %r276,%r180,%r265;
}

	
	{fma.rn.f16x2 %r279,%r183,%r263,%r276;
}

	shl.b32 %r300, %r5, 2;
and.b32 %r39, %r300, -256;
barrier.sync 0;
shl.b32 %r301, %r32, 2;
add.s32 %r302, %r301, %r39;
shl.b32 %r303, %r302, 2;
mov.u32 %r304, smem_full;
add.s32 %r40, %r304, %r303;
st.shared.u32 [%r40], %r162;
st.shared.u32 [%r40+4], %r198;
st.shared.u32 [%r40+8], %r235;
st.shared.u32 [%r40+12], %r272;
barrier.sync 0;
add.s32 %r305, %r32, %r39;
shl.b32 %r306, %r305, 2;
add.s32 %r41, %r304, %r306;
ld.shared.u32 %r42, [%r41];
ld.shared.u32 %r43, [%r41+256];
ld.shared.u32 %r44, [%r41+512];
ld.shared.u32 %r45, [%r41+768];
barrier.sync 0;
st.shared.u32 [%r40], %r165;
st.shared.u32 [%r40+4], %r205;
st.shared.u32 [%r40+8], %r242;
st.shared.u32 [%r40+12], %r279;
barrier.sync 0;
ld.shared.u32 %r312, [%r41];
ld.shared.u32 %r324, [%r41+256];
ld.shared.u32 %r313, [%r41+512];
ld.shared.u32 %r325, [%r41+768];

	{add.f16x2 %r308,%r42,%r44;
}

	
	{add.f16x2 %r311,%r312,%r313;
}

	
	{sub.f16x2 %r314,%r42,%r44;
}

	
	{sub.f16x2 %r317,%r312,%r313;
}

	
	{add.f16x2 %r320,%r43,%r45;
}

	
	{add.f16x2 %r323,%r324,%r325;
}

	
	{sub.f16x2 %r326,%r43,%r45;
}

	
	{sub.f16x2 %r329,%r324,%r325;
}

	
	{xor.b32 %r332,%r326,0x80008000;
}

	
	{add.f16x2 %r334,%r308,%r320;
}

	
	{add.f16x2 %r337,%r311,%r323;
}

	
	{sub.f16x2 %r340,%r308,%r320;
}

	
	{sub.f16x2 %r343,%r311,%r323;
}

	
	{add.f16x2 %r346,%r314,%r329;
}

	
	{add.f16x2 %r349,%r317,%r332;
}

	
	{sub.f16x2 %r352,%r314,%r329;
}

	
	{sub.f16x2 %r355,%r317,%r332;
}

	and.b32 %r48, %r5, 60;
bfe.u32 %r472, %r5, 2, 4;
cvt.rn.f32.u32	%f25, %r472;
mul.f32 %f26, %f25, 0f3DC90FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r358, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r361, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r363, {high,high};}


	
	{mul.f16x2 %r365,%r349,%r363;
}

	
	{xor.b32 %r368,%r365,0x80008000;
}

	
	{fma.rn.f16x2 %r370,%r346,%r361,%r368;
}

	
	{mul.f16x2 %r374,%r346,%r363;
}

	
	{fma.rn.f16x2 %r377,%r349,%r361,%r374;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r381, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r383, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r385, {low,high};}


	
	{mul.f16x2 %r386,%r383,%r385;
}

	
	{mul.f16x2 %r389,%r358,%r381;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r392, {high,low};}


	
	{fma.rn.f16x2 %r394,%r386,%r392,%r389;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r394;
mov.b32 %r398, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r394;
mov.b32 %r400, {high,high};}


	
	{mul.f16x2 %r402,%r343,%r400;
}

	
	{xor.b32 %r405,%r402,0x80008000;
}

	
	{fma.rn.f16x2 %r407,%r340,%r398,%r405;
}

	
	{mul.f16x2 %r411,%r340,%r400;
}

	
	{fma.rn.f16x2 %r414,%r343,%r398,%r411;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r418, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r420, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r422, {low,high};}


	
	{mul.f16x2 %r423,%r420,%r422;
}

	
	{mul.f16x2 %r426,%r394,%r418;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r394;
mov.b32 %r429, {high,low};}


	
	{fma.rn.f16x2 %r431,%r423,%r429,%r426;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r431;
mov.b32 %r435, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r431;
mov.b32 %r437, {high,high};}


	
	{mul.f16x2 %r439,%r355,%r437;
}

	
	{xor.b32 %r442,%r439,0x80008000;
}

	
	{fma.rn.f16x2 %r444,%r352,%r435,%r442;
}

	
	{mul.f16x2 %r448,%r352,%r437;
}

	
	{fma.rn.f16x2 %r451,%r355,%r435,%r448;
}

	and.b32 %r473, %r5, 3;
add.s32 %r55, %r39, %r473;
barrier.sync 0;
shl.b32 %r474, %r48, 2;
add.s32 %r475, %r474, %r55;
shl.b32 %r476, %r475, 2;
add.s32 %r56, %r304, %r476;
st.shared.u32 [%r56], %r334;
st.shared.u32 [%r56+16], %r370;
st.shared.u32 [%r56+32], %r407;
st.shared.u32 [%r56+48], %r444;
barrier.sync 0;
add.s32 %r478, %r48, %r55;
shl.b32 %r479, %r478, 2;
add.s32 %r57, %r304, %r479;
ld.shared.u32 %r58, [%r57];
ld.shared.u32 %r59, [%r57+256];
ld.shared.u32 %r60, [%r57+512];
ld.shared.u32 %r61, [%r57+768];
barrier.sync 0;
st.shared.u32 [%r56], %r337;
st.shared.u32 [%r56+16], %r377;
st.shared.u32 [%r56+32], %r414;
st.shared.u32 [%r56+48], %r451;
barrier.sync 0;
ld.shared.u32 %r485, [%r57];
ld.shared.u32 %r497, [%r57+256];
ld.shared.u32 %r486, [%r57+512];
ld.shared.u32 %r498, [%r57+768];

	{add.f16x2 %r481,%r58,%r60;
}

	
	{add.f16x2 %r484,%r485,%r486;
}

	
	{sub.f16x2 %r487,%r58,%r60;
}

	
	{sub.f16x2 %r490,%r485,%r486;
}

	
	{add.f16x2 %r493,%r59,%r61;
}

	
	{add.f16x2 %r496,%r497,%r498;
}

	
	{sub.f16x2 %r499,%r59,%r61;
}

	
	{sub.f16x2 %r502,%r497,%r498;
}

	
	{xor.b32 %r505,%r499,0x80008000;
}

	
	{add.f16x2 %r507,%r481,%r493;
}

	
	{add.f16x2 %r510,%r484,%r496;
}

	
	{sub.f16x2 %r513,%r481,%r493;
}

	
	{sub.f16x2 %r516,%r484,%r496;
}

	
	{add.f16x2 %r519,%r487,%r502;
}

	
	{add.f16x2 %r522,%r490,%r505;
}

	
	{sub.f16x2 %r525,%r487,%r502;
}

	
	{sub.f16x2 %r528,%r490,%r505;
}

	and.b32 %r64, %r5, 48;
bfe.u32 %r645, %r5, 4, 2;
cvt.rn.f32.u32	%f38, %r645;
mul.f32 %f39, %f38, 0f3EC90FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r531, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r534, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r536, {high,high};}


	
	{mul.f16x2 %r538,%r522,%r536;
}

	
	{xor.b32 %r541,%r538,0x80008000;
}

	
	{fma.rn.f16x2 %r543,%r519,%r534,%r541;
}

	
	{mul.f16x2 %r547,%r519,%r536;
}

	
	{fma.rn.f16x2 %r550,%r522,%r534,%r547;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r554, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r556, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r558, {low,high};}


	
	{mul.f16x2 %r559,%r556,%r558;
}

	
	{mul.f16x2 %r562,%r531,%r554;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r565, {high,low};}


	
	{fma.rn.f16x2 %r567,%r559,%r565,%r562;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r567;
mov.b32 %r571, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r567;
mov.b32 %r573, {high,high};}


	
	{mul.f16x2 %r575,%r516,%r573;
}

	
	{xor.b32 %r578,%r575,0x80008000;
}

	
	{fma.rn.f16x2 %r580,%r513,%r571,%r578;
}

	
	{mul.f16x2 %r584,%r513,%r573;
}

	
	{fma.rn.f16x2 %r587,%r516,%r571,%r584;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r591, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r593, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r595, {low,high};}


	
	{mul.f16x2 %r596,%r593,%r595;
}

	
	{mul.f16x2 %r599,%r567,%r591;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r567;
mov.b32 %r602, {high,low};}


	
	{fma.rn.f16x2 %r604,%r596,%r602,%r599;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r604;
mov.b32 %r608, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r604;
mov.b32 %r610, {high,high};}


	
	{mul.f16x2 %r612,%r528,%r610;
}

	
	{xor.b32 %r615,%r612,0x80008000;
}

	
	{fma.rn.f16x2 %r617,%r525,%r608,%r615;
}

	
	{mul.f16x2 %r621,%r525,%r610;
}

	
	{fma.rn.f16x2 %r624,%r528,%r608,%r621;
}

	and.b32 %r646, %r5, 15;
add.s32 %r71, %r39, %r646;
barrier.sync 0;
shl.b32 %r647, %r64, 2;
add.s32 %r648, %r647, %r71;
shl.b32 %r649, %r648, 2;
add.s32 %r72, %r304, %r649;
st.shared.u32 [%r72], %r507;
st.shared.u32 [%r72+64], %r543;
st.shared.u32 [%r72+128], %r580;
st.shared.u32 [%r72+192], %r617;
barrier.sync 0;
add.s32 %r651, %r64, %r71;
shl.b32 %r652, %r651, 2;
add.s32 %r73, %r304, %r652;
ld.shared.u32 %r74, [%r73];
ld.shared.u32 %r75, [%r73+256];
ld.shared.u32 %r76, [%r73+512];
ld.shared.u32 %r77, [%r73+768];
barrier.sync 0;
st.shared.u32 [%r72], %r510;
st.shared.u32 [%r72+64], %r550;
st.shared.u32 [%r72+128], %r587;
st.shared.u32 [%r72+192], %r624;
barrier.sync 0;
ld.shared.u32 %r658, [%r73];
ld.shared.u32 %r670, [%r73+256];
ld.shared.u32 %r659, [%r73+512];
ld.shared.u32 %r671, [%r73+768];

	{add.f16x2 %r654,%r74,%r76;
}

	
	{add.f16x2 %r657,%r658,%r659;
}

	
	{sub.f16x2 %r660,%r74,%r76;
}

	
	{sub.f16x2 %r663,%r658,%r659;
}

	
	{add.f16x2 %r666,%r75,%r77;
}

	
	{add.f16x2 %r669,%r670,%r671;
}

	
	{sub.f16x2 %r672,%r75,%r77;
}

	
	{sub.f16x2 %r675,%r670,%r671;
}

	
	{xor.b32 %r678,%r672,0x80008000;
}

	
	{add.f16x2 %r680,%r654,%r666;
}

	
	{add.f16x2 %r683,%r657,%r669;
}

	
	{sub.f16x2 %r686,%r654,%r666;
}

	
	{sub.f16x2 %r689,%r657,%r669;
}

	
	{add.f16x2 %r692,%r660,%r675;
}

	
	{add.f16x2 %r695,%r663,%r678;
}

	
	{sub.f16x2 %r698,%r660,%r675;
}

	
	{sub.f16x2 %r701,%r663,%r678;
}

	selp.b32	%r727, %r698, %r701, %p1;
selp.b32	%r726, %r701, %r698, %p1;
selp.b32	%r721, %r686, %r689, %p1;
selp.b32	%r720, %r689, %r686, %p1;
selp.b32	%r715, %r692, %r695, %p1;
selp.b32	%r714, %r695, %r692, %p1;
selp.b32	%r709, %r680, %r683, %p1;
selp.b32	%r708, %r683, %r680, %p1;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r708;
mov.b32 {blow,bhigh}, %r709;
mov.b32 %r704, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r708;
mov.b32 {blow,bhigh}, %r709;
mov.b32 %r707, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r714;
mov.b32 {blow,bhigh}, %r715;
mov.b32 %r710, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r714;
mov.b32 {blow,bhigh}, %r715;
mov.b32 %r713, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r720;
mov.b32 {blow,bhigh}, %r721;
mov.b32 %r716, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r720;
mov.b32 {blow,bhigh}, %r721;
mov.b32 %r719, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r726;
mov.b32 {blow,bhigh}, %r727;
mov.b32 %r722, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r726;
mov.b32 {blow,bhigh}, %r727;
mov.b32 %r725, {ahigh,bhigh};}


	cvta.to.global.u64 %rd8, %rd5;
shl.b64 %rd9, %rd1, 2;
add.s64 %rd3, %rd8, %rd9;
@%p2 bra BB7_6;
bra.uni BB7_5;

BB7_6:
st.global.u32 [%rd3], %r704;
st.global.u32 [%rd3+256], %r710;
st.global.u32 [%rd3+512], %r716;
st.global.u32 [%rd3+768], %r722;
bra.uni BB7_7;

BB7_5:
shl.b32 %r728, %r3, 1;
add.s32 %r729, %r728, -1;
st.global.u32 [%rd3], %r704;
st.global.u32 [%rd3+256], %r710;
st.global.u32 [%rd3+512], %r716;
st.global.u32 [%rd3+768], %r722;
setp.ge.u32	%p5, %r729, %r1;
@%p5 bra BB7_8;

BB7_7:
st.global.u32 [%rd3+1024], %r707;
st.global.u32 [%rd3+1280], %r713;
st.global.u32 [%rd3+1536], %r719;
st.global.u32 [%rd3+1792], %r725;

BB7_8:
ret;
}


.weak .entry _Z10vector_fftILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 128, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<9>;
.reg .f32 %f<54>;
.reg .b32 %r<901>;
.reg .f64 %fd<2>;
.reg .b64 %rd<10>;


ld.param.u32 %r1, [_Z10vector_fftILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u32 %r102, [_Z10vector_fftILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd5, [_Z10vector_fftILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z10vector_fftILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
mov.u32 %r2, %ctaid.x;
shl.b32 %r123, %r2, 10;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r124, %r123, %r5;
cvt.u64.u32	%rd1, %r124;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r124, 4;
add.s64 %rd2, %rd6, %rd7;
@%p2 bra BB8_2;
bra.uni BB8_1;

BB8_2:
ld.global.u32 %r900, [%rd2];
ld.global.u32 %r898, [%rd2+512];
ld.global.u32 %r896, [%rd2+1024];
ld.global.u32 %r894, [%rd2+1536];
bra.uni BB8_3;

BB8_1:
shl.b32 %r126, %r3, 1;
add.s32 %r127, %r126, -1;
ld.global.u32 %r900, [%rd2];
ld.global.u32 %r898, [%rd2+512];
ld.global.u32 %r896, [%rd2+1024];
ld.global.u32 %r894, [%rd2+1536];
setp.ge.u32	%p3, %r127, %r1;
@%p3 bra BB8_4;

BB8_3:
ld.global.u32 %r899, [%rd2+2048];
ld.global.u32 %r897, [%rd2+2560];
ld.global.u32 %r895, [%rd2+3072];
ld.global.u32 %r893, [%rd2+3584];

BB8_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r900;
mov.b32 {blow,bhigh}, %r899;
mov.b32 %r128, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r900;
mov.b32 {blow,bhigh}, %r899;
mov.b32 %r131, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r898;
mov.b32 {blow,bhigh}, %r897;
mov.b32 %r134, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r898;
mov.b32 {blow,bhigh}, %r897;
mov.b32 %r137, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r896;
mov.b32 {blow,bhigh}, %r895;
mov.b32 %r140, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r896;
mov.b32 {blow,bhigh}, %r895;
mov.b32 %r143, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r894;
mov.b32 {blow,bhigh}, %r893;
mov.b32 %r146, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r894;
mov.b32 {blow,bhigh}, %r893;
mov.b32 %r149, {ahigh,bhigh};}


	setp.eq.s32	%p1, %r102, 1;
selp.b32	%r175, %r146, %r149, %p1;
selp.b32	%r172, %r149, %r146, %p1;
selp.b32	%r163, %r140, %r143, %p1;
selp.b32	%r160, %r143, %r140, %p1;
selp.b32	%r174, %r134, %r137, %p1;
selp.b32	%r171, %r137, %r134, %p1;
selp.b32	%r162, %r128, %r131, %p1;
selp.b32	%r159, %r131, %r128, %p1;

	{add.f16x2 %r152,%r159,%r160;
}

	
	{add.f16x2 %r155,%r162,%r163;
}

	
	{sub.f16x2 %r158,%r159,%r160;
}

	
	{sub.f16x2 %r161,%r162,%r163;
}

	
	{add.f16x2 %r164,%r171,%r172;
}

	
	{add.f16x2 %r167,%r174,%r175;
}

	
	{sub.f16x2 %r170,%r171,%r172;
}

	
	{sub.f16x2 %r173,%r174,%r175;
}

	
	{xor.b32 %r176,%r170,0x80008000;
}

	
	{add.f16x2 %r178,%r152,%r164;
}

	
	{add.f16x2 %r181,%r155,%r167;
}

	
	{sub.f16x2 %r184,%r152,%r164;
}

	
	{sub.f16x2 %r187,%r155,%r167;
}

	
	{add.f16x2 %r190,%r158,%r173;
}

	
	{add.f16x2 %r193,%r161,%r176;
}

	
	{sub.f16x2 %r196,%r158,%r173;
}

	
	{sub.f16x2 %r199,%r161,%r176;
}

	and.b32 %r32, %r5, 127;
cvt.rn.f32.u32	%f12, %r32;
mul.f32 %f13, %f12, 0f3C490FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r202, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r202;
mov.b32 %r205, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r202;
mov.b32 %r207, {high,high};}


	
	{mul.f16x2 %r209,%r193,%r207;
}

	
	{xor.b32 %r212,%r209,0x80008000;
}

	
	{fma.rn.f16x2 %r214,%r190,%r205,%r212;
}

	
	{mul.f16x2 %r218,%r190,%r207;
}

	
	{fma.rn.f16x2 %r221,%r193,%r205,%r218;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r202;
mov.b32 %r225, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r202;
mov.b32 %r227, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r229, {low,high};}


	
	{mul.f16x2 %r230,%r227,%r229;
}

	
	{mul.f16x2 %r233,%r202,%r225;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r202;
mov.b32 %r236, {high,low};}


	
	{fma.rn.f16x2 %r238,%r230,%r236,%r233;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r238;
mov.b32 %r242, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r238;
mov.b32 %r244, {high,high};}


	
	{mul.f16x2 %r246,%r187,%r244;
}

	
	{xor.b32 %r249,%r246,0x80008000;
}

	
	{fma.rn.f16x2 %r251,%r184,%r242,%r249;
}

	
	{mul.f16x2 %r255,%r184,%r244;
}

	
	{fma.rn.f16x2 %r258,%r187,%r242,%r255;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r202;
mov.b32 %r262, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r202;
mov.b32 %r264, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r266, {low,high};}


	
	{mul.f16x2 %r267,%r264,%r266;
}

	
	{mul.f16x2 %r270,%r238,%r262;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r238;
mov.b32 %r273, {high,low};}


	
	{fma.rn.f16x2 %r275,%r267,%r273,%r270;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r275;
mov.b32 %r279, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r275;
mov.b32 %r281, {high,high};}


	
	{mul.f16x2 %r283,%r199,%r281;
}

	
	{xor.b32 %r286,%r283,0x80008000;
}

	
	{fma.rn.f16x2 %r288,%r196,%r279,%r286;
}

	
	{mul.f16x2 %r292,%r196,%r281;
}

	
	{fma.rn.f16x2 %r295,%r199,%r279,%r292;
}

	shl.b32 %r316, %r5, 2;
and.b32 %r39, %r316, -512;
barrier.sync 0;
shl.b32 %r317, %r32, 2;
add.s32 %r318, %r317, %r39;
shl.b32 %r319, %r318, 2;
mov.u32 %r320, smem_full;
add.s32 %r40, %r320, %r319;
st.shared.u32 [%r40], %r178;
st.shared.u32 [%r40+4], %r214;
st.shared.u32 [%r40+8], %r251;
st.shared.u32 [%r40+12], %r288;
barrier.sync 0;
add.s32 %r321, %r32, %r39;
shl.b32 %r322, %r321, 2;
add.s32 %r41, %r320, %r322;
ld.shared.u32 %r42, [%r41];
ld.shared.u32 %r43, [%r41+512];
ld.shared.u32 %r44, [%r41+1024];
ld.shared.u32 %r45, [%r41+1536];
barrier.sync 0;
st.shared.u32 [%r40], %r181;
st.shared.u32 [%r40+4], %r221;
st.shared.u32 [%r40+8], %r258;
st.shared.u32 [%r40+12], %r295;
barrier.sync 0;
ld.shared.u32 %r328, [%r41];
ld.shared.u32 %r340, [%r41+512];
ld.shared.u32 %r329, [%r41+1024];
ld.shared.u32 %r341, [%r41+1536];

	{add.f16x2 %r324,%r42,%r44;
}

	
	{add.f16x2 %r327,%r328,%r329;
}

	
	{sub.f16x2 %r330,%r42,%r44;
}

	
	{sub.f16x2 %r333,%r328,%r329;
}

	
	{add.f16x2 %r336,%r43,%r45;
}

	
	{add.f16x2 %r339,%r340,%r341;
}

	
	{sub.f16x2 %r342,%r43,%r45;
}

	
	{sub.f16x2 %r345,%r340,%r341;
}

	
	{xor.b32 %r348,%r342,0x80008000;
}

	
	{add.f16x2 %r350,%r324,%r336;
}

	
	{add.f16x2 %r353,%r327,%r339;
}

	
	{sub.f16x2 %r356,%r324,%r336;
}

	
	{sub.f16x2 %r359,%r327,%r339;
}

	
	{add.f16x2 %r362,%r330,%r345;
}

	
	{add.f16x2 %r365,%r333,%r348;
}

	
	{sub.f16x2 %r368,%r330,%r345;
}

	
	{sub.f16x2 %r371,%r333,%r348;
}

	and.b32 %r48, %r5, 124;
bfe.u32 %r488, %r5, 2, 5;
cvt.rn.f32.u32	%f25, %r488;
mul.f32 %f26, %f25, 0f3D490FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r374, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r374;
mov.b32 %r377, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r374;
mov.b32 %r379, {high,high};}


	
	{mul.f16x2 %r381,%r365,%r379;
}

	
	{xor.b32 %r384,%r381,0x80008000;
}

	
	{fma.rn.f16x2 %r386,%r362,%r377,%r384;
}

	
	{mul.f16x2 %r390,%r362,%r379;
}

	
	{fma.rn.f16x2 %r393,%r365,%r377,%r390;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r374;
mov.b32 %r397, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r374;
mov.b32 %r399, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r401, {low,high};}


	
	{mul.f16x2 %r402,%r399,%r401;
}

	
	{mul.f16x2 %r405,%r374,%r397;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r374;
mov.b32 %r408, {high,low};}


	
	{fma.rn.f16x2 %r410,%r402,%r408,%r405;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r410;
mov.b32 %r414, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r410;
mov.b32 %r416, {high,high};}


	
	{mul.f16x2 %r418,%r359,%r416;
}

	
	{xor.b32 %r421,%r418,0x80008000;
}

	
	{fma.rn.f16x2 %r423,%r356,%r414,%r421;
}

	
	{mul.f16x2 %r427,%r356,%r416;
}

	
	{fma.rn.f16x2 %r430,%r359,%r414,%r427;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r374;
mov.b32 %r434, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r374;
mov.b32 %r436, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r438, {low,high};}


	
	{mul.f16x2 %r439,%r436,%r438;
}

	
	{mul.f16x2 %r442,%r410,%r434;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r410;
mov.b32 %r445, {high,low};}


	
	{fma.rn.f16x2 %r447,%r439,%r445,%r442;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r447;
mov.b32 %r451, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r447;
mov.b32 %r453, {high,high};}


	
	{mul.f16x2 %r455,%r371,%r453;
}

	
	{xor.b32 %r458,%r455,0x80008000;
}

	
	{fma.rn.f16x2 %r460,%r368,%r451,%r458;
}

	
	{mul.f16x2 %r464,%r368,%r453;
}

	
	{fma.rn.f16x2 %r467,%r371,%r451,%r464;
}

	and.b32 %r489, %r5, 3;
add.s32 %r55, %r39, %r489;
barrier.sync 0;
shl.b32 %r490, %r48, 2;
add.s32 %r491, %r490, %r55;
shl.b32 %r492, %r491, 2;
add.s32 %r56, %r320, %r492;
st.shared.u32 [%r56], %r350;
st.shared.u32 [%r56+16], %r386;
st.shared.u32 [%r56+32], %r423;
st.shared.u32 [%r56+48], %r460;
barrier.sync 0;
add.s32 %r494, %r48, %r55;
shl.b32 %r495, %r494, 2;
add.s32 %r57, %r320, %r495;
ld.shared.u32 %r58, [%r57];
ld.shared.u32 %r59, [%r57+512];
ld.shared.u32 %r60, [%r57+1024];
ld.shared.u32 %r61, [%r57+1536];
barrier.sync 0;
st.shared.u32 [%r56], %r353;
st.shared.u32 [%r56+16], %r393;
st.shared.u32 [%r56+32], %r430;
st.shared.u32 [%r56+48], %r467;
barrier.sync 0;
ld.shared.u32 %r501, [%r57];
ld.shared.u32 %r513, [%r57+512];
ld.shared.u32 %r502, [%r57+1024];
ld.shared.u32 %r514, [%r57+1536];

	{add.f16x2 %r497,%r58,%r60;
}

	
	{add.f16x2 %r500,%r501,%r502;
}

	
	{sub.f16x2 %r503,%r58,%r60;
}

	
	{sub.f16x2 %r506,%r501,%r502;
}

	
	{add.f16x2 %r509,%r59,%r61;
}

	
	{add.f16x2 %r512,%r513,%r514;
}

	
	{sub.f16x2 %r515,%r59,%r61;
}

	
	{sub.f16x2 %r518,%r513,%r514;
}

	
	{xor.b32 %r521,%r515,0x80008000;
}

	
	{add.f16x2 %r523,%r497,%r509;
}

	
	{add.f16x2 %r526,%r500,%r512;
}

	
	{sub.f16x2 %r529,%r497,%r509;
}

	
	{sub.f16x2 %r532,%r500,%r512;
}

	
	{add.f16x2 %r535,%r503,%r518;
}

	
	{add.f16x2 %r538,%r506,%r521;
}

	
	{sub.f16x2 %r541,%r503,%r518;
}

	
	{sub.f16x2 %r544,%r506,%r521;
}

	and.b32 %r64, %r5, 112;
bfe.u32 %r661, %r5, 4, 3;
cvt.rn.f32.u32	%f38, %r661;
mul.f32 %f39, %f38, 0f3E490FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r547, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r547;
mov.b32 %r550, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r547;
mov.b32 %r552, {high,high};}


	
	{mul.f16x2 %r554,%r538,%r552;
}

	
	{xor.b32 %r557,%r554,0x80008000;
}

	
	{fma.rn.f16x2 %r559,%r535,%r550,%r557;
}

	
	{mul.f16x2 %r563,%r535,%r552;
}

	
	{fma.rn.f16x2 %r566,%r538,%r550,%r563;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r547;
mov.b32 %r570, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r547;
mov.b32 %r572, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r574, {low,high};}


	
	{mul.f16x2 %r575,%r572,%r574;
}

	
	{mul.f16x2 %r578,%r547,%r570;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r547;
mov.b32 %r581, {high,low};}


	
	{fma.rn.f16x2 %r583,%r575,%r581,%r578;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r583;
mov.b32 %r587, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r583;
mov.b32 %r589, {high,high};}


	
	{mul.f16x2 %r591,%r532,%r589;
}

	
	{xor.b32 %r594,%r591,0x80008000;
}

	
	{fma.rn.f16x2 %r596,%r529,%r587,%r594;
}

	
	{mul.f16x2 %r600,%r529,%r589;
}

	
	{fma.rn.f16x2 %r603,%r532,%r587,%r600;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r547;
mov.b32 %r607, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r547;
mov.b32 %r609, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r611, {low,high};}


	
	{mul.f16x2 %r612,%r609,%r611;
}

	
	{mul.f16x2 %r615,%r583,%r607;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r583;
mov.b32 %r618, {high,low};}


	
	{fma.rn.f16x2 %r620,%r612,%r618,%r615;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r620;
mov.b32 %r624, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r620;
mov.b32 %r626, {high,high};}


	
	{mul.f16x2 %r628,%r544,%r626;
}

	
	{xor.b32 %r631,%r628,0x80008000;
}

	
	{fma.rn.f16x2 %r633,%r541,%r624,%r631;
}

	
	{mul.f16x2 %r637,%r541,%r626;
}

	
	{fma.rn.f16x2 %r640,%r544,%r624,%r637;
}

	and.b32 %r662, %r5, 15;
add.s32 %r71, %r39, %r662;
barrier.sync 0;
shl.b32 %r663, %r64, 2;
add.s32 %r664, %r663, %r71;
shl.b32 %r665, %r664, 2;
add.s32 %r72, %r320, %r665;
st.shared.u32 [%r72], %r523;
st.shared.u32 [%r72+64], %r559;
st.shared.u32 [%r72+128], %r596;
st.shared.u32 [%r72+192], %r633;
barrier.sync 0;
add.s32 %r667, %r64, %r71;
shl.b32 %r668, %r667, 2;
add.s32 %r73, %r320, %r668;
ld.shared.u32 %r74, [%r73];
ld.shared.u32 %r75, [%r73+512];
ld.shared.u32 %r76, [%r73+1024];
ld.shared.u32 %r77, [%r73+1536];
barrier.sync 0;
st.shared.u32 [%r72], %r526;
st.shared.u32 [%r72+64], %r566;
st.shared.u32 [%r72+128], %r603;
st.shared.u32 [%r72+192], %r640;
barrier.sync 0;
ld.shared.u32 %r674, [%r73];
ld.shared.u32 %r686, [%r73+512];
ld.shared.u32 %r675, [%r73+1024];
ld.shared.u32 %r687, [%r73+1536];

	{add.f16x2 %r670,%r74,%r76;
}

	
	{add.f16x2 %r673,%r674,%r675;
}

	
	{sub.f16x2 %r676,%r74,%r76;
}

	
	{sub.f16x2 %r679,%r674,%r675;
}

	
	{add.f16x2 %r682,%r75,%r77;
}

	
	{add.f16x2 %r685,%r686,%r687;
}

	
	{sub.f16x2 %r688,%r75,%r77;
}

	
	{sub.f16x2 %r691,%r686,%r687;
}

	
	{xor.b32 %r694,%r688,0x80008000;
}

	
	{add.f16x2 %r696,%r670,%r682;
}

	
	{add.f16x2 %r699,%r673,%r685;
}

	
	{sub.f16x2 %r702,%r670,%r682;
}

	
	{sub.f16x2 %r705,%r673,%r685;
}

	
	{add.f16x2 %r708,%r676,%r691;
}

	
	{add.f16x2 %r711,%r679,%r694;
}

	
	{sub.f16x2 %r714,%r676,%r691;
}

	
	{sub.f16x2 %r717,%r679,%r694;
}

	and.b32 %r80, %r5, 64;
bfe.u32 %r834, %r5, 6, 1;
cvt.rn.f32.u32	%f51, %r834;
mul.f32 %f52, %f51, 0f3F490FDB;
cos.approx.f32 %f41, %f52;
sin.approx.f32 %f53, %f52;
neg.f32 %f42, %f53;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f41;
cvt.rn.f16.f32 high, %f42;
mov.b32 %r720, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r720;
mov.b32 %r723, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r720;
mov.b32 %r725, {high,high};}


	
	{mul.f16x2 %r727,%r711,%r725;
}

	
	{xor.b32 %r730,%r727,0x80008000;
}

	
	{fma.rn.f16x2 %r732,%r708,%r723,%r730;
}

	
	{mul.f16x2 %r736,%r708,%r725;
}

	
	{fma.rn.f16x2 %r739,%r711,%r723,%r736;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r720;
mov.b32 %r743, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r720;
mov.b32 %r745, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r747, {low,high};}


	
	{mul.f16x2 %r748,%r745,%r747;
}

	
	{mul.f16x2 %r751,%r720,%r743;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r720;
mov.b32 %r754, {high,low};}


	
	{fma.rn.f16x2 %r756,%r748,%r754,%r751;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r756;
mov.b32 %r760, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r756;
mov.b32 %r762, {high,high};}


	
	{mul.f16x2 %r764,%r705,%r762;
}

	
	{xor.b32 %r767,%r764,0x80008000;
}

	
	{fma.rn.f16x2 %r769,%r702,%r760,%r767;
}

	
	{mul.f16x2 %r773,%r702,%r762;
}

	
	{fma.rn.f16x2 %r776,%r705,%r760,%r773;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r720;
mov.b32 %r780, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r720;
mov.b32 %r782, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r784, {low,high};}


	
	{mul.f16x2 %r785,%r782,%r784;
}

	
	{mul.f16x2 %r788,%r756,%r780;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r756;
mov.b32 %r791, {high,low};}


	
	{fma.rn.f16x2 %r793,%r785,%r791,%r788;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r793;
mov.b32 %r797, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r793;
mov.b32 %r799, {high,high};}


	
	{mul.f16x2 %r801,%r717,%r799;
}

	
	{xor.b32 %r804,%r801,0x80008000;
}

	
	{fma.rn.f16x2 %r806,%r714,%r797,%r804;
}

	
	{mul.f16x2 %r810,%r714,%r799;
}

	
	{fma.rn.f16x2 %r813,%r717,%r797,%r810;
}

	and.b32 %r835, %r5, 63;
add.s32 %r87, %r39, %r835;
barrier.sync 0;
shl.b32 %r836, %r80, 2;
add.s32 %r837, %r836, %r87;
shl.b32 %r838, %r837, 2;
add.s32 %r88, %r320, %r838;
st.shared.u32 [%r88], %r696;
st.shared.u32 [%r88+256], %r732;
st.shared.u32 [%r88+512], %r769;
st.shared.u32 [%r88+768], %r806;
barrier.sync 0;
add.s32 %r840, %r80, %r87;
shl.b32 %r841, %r840, 2;
add.s32 %r89, %r320, %r841;
ld.shared.u32 %r90, [%r89];
ld.shared.u32 %r91, [%r89+512];
ld.shared.u32 %r92, [%r89+1024];
ld.shared.u32 %r93, [%r89+1536];
barrier.sync 0;
st.shared.u32 [%r88], %r699;
st.shared.u32 [%r88+256], %r739;
st.shared.u32 [%r88+512], %r776;
st.shared.u32 [%r88+768], %r813;
barrier.sync 0;
ld.shared.u32 %r847, [%r89];
ld.shared.u32 %r859, [%r89+512];
ld.shared.u32 %r848, [%r89+1024];
ld.shared.u32 %r860, [%r89+1536];

	{add.f16x2 %r843,%r90,%r92;
}

	
	{add.f16x2 %r846,%r847,%r848;
}

	
	{sub.f16x2 %r849,%r90,%r92;
}

	
	{sub.f16x2 %r852,%r847,%r848;
}

	
	{add.f16x2 %r855,%r91,%r93;
}

	
	{add.f16x2 %r858,%r859,%r860;
}

	
	{sub.f16x2 %r861,%r91,%r93;
}

	
	{sub.f16x2 %r864,%r859,%r860;
}

	selp.b32	%r890, %r861, %r864, %p1;
selp.b32	%r889, %r864, %r861, %p1;
selp.b32	%r884, %r849, %r852, %p1;
selp.b32	%r883, %r852, %r849, %p1;
selp.b32	%r878, %r855, %r858, %p1;
selp.b32	%r877, %r858, %r855, %p1;
selp.b32	%r872, %r843, %r846, %p1;
selp.b32	%r871, %r846, %r843, %p1;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r871;
mov.b32 {blow,bhigh}, %r872;
mov.b32 %r867, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r871;
mov.b32 {blow,bhigh}, %r872;
mov.b32 %r870, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r877;
mov.b32 {blow,bhigh}, %r878;
mov.b32 %r873, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r877;
mov.b32 {blow,bhigh}, %r878;
mov.b32 %r876, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r883;
mov.b32 {blow,bhigh}, %r884;
mov.b32 %r879, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r883;
mov.b32 {blow,bhigh}, %r884;
mov.b32 %r882, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r889;
mov.b32 {blow,bhigh}, %r890;
mov.b32 %r885, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r889;
mov.b32 {blow,bhigh}, %r890;
mov.b32 %r888, {ahigh,bhigh};}


	cvta.to.global.u64 %rd8, %rd5;
shl.b64 %rd9, %rd1, 2;
add.s64 %rd3, %rd8, %rd9;
@%p2 bra BB8_6;
bra.uni BB8_5;

BB8_6:
st.global.u32 [%rd3], %r867;
st.global.u32 [%rd3+512], %r873;
st.global.u32 [%rd3+1024], %r879;
st.global.u32 [%rd3+1536], %r885;
bra.uni BB8_7;

BB8_5:
shl.b32 %r891, %r3, 1;
add.s32 %r892, %r891, -1;
st.global.u32 [%rd3], %r867;
st.global.u32 [%rd3+512], %r873;
st.global.u32 [%rd3+1024], %r879;
st.global.u32 [%rd3+1536], %r885;
setp.ge.u32	%p5, %r892, %r1;
@%p5 bra BB8_8;

BB8_7:
st.global.u32 [%rd3+2048], %r870;
st.global.u32 [%rd3+2560], %r876;
st.global.u32 [%rd3+3072], %r882;
st.global.u32 [%rd3+3584], %r888;

BB8_8:
ret;
}


.weak .entry _Z10vector_fftILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 256, 1, 1
{
.reg .pred %p<8>;
.reg .b16 %rs<9>;
.reg .f32 %f<58>;
.reg .b32 %r<934>;
.reg .f64 %fd<2>;
.reg .b64 %rd<10>;


ld.param.u32 %r1, [_Z10vector_fftILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u32 %r109, [_Z10vector_fftILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd5, [_Z10vector_fftILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z10vector_fftILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
mov.u32 %r2, %ctaid.x;
shl.b32 %r130, %r2, 11;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p1, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r131, %r130, %r5;
cvt.u64.u32	%rd1, %r131;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r131, 4;
add.s64 %rd2, %rd6, %rd7;
@%p1 bra BB9_2;
bra.uni BB9_1;

BB9_2:
ld.global.u32 %r933, [%rd2];
ld.global.u32 %r931, [%rd2+1024];
ld.global.u32 %r929, [%rd2+2048];
ld.global.u32 %r927, [%rd2+3072];
bra.uni BB9_3;

BB9_1:
shl.b32 %r133, %r3, 1;
add.s32 %r134, %r133, -1;
ld.global.u32 %r933, [%rd2];
ld.global.u32 %r931, [%rd2+1024];
ld.global.u32 %r929, [%rd2+2048];
ld.global.u32 %r927, [%rd2+3072];
setp.ge.u32	%p2, %r134, %r1;
@%p2 bra BB9_4;

BB9_3:
ld.global.u32 %r932, [%rd2+4096];
ld.global.u32 %r930, [%rd2+5120];
ld.global.u32 %r928, [%rd2+6144];
ld.global.u32 %r926, [%rd2+7168];

BB9_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r933;
mov.b32 {blow,bhigh}, %r932;
mov.b32 %r135, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r933;
mov.b32 {blow,bhigh}, %r932;
mov.b32 %r138, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r931;
mov.b32 {blow,bhigh}, %r930;
mov.b32 %r141, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r931;
mov.b32 {blow,bhigh}, %r930;
mov.b32 %r144, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r929;
mov.b32 {blow,bhigh}, %r928;
mov.b32 %r147, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r929;
mov.b32 {blow,bhigh}, %r928;
mov.b32 %r150, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r927;
mov.b32 {blow,bhigh}, %r926;
mov.b32 %r153, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r927;
mov.b32 {blow,bhigh}, %r926;
mov.b32 %r156, {ahigh,bhigh};}


	setp.eq.s32	%p3, %r109, 1;
selp.b32	%r182, %r153, %r156, %p3;
selp.b32	%r179, %r156, %r153, %p3;
selp.b32	%r170, %r147, %r150, %p3;
selp.b32	%r167, %r150, %r147, %p3;
selp.b32	%r181, %r141, %r144, %p3;
selp.b32	%r178, %r144, %r141, %p3;
selp.b32	%r169, %r135, %r138, %p3;
selp.b32	%r166, %r138, %r135, %p3;

	{add.f16x2 %r159,%r166,%r167;
}

	
	{add.f16x2 %r162,%r169,%r170;
}

	
	{sub.f16x2 %r165,%r166,%r167;
}

	
	{sub.f16x2 %r168,%r169,%r170;
}

	
	{add.f16x2 %r171,%r178,%r179;
}

	
	{add.f16x2 %r174,%r181,%r182;
}

	
	{sub.f16x2 %r177,%r178,%r179;
}

	
	{sub.f16x2 %r180,%r181,%r182;
}

	
	{xor.b32 %r183,%r177,0x80008000;
}

	
	{add.f16x2 %r185,%r159,%r171;
}

	
	{add.f16x2 %r188,%r162,%r174;
}

	
	{sub.f16x2 %r191,%r159,%r171;
}

	
	{sub.f16x2 %r194,%r162,%r174;
}

	
	{add.f16x2 %r197,%r165,%r180;
}

	
	{add.f16x2 %r200,%r168,%r183;
}

	
	{sub.f16x2 %r203,%r165,%r180;
}

	
	{sub.f16x2 %r206,%r168,%r183;
}

	and.b32 %r39, %r5, 255;
cvt.rn.f32.u32	%f6, %r39;
mul.f32 %f1, %f6, 0f3BC90FDB;
setp.eq.s32	%p4, %r39, 255;
mov.f32 %f57, 0f3BC90F88;
@%p4 bra BB9_6;

cos.approx.f32 %f57, %f1;

BB9_6:
shl.b32 %r323, %r5, 2;
and.b32 %r40, %r323, -1024;
sin.approx.f32 %f17, %f1;
neg.f32 %f8, %f17;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f57;
cvt.rn.f16.f32 high, %f8;
mov.b32 %r209, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r209;
mov.b32 %r212, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r209;
mov.b32 %r214, {high,high};}


	
	{mul.f16x2 %r216,%r200,%r214;
}

	
	{xor.b32 %r219,%r216,0x80008000;
}

	
	{fma.rn.f16x2 %r221,%r197,%r212,%r219;
}

	
	{mul.f16x2 %r225,%r197,%r214;
}

	
	{fma.rn.f16x2 %r228,%r200,%r212,%r225;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r209;
mov.b32 %r232, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r209;
mov.b32 %r234, {high,high};}


	mov.f32 %f13, 0fBF800000;
mov.f32 %f14, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r236, {low,high};}


	
	{mul.f16x2 %r237,%r234,%r236;
}

	
	{mul.f16x2 %r240,%r209,%r232;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r209;
mov.b32 %r243, {high,low};}


	
	{fma.rn.f16x2 %r245,%r237,%r243,%r240;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r245;
mov.b32 %r249, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r245;
mov.b32 %r251, {high,high};}


	
	{mul.f16x2 %r253,%r194,%r251;
}

	
	{xor.b32 %r256,%r253,0x80008000;
}

	
	{fma.rn.f16x2 %r258,%r191,%r249,%r256;
}

	
	{mul.f16x2 %r262,%r191,%r251;
}

	
	{fma.rn.f16x2 %r265,%r194,%r249,%r262;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r209;
mov.b32 %r269, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r209;
mov.b32 %r271, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r273, {low,high};}


	
	{mul.f16x2 %r274,%r271,%r273;
}

	
	{mul.f16x2 %r277,%r245,%r269;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r245;
mov.b32 %r280, {high,low};}


	
	{fma.rn.f16x2 %r282,%r274,%r280,%r277;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r282;
mov.b32 %r286, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r282;
mov.b32 %r288, {high,high};}


	
	{mul.f16x2 %r290,%r206,%r288;
}

	
	{xor.b32 %r293,%r290,0x80008000;
}

	
	{fma.rn.f16x2 %r295,%r203,%r286,%r293;
}

	
	{mul.f16x2 %r299,%r203,%r288;
}

	
	{fma.rn.f16x2 %r302,%r206,%r286,%r299;
}

	barrier.sync 0;
shl.b32 %r324, %r39, 2;
add.s32 %r325, %r324, %r40;
shl.b32 %r326, %r325, 2;
mov.u32 %r327, smem_full;
add.s32 %r47, %r327, %r326;
st.shared.u32 [%r47], %r185;
st.shared.u32 [%r47+4], %r221;
st.shared.u32 [%r47+8], %r258;
st.shared.u32 [%r47+12], %r295;
barrier.sync 0;
add.s32 %r328, %r39, %r40;
shl.b32 %r329, %r328, 2;
add.s32 %r48, %r327, %r329;
ld.shared.u32 %r49, [%r48];
ld.shared.u32 %r50, [%r48+1024];
ld.shared.u32 %r51, [%r48+2048];
ld.shared.u32 %r52, [%r48+3072];
barrier.sync 0;
st.shared.u32 [%r47], %r188;
st.shared.u32 [%r47+4], %r228;
st.shared.u32 [%r47+8], %r265;
st.shared.u32 [%r47+12], %r302;
barrier.sync 0;
ld.shared.u32 %r335, [%r48];
ld.shared.u32 %r347, [%r48+1024];
ld.shared.u32 %r336, [%r48+2048];
ld.shared.u32 %r348, [%r48+3072];

	{add.f16x2 %r331,%r49,%r51;
}

	
	{add.f16x2 %r334,%r335,%r336;
}

	
	{sub.f16x2 %r337,%r49,%r51;
}

	
	{sub.f16x2 %r340,%r335,%r336;
}

	
	{add.f16x2 %r343,%r50,%r52;
}

	
	{add.f16x2 %r346,%r347,%r348;
}

	
	{sub.f16x2 %r349,%r50,%r52;
}

	
	{sub.f16x2 %r352,%r347,%r348;
}

	
	{xor.b32 %r355,%r349,0x80008000;
}

	
	{add.f16x2 %r357,%r331,%r343;
}

	
	{add.f16x2 %r360,%r334,%r346;
}

	
	{sub.f16x2 %r363,%r331,%r343;
}

	
	{sub.f16x2 %r366,%r334,%r346;
}

	
	{add.f16x2 %r369,%r337,%r352;
}

	
	{add.f16x2 %r372,%r340,%r355;
}

	
	{sub.f16x2 %r375,%r337,%r352;
}

	
	{sub.f16x2 %r378,%r340,%r355;
}

	and.b32 %r55, %r5, 252;
bfe.u32 %r495, %r5, 2, 6;
cvt.rn.f32.u32	%f28, %r495;
mul.f32 %f29, %f28, 0f3CC90FDB;
cos.approx.f32 %f18, %f29;
sin.approx.f32 %f30, %f29;
neg.f32 %f19, %f30;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f18;
cvt.rn.f16.f32 high, %f19;
mov.b32 %r381, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r381;
mov.b32 %r384, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r381;
mov.b32 %r386, {high,high};}


	
	{mul.f16x2 %r388,%r372,%r386;
}

	
	{xor.b32 %r391,%r388,0x80008000;
}

	
	{fma.rn.f16x2 %r393,%r369,%r384,%r391;
}

	
	{mul.f16x2 %r397,%r369,%r386;
}

	
	{fma.rn.f16x2 %r400,%r372,%r384,%r397;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r381;
mov.b32 %r404, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r381;
mov.b32 %r406, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r408, {low,high};}


	
	{mul.f16x2 %r409,%r406,%r408;
}

	
	{mul.f16x2 %r412,%r381,%r404;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r381;
mov.b32 %r415, {high,low};}


	
	{fma.rn.f16x2 %r417,%r409,%r415,%r412;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r417;
mov.b32 %r421, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r417;
mov.b32 %r423, {high,high};}


	
	{mul.f16x2 %r425,%r366,%r423;
}

	
	{xor.b32 %r428,%r425,0x80008000;
}

	
	{fma.rn.f16x2 %r430,%r363,%r421,%r428;
}

	
	{mul.f16x2 %r434,%r363,%r423;
}

	
	{fma.rn.f16x2 %r437,%r366,%r421,%r434;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r381;
mov.b32 %r441, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r381;
mov.b32 %r443, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r445, {low,high};}


	
	{mul.f16x2 %r446,%r443,%r445;
}

	
	{mul.f16x2 %r449,%r417,%r441;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r417;
mov.b32 %r452, {high,low};}


	
	{fma.rn.f16x2 %r454,%r446,%r452,%r449;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r454;
mov.b32 %r458, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r454;
mov.b32 %r460, {high,high};}


	
	{mul.f16x2 %r462,%r378,%r460;
}

	
	{xor.b32 %r465,%r462,0x80008000;
}

	
	{fma.rn.f16x2 %r467,%r375,%r458,%r465;
}

	
	{mul.f16x2 %r471,%r375,%r460;
}

	
	{fma.rn.f16x2 %r474,%r378,%r458,%r471;
}

	and.b32 %r496, %r5, 3;
add.s32 %r62, %r40, %r496;
barrier.sync 0;
shl.b32 %r497, %r55, 2;
add.s32 %r498, %r497, %r62;
shl.b32 %r499, %r498, 2;
add.s32 %r63, %r327, %r499;
st.shared.u32 [%r63], %r357;
st.shared.u32 [%r63+16], %r393;
st.shared.u32 [%r63+32], %r430;
st.shared.u32 [%r63+48], %r467;
barrier.sync 0;
add.s32 %r501, %r55, %r62;
shl.b32 %r502, %r501, 2;
add.s32 %r64, %r327, %r502;
ld.shared.u32 %r65, [%r64];
ld.shared.u32 %r66, [%r64+1024];
ld.shared.u32 %r67, [%r64+2048];
ld.shared.u32 %r68, [%r64+3072];
barrier.sync 0;
st.shared.u32 [%r63], %r360;
st.shared.u32 [%r63+16], %r400;
st.shared.u32 [%r63+32], %r437;
st.shared.u32 [%r63+48], %r474;
barrier.sync 0;
ld.shared.u32 %r508, [%r64];
ld.shared.u32 %r520, [%r64+1024];
ld.shared.u32 %r509, [%r64+2048];
ld.shared.u32 %r521, [%r64+3072];

	{add.f16x2 %r504,%r65,%r67;
}

	
	{add.f16x2 %r507,%r508,%r509;
}

	
	{sub.f16x2 %r510,%r65,%r67;
}

	
	{sub.f16x2 %r513,%r508,%r509;
}

	
	{add.f16x2 %r516,%r66,%r68;
}

	
	{add.f16x2 %r519,%r520,%r521;
}

	
	{sub.f16x2 %r522,%r66,%r68;
}

	
	{sub.f16x2 %r525,%r520,%r521;
}

	
	{xor.b32 %r528,%r522,0x80008000;
}

	
	{add.f16x2 %r530,%r504,%r516;
}

	
	{add.f16x2 %r533,%r507,%r519;
}

	
	{sub.f16x2 %r536,%r504,%r516;
}

	
	{sub.f16x2 %r539,%r507,%r519;
}

	
	{add.f16x2 %r542,%r510,%r525;
}

	
	{add.f16x2 %r545,%r513,%r528;
}

	
	{sub.f16x2 %r548,%r510,%r525;
}

	
	{sub.f16x2 %r551,%r513,%r528;
}

	and.b32 %r71, %r5, 240;
bfe.u32 %r668, %r5, 4, 4;
cvt.rn.f32.u32	%f41, %r668;
mul.f32 %f42, %f41, 0f3DC90FDB;
cos.approx.f32 %f31, %f42;
sin.approx.f32 %f43, %f42;
neg.f32 %f32, %f43;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f31;
cvt.rn.f16.f32 high, %f32;
mov.b32 %r554, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r554;
mov.b32 %r557, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r554;
mov.b32 %r559, {high,high};}


	
	{mul.f16x2 %r561,%r545,%r559;
}

	
	{xor.b32 %r564,%r561,0x80008000;
}

	
	{fma.rn.f16x2 %r566,%r542,%r557,%r564;
}

	
	{mul.f16x2 %r570,%r542,%r559;
}

	
	{fma.rn.f16x2 %r573,%r545,%r557,%r570;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r554;
mov.b32 %r577, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r554;
mov.b32 %r579, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r581, {low,high};}


	
	{mul.f16x2 %r582,%r579,%r581;
}

	
	{mul.f16x2 %r585,%r554,%r577;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r554;
mov.b32 %r588, {high,low};}


	
	{fma.rn.f16x2 %r590,%r582,%r588,%r585;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r590;
mov.b32 %r594, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r590;
mov.b32 %r596, {high,high};}


	
	{mul.f16x2 %r598,%r539,%r596;
}

	
	{xor.b32 %r601,%r598,0x80008000;
}

	
	{fma.rn.f16x2 %r603,%r536,%r594,%r601;
}

	
	{mul.f16x2 %r607,%r536,%r596;
}

	
	{fma.rn.f16x2 %r610,%r539,%r594,%r607;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r554;
mov.b32 %r614, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r554;
mov.b32 %r616, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r618, {low,high};}


	
	{mul.f16x2 %r619,%r616,%r618;
}

	
	{mul.f16x2 %r622,%r590,%r614;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r590;
mov.b32 %r625, {high,low};}


	
	{fma.rn.f16x2 %r627,%r619,%r625,%r622;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r627;
mov.b32 %r631, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r627;
mov.b32 %r633, {high,high};}


	
	{mul.f16x2 %r635,%r551,%r633;
}

	
	{xor.b32 %r638,%r635,0x80008000;
}

	
	{fma.rn.f16x2 %r640,%r548,%r631,%r638;
}

	
	{mul.f16x2 %r644,%r548,%r633;
}

	
	{fma.rn.f16x2 %r647,%r551,%r631,%r644;
}

	and.b32 %r669, %r5, 15;
add.s32 %r78, %r40, %r669;
barrier.sync 0;
shl.b32 %r670, %r71, 2;
add.s32 %r671, %r670, %r78;
shl.b32 %r672, %r671, 2;
add.s32 %r79, %r327, %r672;
st.shared.u32 [%r79], %r530;
st.shared.u32 [%r79+64], %r566;
st.shared.u32 [%r79+128], %r603;
st.shared.u32 [%r79+192], %r640;
barrier.sync 0;
add.s32 %r674, %r71, %r78;
shl.b32 %r675, %r674, 2;
add.s32 %r80, %r327, %r675;
ld.shared.u32 %r81, [%r80];
ld.shared.u32 %r82, [%r80+1024];
ld.shared.u32 %r83, [%r80+2048];
ld.shared.u32 %r84, [%r80+3072];
barrier.sync 0;
st.shared.u32 [%r79], %r533;
st.shared.u32 [%r79+64], %r573;
st.shared.u32 [%r79+128], %r610;
st.shared.u32 [%r79+192], %r647;
barrier.sync 0;
ld.shared.u32 %r681, [%r80];
ld.shared.u32 %r693, [%r80+1024];
ld.shared.u32 %r682, [%r80+2048];
ld.shared.u32 %r694, [%r80+3072];

	{add.f16x2 %r677,%r81,%r83;
}

	
	{add.f16x2 %r680,%r681,%r682;
}

	
	{sub.f16x2 %r683,%r81,%r83;
}

	
	{sub.f16x2 %r686,%r681,%r682;
}

	
	{add.f16x2 %r689,%r82,%r84;
}

	
	{add.f16x2 %r692,%r693,%r694;
}

	
	{sub.f16x2 %r695,%r82,%r84;
}

	
	{sub.f16x2 %r698,%r693,%r694;
}

	
	{xor.b32 %r701,%r695,0x80008000;
}

	
	{add.f16x2 %r703,%r677,%r689;
}

	
	{add.f16x2 %r706,%r680,%r692;
}

	
	{sub.f16x2 %r709,%r677,%r689;
}

	
	{sub.f16x2 %r712,%r680,%r692;
}

	
	{add.f16x2 %r715,%r683,%r698;
}

	
	{add.f16x2 %r718,%r686,%r701;
}

	
	{sub.f16x2 %r721,%r683,%r698;
}

	
	{sub.f16x2 %r724,%r686,%r701;
}

	and.b32 %r87, %r5, 192;
bfe.u32 %r841, %r5, 6, 2;
cvt.rn.f32.u32	%f54, %r841;
mul.f32 %f55, %f54, 0f3EC90FDB;
cos.approx.f32 %f44, %f55;
sin.approx.f32 %f56, %f55;
neg.f32 %f45, %f56;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r727, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r727;
mov.b32 %r730, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r727;
mov.b32 %r732, {high,high};}


	
	{mul.f16x2 %r734,%r718,%r732;
}

	
	{xor.b32 %r737,%r734,0x80008000;
}

	
	{fma.rn.f16x2 %r739,%r715,%r730,%r737;
}

	
	{mul.f16x2 %r743,%r715,%r732;
}

	
	{fma.rn.f16x2 %r746,%r718,%r730,%r743;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r727;
mov.b32 %r750, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r727;
mov.b32 %r752, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r754, {low,high};}


	
	{mul.f16x2 %r755,%r752,%r754;
}

	
	{mul.f16x2 %r758,%r727,%r750;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r727;
mov.b32 %r761, {high,low};}


	
	{fma.rn.f16x2 %r763,%r755,%r761,%r758;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r763;
mov.b32 %r767, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r763;
mov.b32 %r769, {high,high};}


	
	{mul.f16x2 %r771,%r712,%r769;
}

	
	{xor.b32 %r774,%r771,0x80008000;
}

	
	{fma.rn.f16x2 %r776,%r709,%r767,%r774;
}

	
	{mul.f16x2 %r780,%r709,%r769;
}

	
	{fma.rn.f16x2 %r783,%r712,%r767,%r780;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r727;
mov.b32 %r787, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r727;
mov.b32 %r789, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r791, {low,high};}


	
	{mul.f16x2 %r792,%r789,%r791;
}

	
	{mul.f16x2 %r795,%r763,%r787;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r763;
mov.b32 %r798, {high,low};}


	
	{fma.rn.f16x2 %r800,%r792,%r798,%r795;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r800;
mov.b32 %r804, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r800;
mov.b32 %r806, {high,high};}


	
	{mul.f16x2 %r808,%r724,%r806;
}

	
	{xor.b32 %r811,%r808,0x80008000;
}

	
	{fma.rn.f16x2 %r813,%r721,%r804,%r811;
}

	
	{mul.f16x2 %r817,%r721,%r806;
}

	
	{fma.rn.f16x2 %r820,%r724,%r804,%r817;
}

	and.b32 %r842, %r5, 63;
add.s32 %r94, %r40, %r842;
barrier.sync 0;
shl.b32 %r843, %r87, 2;
add.s32 %r844, %r843, %r94;
shl.b32 %r845, %r844, 2;
add.s32 %r95, %r327, %r845;
st.shared.u32 [%r95], %r703;
st.shared.u32 [%r95+256], %r739;
st.shared.u32 [%r95+512], %r776;
st.shared.u32 [%r95+768], %r813;
barrier.sync 0;
add.s32 %r847, %r87, %r94;
shl.b32 %r848, %r847, 2;
add.s32 %r96, %r327, %r848;
ld.shared.u32 %r97, [%r96];
ld.shared.u32 %r98, [%r96+1024];
ld.shared.u32 %r99, [%r96+2048];
ld.shared.u32 %r100, [%r96+3072];
barrier.sync 0;
st.shared.u32 [%r95], %r706;
st.shared.u32 [%r95+256], %r746;
st.shared.u32 [%r95+512], %r783;
st.shared.u32 [%r95+768], %r820;
barrier.sync 0;
ld.shared.u32 %r854, [%r96];
ld.shared.u32 %r866, [%r96+1024];
ld.shared.u32 %r855, [%r96+2048];
ld.shared.u32 %r867, [%r96+3072];

	{add.f16x2 %r850,%r97,%r99;
}

	
	{add.f16x2 %r853,%r854,%r855;
}

	
	{sub.f16x2 %r856,%r97,%r99;
}

	
	{sub.f16x2 %r859,%r854,%r855;
}

	
	{add.f16x2 %r862,%r98,%r100;
}

	
	{add.f16x2 %r865,%r866,%r867;
}

	
	{sub.f16x2 %r868,%r98,%r100;
}

	
	{sub.f16x2 %r871,%r866,%r867;
}

	
	{xor.b32 %r874,%r868,0x80008000;
}

	
	{add.f16x2 %r876,%r850,%r862;
}

	
	{add.f16x2 %r879,%r853,%r865;
}

	
	{sub.f16x2 %r882,%r850,%r862;
}

	
	{sub.f16x2 %r885,%r853,%r865;
}

	
	{add.f16x2 %r888,%r856,%r871;
}

	
	{add.f16x2 %r891,%r859,%r874;
}

	
	{sub.f16x2 %r894,%r856,%r871;
}

	
	{sub.f16x2 %r897,%r859,%r874;
}

	selp.b32	%r923, %r894, %r897, %p3;
selp.b32	%r922, %r897, %r894, %p3;
selp.b32	%r917, %r882, %r885, %p3;
selp.b32	%r916, %r885, %r882, %p3;
selp.b32	%r911, %r888, %r891, %p3;
selp.b32	%r910, %r891, %r888, %p3;
selp.b32	%r905, %r876, %r879, %p3;
selp.b32	%r904, %r879, %r876, %p3;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r904;
mov.b32 {blow,bhigh}, %r905;
mov.b32 %r900, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r904;
mov.b32 {blow,bhigh}, %r905;
mov.b32 %r903, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r910;
mov.b32 {blow,bhigh}, %r911;
mov.b32 %r906, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r910;
mov.b32 {blow,bhigh}, %r911;
mov.b32 %r909, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r916;
mov.b32 {blow,bhigh}, %r917;
mov.b32 %r912, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r916;
mov.b32 {blow,bhigh}, %r917;
mov.b32 %r915, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r922;
mov.b32 {blow,bhigh}, %r923;
mov.b32 %r918, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r922;
mov.b32 {blow,bhigh}, %r923;
mov.b32 %r921, {ahigh,bhigh};}


	cvta.to.global.u64 %rd8, %rd5;
shl.b64 %rd9, %rd1, 2;
add.s64 %rd3, %rd8, %rd9;
@%p1 bra BB9_8;
bra.uni BB9_7;

BB9_8:
st.global.u32 [%rd3], %r900;
st.global.u32 [%rd3+1024], %r906;
st.global.u32 [%rd3+2048], %r912;
st.global.u32 [%rd3+3072], %r918;
bra.uni BB9_9;

BB9_7:
shl.b32 %r924, %r3, 1;
add.s32 %r925, %r924, -1;
st.global.u32 [%rd3], %r900;
st.global.u32 [%rd3+1024], %r906;
st.global.u32 [%rd3+2048], %r912;
st.global.u32 [%rd3+3072], %r918;
setp.ge.u32	%p7, %r925, %r1;
@%p7 bra BB9_10;

BB9_9:
st.global.u32 [%rd3+4096], %r903;
st.global.u32 [%rd3+5120], %r909;
st.global.u32 [%rd3+6144], %r915;
st.global.u32 [%rd3+7168], %r921;

BB9_10:
ret;
}


.weak .entry _Z10vector_fftILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 512, 1, 1
{
.reg .pred %p<9>;
.reg .b16 %rs<9>;
.reg .f32 %f<72>;
.reg .b32 %r<1071>;
.reg .f64 %fd<2>;
.reg .b64 %rd<10>;


ld.param.u32 %r1, [_Z10vector_fftILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u32 %r100, [_Z10vector_fftILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd5, [_Z10vector_fftILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z10vector_fftILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
mov.u32 %r2, %ctaid.x;
shl.b32 %r121, %r2, 12;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p1, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r122, %r121, %r5;
cvt.u64.u32	%rd1, %r122;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r122, 4;
add.s64 %rd2, %rd6, %rd7;
@%p1 bra BB10_2;
bra.uni BB10_1;

BB10_2:
ld.global.u32 %r1070, [%rd2];
ld.global.u32 %r1068, [%rd2+2048];
ld.global.u32 %r1066, [%rd2+4096];
ld.global.u32 %r1064, [%rd2+6144];
bra.uni BB10_3;

BB10_1:
shl.b32 %r124, %r3, 1;
add.s32 %r125, %r124, -1;
ld.global.u32 %r1070, [%rd2];
ld.global.u32 %r1068, [%rd2+2048];
ld.global.u32 %r1066, [%rd2+4096];
ld.global.u32 %r1064, [%rd2+6144];
setp.ge.u32	%p2, %r125, %r1;
@%p2 bra BB10_4;

BB10_3:
ld.global.u32 %r1069, [%rd2+8192];
ld.global.u32 %r1067, [%rd2+10240];
ld.global.u32 %r1065, [%rd2+12288];
ld.global.u32 %r1063, [%rd2+14336];

BB10_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1070;
mov.b32 {blow,bhigh}, %r1069;
mov.b32 %r126, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1070;
mov.b32 {blow,bhigh}, %r1069;
mov.b32 %r129, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1068;
mov.b32 {blow,bhigh}, %r1067;
mov.b32 %r132, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1068;
mov.b32 {blow,bhigh}, %r1067;
mov.b32 %r135, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1066;
mov.b32 {blow,bhigh}, %r1065;
mov.b32 %r138, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1066;
mov.b32 {blow,bhigh}, %r1065;
mov.b32 %r141, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1064;
mov.b32 {blow,bhigh}, %r1063;
mov.b32 %r144, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1064;
mov.b32 {blow,bhigh}, %r1063;
mov.b32 %r147, {ahigh,bhigh};}


	setp.eq.s32	%p3, %r100, 1;
selp.b32	%r173, %r144, %r147, %p3;
selp.b32	%r170, %r147, %r144, %p3;
selp.b32	%r161, %r138, %r141, %p3;
selp.b32	%r158, %r141, %r138, %p3;
selp.b32	%r172, %r132, %r135, %p3;
selp.b32	%r169, %r135, %r132, %p3;
selp.b32	%r160, %r126, %r129, %p3;
selp.b32	%r157, %r129, %r126, %p3;

	{add.f16x2 %r150,%r157,%r158;
}

	
	{add.f16x2 %r153,%r160,%r161;
}

	
	{sub.f16x2 %r156,%r157,%r158;
}

	
	{sub.f16x2 %r159,%r160,%r161;
}

	
	{add.f16x2 %r162,%r169,%r170;
}

	
	{add.f16x2 %r165,%r172,%r173;
}

	
	{sub.f16x2 %r168,%r169,%r170;
}

	
	{sub.f16x2 %r171,%r172,%r173;
}

	
	{xor.b32 %r174,%r168,0x80008000;
}

	
	{add.f16x2 %r176,%r150,%r162;
}

	
	{add.f16x2 %r179,%r153,%r165;
}

	
	{sub.f16x2 %r182,%r150,%r162;
}

	
	{sub.f16x2 %r185,%r153,%r165;
}

	
	{add.f16x2 %r188,%r156,%r171;
}

	
	{add.f16x2 %r191,%r159,%r174;
}

	
	{sub.f16x2 %r194,%r156,%r171;
}

	
	{sub.f16x2 %r197,%r159,%r174;
}

	and.b32 %r39, %r5, 511;
cvt.rn.f32.u32	%f6, %r39;
mul.f32 %f1, %f6, 0f3B490FDB;
setp.eq.s32	%p4, %r39, 510;
mov.f32 %f71, 0f3BC90F88;
@%p4 bra BB10_7;

setp.eq.s32	%p5, %r39, 511;
mov.f32 %f71, 0f3B490FC6;
@%p5 bra BB10_7;

cos.approx.f32 %f71, %f1;

BB10_7:
shl.b32 %r314, %r5, 2;
and.b32 %r40, %r314, -2048;
sin.approx.f32 %f18, %f1;
neg.f32 %f9, %f18;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f71;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r200, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r200;
mov.b32 %r203, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r200;
mov.b32 %r205, {high,high};}


	
	{mul.f16x2 %r207,%r191,%r205;
}

	
	{xor.b32 %r210,%r207,0x80008000;
}

	
	{fma.rn.f16x2 %r212,%r188,%r203,%r210;
}

	
	{mul.f16x2 %r216,%r188,%r205;
}

	
	{fma.rn.f16x2 %r219,%r191,%r203,%r216;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r200;
mov.b32 %r223, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r200;
mov.b32 %r225, {high,high};}


	mov.f32 %f14, 0fBF800000;
mov.f32 %f15, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r227, {low,high};}


	
	{mul.f16x2 %r228,%r225,%r227;
}

	
	{mul.f16x2 %r231,%r200,%r223;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r200;
mov.b32 %r234, {high,low};}


	
	{fma.rn.f16x2 %r236,%r228,%r234,%r231;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r236;
mov.b32 %r240, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r236;
mov.b32 %r242, {high,high};}


	
	{mul.f16x2 %r244,%r185,%r242;
}

	
	{xor.b32 %r247,%r244,0x80008000;
}

	
	{fma.rn.f16x2 %r249,%r182,%r240,%r247;
}

	
	{mul.f16x2 %r253,%r182,%r242;
}

	
	{fma.rn.f16x2 %r256,%r185,%r240,%r253;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r200;
mov.b32 %r260, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r200;
mov.b32 %r262, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r264, {low,high};}


	
	{mul.f16x2 %r265,%r262,%r264;
}

	
	{mul.f16x2 %r268,%r236,%r260;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r236;
mov.b32 %r271, {high,low};}


	
	{fma.rn.f16x2 %r273,%r265,%r271,%r268;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r273;
mov.b32 %r277, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r273;
mov.b32 %r279, {high,high};}


	
	{mul.f16x2 %r281,%r197,%r279;
}

	
	{xor.b32 %r284,%r281,0x80008000;
}

	
	{fma.rn.f16x2 %r286,%r194,%r277,%r284;
}

	
	{mul.f16x2 %r290,%r194,%r279;
}

	
	{fma.rn.f16x2 %r293,%r197,%r277,%r290;
}

	barrier.sync 0;
shl.b32 %r315, %r40, 3;
mov.u32 %r316, smem_full;
add.s32 %r47, %r316, %r315;
shl.b32 %r317, %r39, 5;
add.s32 %r318, %r47, %r317;
st.shared.u32 [%r318], %r176;
st.shared.u32 [%r318+4], %r179;
st.shared.u32 [%r318+8], %r212;
st.shared.u32 [%r318+12], %r219;
st.shared.u32 [%r318+16], %r249;
st.shared.u32 [%r318+20], %r256;
st.shared.u32 [%r318+24], %r286;
st.shared.u32 [%r318+28], %r293;
barrier.sync 0;
shl.b32 %r483, %r39, 3;
add.s32 %r484, %r47, %r483;
ld.shared.u32 %r320, [%r484];
ld.shared.u32 %r323, [%r484+4];
ld.shared.u32 %r332, [%r484+4096];
ld.shared.u32 %r335, [%r484+4100];
ld.shared.u32 %r321, [%r484+8192];
ld.shared.u32 %r324, [%r484+8196];
ld.shared.u32 %r333, [%r484+12288];
ld.shared.u32 %r336, [%r484+12292];

	{add.f16x2 %r319,%r320,%r321;
}

	
	{add.f16x2 %r322,%r323,%r324;
}

	
	{sub.f16x2 %r325,%r320,%r321;
}

	
	{sub.f16x2 %r328,%r323,%r324;
}

	
	{add.f16x2 %r331,%r332,%r333;
}

	
	{add.f16x2 %r334,%r335,%r336;
}

	
	{sub.f16x2 %r337,%r332,%r333;
}

	
	{sub.f16x2 %r340,%r335,%r336;
}

	
	{xor.b32 %r343,%r337,0x80008000;
}

	
	{add.f16x2 %r345,%r319,%r331;
}

	
	{add.f16x2 %r348,%r322,%r334;
}

	
	{sub.f16x2 %r351,%r319,%r331;
}

	
	{sub.f16x2 %r354,%r322,%r334;
}

	
	{add.f16x2 %r357,%r325,%r340;
}

	
	{add.f16x2 %r360,%r328,%r343;
}

	
	{sub.f16x2 %r363,%r325,%r340;
}

	
	{sub.f16x2 %r366,%r328,%r343;
}

	and.b32 %r50, %r5, 508;
bfe.u32 %r485, %r5, 2, 7;
and.b32 %r486, %r5, 3;
add.s32 %r487, %r40, %r486;
cvt.rn.f32.u32	%f29, %r485;
mul.f32 %f30, %f29, 0f3C490FDB;
cos.approx.f32 %f19, %f30;
sin.approx.f32 %f31, %f30;
neg.f32 %f20, %f31;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f19;
cvt.rn.f16.f32 high, %f20;
mov.b32 %r369, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r369;
mov.b32 %r372, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r369;
mov.b32 %r374, {high,high};}


	
	{mul.f16x2 %r376,%r360,%r374;
}

	
	{xor.b32 %r379,%r376,0x80008000;
}

	
	{fma.rn.f16x2 %r381,%r357,%r372,%r379;
}

	
	{mul.f16x2 %r385,%r357,%r374;
}

	
	{fma.rn.f16x2 %r388,%r360,%r372,%r385;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r369;
mov.b32 %r392, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r369;
mov.b32 %r394, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r396, {low,high};}


	
	{mul.f16x2 %r397,%r394,%r396;
}

	
	{mul.f16x2 %r400,%r369,%r392;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r369;
mov.b32 %r403, {high,low};}


	
	{fma.rn.f16x2 %r405,%r397,%r403,%r400;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r405;
mov.b32 %r409, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r405;
mov.b32 %r411, {high,high};}


	
	{mul.f16x2 %r413,%r354,%r411;
}

	
	{xor.b32 %r416,%r413,0x80008000;
}

	
	{fma.rn.f16x2 %r418,%r351,%r409,%r416;
}

	
	{mul.f16x2 %r422,%r351,%r411;
}

	
	{fma.rn.f16x2 %r425,%r354,%r409,%r422;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r369;
mov.b32 %r429, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r369;
mov.b32 %r431, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r433, {low,high};}


	
	{mul.f16x2 %r434,%r431,%r433;
}

	
	{mul.f16x2 %r437,%r405,%r429;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r405;
mov.b32 %r440, {high,low};}


	
	{fma.rn.f16x2 %r442,%r434,%r440,%r437;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r442;
mov.b32 %r446, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r442;
mov.b32 %r448, {high,high};}


	
	{mul.f16x2 %r450,%r366,%r448;
}

	
	{xor.b32 %r453,%r450,0x80008000;
}

	
	{fma.rn.f16x2 %r455,%r363,%r446,%r453;
}

	
	{mul.f16x2 %r459,%r363,%r448;
}

	
	{fma.rn.f16x2 %r462,%r366,%r446,%r459;
}

	shl.b32 %r488, %r487, 3;
add.s32 %r57, %r316, %r488;
barrier.sync 0;
shl.b32 %r490, %r50, 5;
add.s32 %r491, %r57, %r490;
st.shared.u32 [%r491], %r345;
st.shared.u32 [%r491+4], %r348;
st.shared.u32 [%r491+32], %r381;
st.shared.u32 [%r491+36], %r388;
st.shared.u32 [%r491+64], %r418;
st.shared.u32 [%r491+68], %r425;
st.shared.u32 [%r491+96], %r455;
st.shared.u32 [%r491+100], %r462;
barrier.sync 0;
shl.b32 %r656, %r50, 3;
add.s32 %r657, %r57, %r656;
ld.shared.u32 %r493, [%r657];
ld.shared.u32 %r496, [%r657+4];
ld.shared.u32 %r505, [%r657+4096];
ld.shared.u32 %r508, [%r657+4100];
ld.shared.u32 %r494, [%r657+8192];
ld.shared.u32 %r497, [%r657+8196];
ld.shared.u32 %r506, [%r657+12288];
ld.shared.u32 %r509, [%r657+12292];

	{add.f16x2 %r492,%r493,%r494;
}

	
	{add.f16x2 %r495,%r496,%r497;
}

	
	{sub.f16x2 %r498,%r493,%r494;
}

	
	{sub.f16x2 %r501,%r496,%r497;
}

	
	{add.f16x2 %r504,%r505,%r506;
}

	
	{add.f16x2 %r507,%r508,%r509;
}

	
	{sub.f16x2 %r510,%r505,%r506;
}

	
	{sub.f16x2 %r513,%r508,%r509;
}

	
	{xor.b32 %r516,%r510,0x80008000;
}

	
	{add.f16x2 %r518,%r492,%r504;
}

	
	{add.f16x2 %r521,%r495,%r507;
}

	
	{sub.f16x2 %r524,%r492,%r504;
}

	
	{sub.f16x2 %r527,%r495,%r507;
}

	
	{add.f16x2 %r530,%r498,%r513;
}

	
	{add.f16x2 %r533,%r501,%r516;
}

	
	{sub.f16x2 %r536,%r498,%r513;
}

	
	{sub.f16x2 %r539,%r501,%r516;
}

	and.b32 %r61, %r5, 496;
bfe.u32 %r658, %r5, 4, 5;
and.b32 %r659, %r5, 15;
add.s32 %r660, %r40, %r659;
cvt.rn.f32.u32	%f42, %r658;
mul.f32 %f43, %f42, 0f3D490FDB;
cos.approx.f32 %f32, %f43;
sin.approx.f32 %f44, %f43;
neg.f32 %f33, %f44;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f32;
cvt.rn.f16.f32 high, %f33;
mov.b32 %r542, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r542;
mov.b32 %r545, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r542;
mov.b32 %r547, {high,high};}


	
	{mul.f16x2 %r549,%r533,%r547;
}

	
	{xor.b32 %r552,%r549,0x80008000;
}

	
	{fma.rn.f16x2 %r554,%r530,%r545,%r552;
}

	
	{mul.f16x2 %r558,%r530,%r547;
}

	
	{fma.rn.f16x2 %r561,%r533,%r545,%r558;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r542;
mov.b32 %r565, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r542;
mov.b32 %r567, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r569, {low,high};}


	
	{mul.f16x2 %r570,%r567,%r569;
}

	
	{mul.f16x2 %r573,%r542,%r565;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r542;
mov.b32 %r576, {high,low};}


	
	{fma.rn.f16x2 %r578,%r570,%r576,%r573;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r578;
mov.b32 %r582, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r578;
mov.b32 %r584, {high,high};}


	
	{mul.f16x2 %r586,%r527,%r584;
}

	
	{xor.b32 %r589,%r586,0x80008000;
}

	
	{fma.rn.f16x2 %r591,%r524,%r582,%r589;
}

	
	{mul.f16x2 %r595,%r524,%r584;
}

	
	{fma.rn.f16x2 %r598,%r527,%r582,%r595;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r542;
mov.b32 %r602, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r542;
mov.b32 %r604, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r606, {low,high};}


	
	{mul.f16x2 %r607,%r604,%r606;
}

	
	{mul.f16x2 %r610,%r578,%r602;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r578;
mov.b32 %r613, {high,low};}


	
	{fma.rn.f16x2 %r615,%r607,%r613,%r610;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r615;
mov.b32 %r619, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r615;
mov.b32 %r621, {high,high};}


	
	{mul.f16x2 %r623,%r539,%r621;
}

	
	{xor.b32 %r626,%r623,0x80008000;
}

	
	{fma.rn.f16x2 %r628,%r536,%r619,%r626;
}

	
	{mul.f16x2 %r632,%r536,%r621;
}

	
	{fma.rn.f16x2 %r635,%r539,%r619,%r632;
}

	shl.b32 %r661, %r660, 3;
add.s32 %r68, %r316, %r661;
barrier.sync 0;
shl.b32 %r663, %r61, 5;
add.s32 %r664, %r68, %r663;
st.shared.u32 [%r664], %r518;
st.shared.u32 [%r664+4], %r521;
st.shared.u32 [%r664+128], %r554;
st.shared.u32 [%r664+132], %r561;
st.shared.u32 [%r664+256], %r591;
st.shared.u32 [%r664+260], %r598;
st.shared.u32 [%r664+384], %r628;
st.shared.u32 [%r664+388], %r635;
barrier.sync 0;
shl.b32 %r829, %r61, 3;
add.s32 %r830, %r68, %r829;
ld.shared.u32 %r666, [%r830];
ld.shared.u32 %r669, [%r830+4];
ld.shared.u32 %r678, [%r830+4096];
ld.shared.u32 %r681, [%r830+4100];
ld.shared.u32 %r667, [%r830+8192];
ld.shared.u32 %r670, [%r830+8196];
ld.shared.u32 %r679, [%r830+12288];
ld.shared.u32 %r682, [%r830+12292];

	{add.f16x2 %r665,%r666,%r667;
}

	
	{add.f16x2 %r668,%r669,%r670;
}

	
	{sub.f16x2 %r671,%r666,%r667;
}

	
	{sub.f16x2 %r674,%r669,%r670;
}

	
	{add.f16x2 %r677,%r678,%r679;
}

	
	{add.f16x2 %r680,%r681,%r682;
}

	
	{sub.f16x2 %r683,%r678,%r679;
}

	
	{sub.f16x2 %r686,%r681,%r682;
}

	
	{xor.b32 %r689,%r683,0x80008000;
}

	
	{add.f16x2 %r691,%r665,%r677;
}

	
	{add.f16x2 %r694,%r668,%r680;
}

	
	{sub.f16x2 %r697,%r665,%r677;
}

	
	{sub.f16x2 %r700,%r668,%r680;
}

	
	{add.f16x2 %r703,%r671,%r686;
}

	
	{add.f16x2 %r706,%r674,%r689;
}

	
	{sub.f16x2 %r709,%r671,%r686;
}

	
	{sub.f16x2 %r712,%r674,%r689;
}

	and.b32 %r72, %r5, 448;
bfe.u32 %r831, %r5, 6, 3;
and.b32 %r832, %r5, 63;
add.s32 %r833, %r40, %r832;
cvt.rn.f32.u32	%f55, %r831;
mul.f32 %f56, %f55, 0f3E490FDB;
cos.approx.f32 %f45, %f56;
sin.approx.f32 %f57, %f56;
neg.f32 %f46, %f57;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f45;
cvt.rn.f16.f32 high, %f46;
mov.b32 %r715, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r715;
mov.b32 %r718, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r715;
mov.b32 %r720, {high,high};}


	
	{mul.f16x2 %r722,%r706,%r720;
}

	
	{xor.b32 %r725,%r722,0x80008000;
}

	
	{fma.rn.f16x2 %r727,%r703,%r718,%r725;
}

	
	{mul.f16x2 %r731,%r703,%r720;
}

	
	{fma.rn.f16x2 %r734,%r706,%r718,%r731;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r715;
mov.b32 %r738, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r715;
mov.b32 %r740, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r742, {low,high};}


	
	{mul.f16x2 %r743,%r740,%r742;
}

	
	{mul.f16x2 %r746,%r715,%r738;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r715;
mov.b32 %r749, {high,low};}


	
	{fma.rn.f16x2 %r751,%r743,%r749,%r746;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r751;
mov.b32 %r755, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r751;
mov.b32 %r757, {high,high};}


	
	{mul.f16x2 %r759,%r700,%r757;
}

	
	{xor.b32 %r762,%r759,0x80008000;
}

	
	{fma.rn.f16x2 %r764,%r697,%r755,%r762;
}

	
	{mul.f16x2 %r768,%r697,%r757;
}

	
	{fma.rn.f16x2 %r771,%r700,%r755,%r768;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r715;
mov.b32 %r775, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r715;
mov.b32 %r777, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r779, {low,high};}


	
	{mul.f16x2 %r780,%r777,%r779;
}

	
	{mul.f16x2 %r783,%r751,%r775;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r751;
mov.b32 %r786, {high,low};}


	
	{fma.rn.f16x2 %r788,%r780,%r786,%r783;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r788;
mov.b32 %r792, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r788;
mov.b32 %r794, {high,high};}


	
	{mul.f16x2 %r796,%r712,%r794;
}

	
	{xor.b32 %r799,%r796,0x80008000;
}

	
	{fma.rn.f16x2 %r801,%r709,%r792,%r799;
}

	
	{mul.f16x2 %r805,%r709,%r794;
}

	
	{fma.rn.f16x2 %r808,%r712,%r792,%r805;
}

	shl.b32 %r834, %r833, 3;
add.s32 %r79, %r316, %r834;
barrier.sync 0;
shl.b32 %r836, %r72, 5;
add.s32 %r837, %r79, %r836;
st.shared.u32 [%r837], %r691;
st.shared.u32 [%r837+4], %r694;
st.shared.u32 [%r837+512], %r727;
st.shared.u32 [%r837+516], %r734;
st.shared.u32 [%r837+1024], %r764;
st.shared.u32 [%r837+1028], %r771;
st.shared.u32 [%r837+1536], %r801;
st.shared.u32 [%r837+1540], %r808;
barrier.sync 0;
shl.b32 %r1002, %r72, 3;
add.s32 %r1003, %r79, %r1002;
ld.shared.u32 %r839, [%r1003];
ld.shared.u32 %r842, [%r1003+4];
ld.shared.u32 %r851, [%r1003+4096];
ld.shared.u32 %r854, [%r1003+4100];
ld.shared.u32 %r840, [%r1003+8192];
ld.shared.u32 %r843, [%r1003+8196];
ld.shared.u32 %r852, [%r1003+12288];
ld.shared.u32 %r855, [%r1003+12292];

	{add.f16x2 %r838,%r839,%r840;
}

	
	{add.f16x2 %r841,%r842,%r843;
}

	
	{sub.f16x2 %r844,%r839,%r840;
}

	
	{sub.f16x2 %r847,%r842,%r843;
}

	
	{add.f16x2 %r850,%r851,%r852;
}

	
	{add.f16x2 %r853,%r854,%r855;
}

	
	{sub.f16x2 %r856,%r851,%r852;
}

	
	{sub.f16x2 %r859,%r854,%r855;
}

	
	{xor.b32 %r862,%r856,0x80008000;
}

	
	{add.f16x2 %r864,%r838,%r850;
}

	
	{add.f16x2 %r867,%r841,%r853;
}

	
	{sub.f16x2 %r870,%r838,%r850;
}

	
	{sub.f16x2 %r873,%r841,%r853;
}

	
	{add.f16x2 %r876,%r844,%r859;
}

	
	{add.f16x2 %r879,%r847,%r862;
}

	
	{sub.f16x2 %r882,%r844,%r859;
}

	
	{sub.f16x2 %r885,%r847,%r862;
}

	and.b32 %r83, %r5, 256;
bfe.u32 %r1004, %r5, 8, 1;
and.b32 %r1005, %r5, 255;
add.s32 %r1006, %r40, %r1005;
cvt.rn.f32.u32	%f68, %r1004;
mul.f32 %f69, %f68, 0f3F490FDB;
cos.approx.f32 %f58, %f69;
sin.approx.f32 %f70, %f69;
neg.f32 %f59, %f70;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f58;
cvt.rn.f16.f32 high, %f59;
mov.b32 %r888, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r888;
mov.b32 %r891, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r888;
mov.b32 %r893, {high,high};}


	
	{mul.f16x2 %r895,%r879,%r893;
}

	
	{xor.b32 %r898,%r895,0x80008000;
}

	
	{fma.rn.f16x2 %r900,%r876,%r891,%r898;
}

	
	{mul.f16x2 %r904,%r876,%r893;
}

	
	{fma.rn.f16x2 %r907,%r879,%r891,%r904;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r888;
mov.b32 %r911, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r888;
mov.b32 %r913, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r915, {low,high};}


	
	{mul.f16x2 %r916,%r913,%r915;
}

	
	{mul.f16x2 %r919,%r888,%r911;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r888;
mov.b32 %r922, {high,low};}


	
	{fma.rn.f16x2 %r924,%r916,%r922,%r919;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r924;
mov.b32 %r928, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r924;
mov.b32 %r930, {high,high};}


	
	{mul.f16x2 %r932,%r873,%r930;
}

	
	{xor.b32 %r935,%r932,0x80008000;
}

	
	{fma.rn.f16x2 %r937,%r870,%r928,%r935;
}

	
	{mul.f16x2 %r941,%r870,%r930;
}

	
	{fma.rn.f16x2 %r944,%r873,%r928,%r941;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r888;
mov.b32 %r948, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r888;
mov.b32 %r950, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r952, {low,high};}


	
	{mul.f16x2 %r953,%r950,%r952;
}

	
	{mul.f16x2 %r956,%r924,%r948;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r924;
mov.b32 %r959, {high,low};}


	
	{fma.rn.f16x2 %r961,%r953,%r959,%r956;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r961;
mov.b32 %r965, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r961;
mov.b32 %r967, {high,high};}


	
	{mul.f16x2 %r969,%r885,%r967;
}

	
	{xor.b32 %r972,%r969,0x80008000;
}

	
	{fma.rn.f16x2 %r974,%r882,%r965,%r972;
}

	
	{mul.f16x2 %r978,%r882,%r967;
}

	
	{fma.rn.f16x2 %r981,%r885,%r965,%r978;
}

	shl.b32 %r1007, %r1006, 3;
add.s32 %r90, %r316, %r1007;
barrier.sync 0;
shl.b32 %r1009, %r83, 5;
add.s32 %r1010, %r90, %r1009;
st.shared.u32 [%r1010], %r864;
st.shared.u32 [%r1010+4], %r867;
st.shared.u32 [%r1010+2048], %r900;
st.shared.u32 [%r1010+2052], %r907;
st.shared.u32 [%r1010+4096], %r937;
st.shared.u32 [%r1010+4100], %r944;
st.shared.u32 [%r1010+6144], %r974;
st.shared.u32 [%r1010+6148], %r981;
barrier.sync 0;
shl.b32 %r1059, %r83, 3;
add.s32 %r1060, %r90, %r1059;
ld.shared.u32 %r1012, [%r1060];
ld.shared.u32 %r1015, [%r1060+4];
ld.shared.u32 %r1024, [%r1060+4096];
ld.shared.u32 %r1027, [%r1060+4100];
ld.shared.u32 %r1013, [%r1060+8192];
ld.shared.u32 %r1016, [%r1060+8196];
ld.shared.u32 %r1025, [%r1060+12288];
ld.shared.u32 %r1028, [%r1060+12292];

	{add.f16x2 %r1011,%r1012,%r1013;
}

	
	{add.f16x2 %r1014,%r1015,%r1016;
}

	
	{sub.f16x2 %r1017,%r1012,%r1013;
}

	
	{sub.f16x2 %r1020,%r1015,%r1016;
}

	
	{add.f16x2 %r1023,%r1024,%r1025;
}

	
	{add.f16x2 %r1026,%r1027,%r1028;
}

	
	{sub.f16x2 %r1029,%r1024,%r1025;
}

	
	{sub.f16x2 %r1032,%r1027,%r1028;
}

	selp.b32	%r1058, %r1029, %r1032, %p3;
selp.b32	%r1057, %r1032, %r1029, %p3;
selp.b32	%r1052, %r1017, %r1020, %p3;
selp.b32	%r1051, %r1020, %r1017, %p3;
selp.b32	%r1046, %r1023, %r1026, %p3;
selp.b32	%r1045, %r1026, %r1023, %p3;
selp.b32	%r1040, %r1011, %r1014, %p3;
selp.b32	%r1039, %r1014, %r1011, %p3;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1039;
mov.b32 {blow,bhigh}, %r1040;
mov.b32 %r1035, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1039;
mov.b32 {blow,bhigh}, %r1040;
mov.b32 %r1038, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1045;
mov.b32 {blow,bhigh}, %r1046;
mov.b32 %r1041, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1045;
mov.b32 {blow,bhigh}, %r1046;
mov.b32 %r1044, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1051;
mov.b32 {blow,bhigh}, %r1052;
mov.b32 %r1047, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1051;
mov.b32 {blow,bhigh}, %r1052;
mov.b32 %r1050, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1057;
mov.b32 {blow,bhigh}, %r1058;
mov.b32 %r1053, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1057;
mov.b32 {blow,bhigh}, %r1058;
mov.b32 %r1056, {ahigh,bhigh};}


	cvta.to.global.u64 %rd8, %rd5;
shl.b64 %rd9, %rd1, 2;
add.s64 %rd3, %rd8, %rd9;
@%p1 bra BB10_9;
bra.uni BB10_8;

BB10_9:
st.global.u32 [%rd3], %r1035;
st.global.u32 [%rd3+2048], %r1041;
st.global.u32 [%rd3+4096], %r1047;
st.global.u32 [%rd3+6144], %r1053;
bra.uni BB10_10;

BB10_8:
shl.b32 %r1061, %r3, 1;
add.s32 %r1062, %r1061, -1;
st.global.u32 [%rd3], %r1035;
st.global.u32 [%rd3+2048], %r1041;
st.global.u32 [%rd3+4096], %r1047;
st.global.u32 [%rd3+6144], %r1053;
setp.ge.u32	%p8, %r1062, %r1;
@%p8 bra BB10_11;

BB10_10:
st.global.u32 [%rd3+8192], %r1038;
st.global.u32 [%rd3+10240], %r1044;
st.global.u32 [%rd3+12288], %r1050;
st.global.u32 [%rd3+14336], %r1056;

BB10_11:
ret;
}


.weak .entry _Z10vector_fftILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 512, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<9>;
.reg .f32 %f<177>;
.reg .b32 %r<1863>;
.reg .f64 %fd<2>;
.reg .b64 %rd<10>;


ld.param.u32 %r1, [_Z10vector_fftILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u32 %r128, [_Z10vector_fftILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd5, [_Z10vector_fftILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z10vector_fftILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
mov.u32 %r2, %ctaid.x;
shl.b32 %r149, %r2, 13;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r150, %r149, %r5;
cvt.u64.u32	%rd1, %r150;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r150, 4;
add.s64 %rd2, %rd6, %rd7;
@%p2 bra BB11_2;
bra.uni BB11_1;

BB11_2:
ld.global.u32 %r1862, [%rd2];
ld.global.u32 %r1860, [%rd2+2048];
ld.global.u32 %r1858, [%rd2+4096];
ld.global.u32 %r1856, [%rd2+6144];
ld.global.u32 %r1854, [%rd2+8192];
ld.global.u32 %r1852, [%rd2+10240];
ld.global.u32 %r1850, [%rd2+12288];
ld.global.u32 %r1848, [%rd2+14336];
bra.uni BB11_3;

BB11_1:
shl.b32 %r152, %r3, 1;
add.s32 %r153, %r152, -1;
ld.global.u32 %r1862, [%rd2];
ld.global.u32 %r1860, [%rd2+2048];
ld.global.u32 %r1858, [%rd2+4096];
ld.global.u32 %r1856, [%rd2+6144];
ld.global.u32 %r1854, [%rd2+8192];
ld.global.u32 %r1852, [%rd2+10240];
ld.global.u32 %r1850, [%rd2+12288];
ld.global.u32 %r1848, [%rd2+14336];
setp.ge.u32	%p3, %r153, %r1;
@%p3 bra BB11_4;

BB11_3:
ld.global.u32 %r1861, [%rd2+16384];
ld.global.u32 %r1859, [%rd2+18432];
ld.global.u32 %r1857, [%rd2+20480];
ld.global.u32 %r1855, [%rd2+22528];
ld.global.u32 %r1853, [%rd2+24576];
ld.global.u32 %r1851, [%rd2+26624];
ld.global.u32 %r1849, [%rd2+28672];
ld.global.u32 %r1847, [%rd2+30720];

BB11_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1862;
mov.b32 {blow,bhigh}, %r1861;
mov.b32 %r154, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1862;
mov.b32 {blow,bhigh}, %r1861;
mov.b32 %r157, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1860;
mov.b32 {blow,bhigh}, %r1859;
mov.b32 %r160, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1860;
mov.b32 {blow,bhigh}, %r1859;
mov.b32 %r163, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1858;
mov.b32 {blow,bhigh}, %r1857;
mov.b32 %r166, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1858;
mov.b32 {blow,bhigh}, %r1857;
mov.b32 %r169, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1856;
mov.b32 {blow,bhigh}, %r1855;
mov.b32 %r172, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1856;
mov.b32 {blow,bhigh}, %r1855;
mov.b32 %r175, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1854;
mov.b32 {blow,bhigh}, %r1853;
mov.b32 %r178, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1854;
mov.b32 {blow,bhigh}, %r1853;
mov.b32 %r181, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1852;
mov.b32 {blow,bhigh}, %r1851;
mov.b32 %r184, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1852;
mov.b32 {blow,bhigh}, %r1851;
mov.b32 %r187, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1850;
mov.b32 {blow,bhigh}, %r1849;
mov.b32 %r190, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1850;
mov.b32 {blow,bhigh}, %r1849;
mov.b32 %r193, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1848;
mov.b32 {blow,bhigh}, %r1847;
mov.b32 %r196, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1848;
mov.b32 {blow,bhigh}, %r1847;
mov.b32 %r199, {ahigh,bhigh};}


	setp.eq.s32	%p1, %r128, 1;
selp.b32	%r275, %r196, %r199, %p1;
selp.b32	%r272, %r199, %r196, %p1;
selp.b32	%r225, %r190, %r193, %p1;
selp.b32	%r222, %r193, %r190, %p1;
selp.b32	%r263, %r184, %r187, %p1;
selp.b32	%r260, %r187, %r184, %p1;
selp.b32	%r213, %r178, %r181, %p1;
selp.b32	%r210, %r181, %r178, %p1;
selp.b32	%r274, %r172, %r175, %p1;
selp.b32	%r271, %r175, %r172, %p1;
selp.b32	%r224, %r166, %r169, %p1;
selp.b32	%r221, %r169, %r166, %p1;
selp.b32	%r262, %r160, %r163, %p1;
selp.b32	%r259, %r163, %r160, %p1;
selp.b32	%r212, %r154, %r157, %p1;
selp.b32	%r209, %r157, %r154, %p1;

	{add.f16x2 %r202,%r209,%r210;
}

	
	{add.f16x2 %r205,%r212,%r213;
}

	
	{sub.f16x2 %r208,%r209,%r210;
}

	
	{sub.f16x2 %r211,%r212,%r213;
}

	
	{add.f16x2 %r214,%r221,%r222;
}

	
	{add.f16x2 %r217,%r224,%r225;
}

	
	{sub.f16x2 %r220,%r221,%r222;
}

	
	{sub.f16x2 %r223,%r224,%r225;
}

	
	{xor.b32 %r226,%r220,0x80008000;
}

	
	{add.f16x2 %r228,%r202,%r214;
}

	
	{add.f16x2 %r231,%r205,%r217;
}

	
	{sub.f16x2 %r234,%r202,%r214;
}

	
	{sub.f16x2 %r237,%r205,%r217;
}

	
	{add.f16x2 %r240,%r208,%r223;
}

	
	{add.f16x2 %r243,%r211,%r226;
}

	
	{sub.f16x2 %r246,%r208,%r223;
}

	
	{sub.f16x2 %r249,%r211,%r226;
}

	
	{add.f16x2 %r252,%r259,%r260;
}

	
	{add.f16x2 %r255,%r262,%r263;
}

	
	{sub.f16x2 %r258,%r259,%r260;
}

	
	{sub.f16x2 %r261,%r262,%r263;
}

	
	{add.f16x2 %r264,%r271,%r272;
}

	
	{add.f16x2 %r267,%r274,%r275;
}

	
	{sub.f16x2 %r270,%r271,%r272;
}

	
	{sub.f16x2 %r273,%r274,%r275;
}

	
	{xor.b32 %r276,%r270,0x80008000;
}

	
	{add.f16x2 %r278,%r252,%r264;
}

	
	{add.f16x2 %r281,%r255,%r267;
}

	
	{sub.f16x2 %r284,%r252,%r264;
}

	
	{sub.f16x2 %r287,%r255,%r267;
}

	
	{add.f16x2 %r290,%r258,%r273;
}

	
	{add.f16x2 %r293,%r261,%r276;
}

	
	{sub.f16x2 %r296,%r258,%r273;
}

	
	{sub.f16x2 %r299,%r261,%r276;
}

	mov.f32 %f3, 0f3F3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r302, {low,high};}


	mov.f32 %f13, 0fBF3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r303, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r306, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r307, {low,high};}


	
	{mul.f16x2 %r316,%r290,%r302;
}

	
	{mul.f16x2 %r319,%r293,%r303;
}

	
	{sub.f16x2 %r322,%r316,%r319;
}

	
	{mul.f16x2 %r325,%r290,%r303;
}

	
	{fma.rn.f16x2 %r328,%r293,%r302,%r325;
}

	
	{xor.b32 %r332,%r284,0x80008000;
}

	
	{mul.f16x2 %r334,%r296,%r306;
}

	
	{mul.f16x2 %r337,%r299,%r307;
}

	
	{sub.f16x2 %r340,%r334,%r337;
}

	
	{mul.f16x2 %r343,%r296,%r307;
}

	
	{fma.rn.f16x2 %r346,%r299,%r306,%r343;
}

	
	{add.f16x2 %r350,%r228,%r278;
}

	
	{add.f16x2 %r353,%r231,%r281;
}

	
	{sub.f16x2 %r356,%r228,%r278;
}

	
	{sub.f16x2 %r359,%r231,%r281;
}

	
	{add.f16x2 %r362,%r240,%r322;
}

	
	{add.f16x2 %r365,%r243,%r328;
}

	
	{sub.f16x2 %r368,%r240,%r322;
}

	
	{sub.f16x2 %r371,%r243,%r328;
}

	
	{add.f16x2 %r374,%r234,%r287;
}

	
	{add.f16x2 %r377,%r237,%r332;
}

	
	{sub.f16x2 %r380,%r234,%r287;
}

	
	{sub.f16x2 %r383,%r237,%r332;
}

	
	{add.f16x2 %r386,%r246,%r340;
}

	
	{add.f16x2 %r389,%r249,%r346;
}

	
	{sub.f16x2 %r392,%r246,%r340;
}

	
	{sub.f16x2 %r395,%r249,%r346;
}

	shr.u32 %r660, %r5, 9;
shl.b32 %r56, %r660, 12;
and.b32 %r57, %r5, 511;
cvt.rn.f32.u32	%f48, %r57;
mul.f32 %f49, %f48, 0f3AC90FDB;
cos.approx.f32 %f30, %f49;
sin.approx.f32 %f50, %f49;
neg.f32 %f31, %f50;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f30;
cvt.rn.f16.f32 high, %f31;
mov.b32 %r398, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r401, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r403, {high,high};}


	
	{mul.f16x2 %r405,%r365,%r403;
}

	
	{xor.b32 %r408,%r405,0x80008000;
}

	
	{fma.rn.f16x2 %r410,%r362,%r401,%r408;
}

	
	{mul.f16x2 %r414,%r362,%r403;
}

	
	{fma.rn.f16x2 %r417,%r365,%r401,%r414;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r421, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r423, {high,high};}


	mov.f32 %f44, 0fBF800000;
mov.f32 %f45, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r425, {low,high};}


	
	{mul.f16x2 %r426,%r423,%r425;
}

	
	{mul.f16x2 %r429,%r398,%r421;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r432, {high,low};}


	
	{fma.rn.f16x2 %r434,%r426,%r432,%r429;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r434;
mov.b32 %r438, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r434;
mov.b32 %r440, {high,high};}


	
	{mul.f16x2 %r442,%r377,%r440;
}

	
	{xor.b32 %r445,%r442,0x80008000;
}

	
	{fma.rn.f16x2 %r447,%r374,%r438,%r445;
}

	
	{mul.f16x2 %r451,%r374,%r440;
}

	
	{fma.rn.f16x2 %r454,%r377,%r438,%r451;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r458, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r460, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r462, {low,high};}


	
	{mul.f16x2 %r463,%r460,%r462;
}

	
	{mul.f16x2 %r466,%r434,%r458;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r434;
mov.b32 %r469, {high,low};}


	
	{fma.rn.f16x2 %r471,%r463,%r469,%r466;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r471;
mov.b32 %r475, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r471;
mov.b32 %r477, {high,high};}


	
	{mul.f16x2 %r479,%r389,%r477;
}

	
	{xor.b32 %r482,%r479,0x80008000;
}

	
	{fma.rn.f16x2 %r484,%r386,%r475,%r482;
}

	
	{mul.f16x2 %r488,%r386,%r477;
}

	
	{fma.rn.f16x2 %r491,%r389,%r475,%r488;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r495, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r497, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r499, {low,high};}


	
	{mul.f16x2 %r500,%r497,%r499;
}

	
	{mul.f16x2 %r503,%r471,%r495;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r471;
mov.b32 %r506, {high,low};}


	
	{fma.rn.f16x2 %r508,%r500,%r506,%r503;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r508;
mov.b32 %r512, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r508;
mov.b32 %r514, {high,high};}


	
	{mul.f16x2 %r516,%r359,%r514;
}

	
	{xor.b32 %r519,%r516,0x80008000;
}

	
	{fma.rn.f16x2 %r521,%r356,%r512,%r519;
}

	
	{mul.f16x2 %r525,%r356,%r514;
}

	
	{fma.rn.f16x2 %r528,%r359,%r512,%r525;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r532, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r534, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r536, {low,high};}


	
	{mul.f16x2 %r537,%r534,%r536;
}

	
	{mul.f16x2 %r540,%r508,%r532;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r508;
mov.b32 %r543, {high,low};}


	
	{fma.rn.f16x2 %r545,%r537,%r543,%r540;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r545;
mov.b32 %r549, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r545;
mov.b32 %r551, {high,high};}


	
	{mul.f16x2 %r553,%r371,%r551;
}

	
	{xor.b32 %r556,%r553,0x80008000;
}

	
	{fma.rn.f16x2 %r558,%r368,%r549,%r556;
}

	
	{mul.f16x2 %r562,%r368,%r551;
}

	
	{fma.rn.f16x2 %r565,%r371,%r549,%r562;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r569, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r571, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r573, {low,high};}


	
	{mul.f16x2 %r574,%r571,%r573;
}

	
	{mul.f16x2 %r577,%r545,%r569;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r545;
mov.b32 %r580, {high,low};}


	
	{fma.rn.f16x2 %r582,%r574,%r580,%r577;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r582;
mov.b32 %r586, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r582;
mov.b32 %r588, {high,high};}


	
	{mul.f16x2 %r590,%r383,%r588;
}

	
	{xor.b32 %r593,%r590,0x80008000;
}

	
	{fma.rn.f16x2 %r595,%r380,%r586,%r593;
}

	
	{mul.f16x2 %r599,%r380,%r588;
}

	
	{fma.rn.f16x2 %r602,%r383,%r586,%r599;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r606, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r608, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r610, {low,high};}


	
	{mul.f16x2 %r611,%r608,%r610;
}

	
	{mul.f16x2 %r614,%r582,%r606;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r582;
mov.b32 %r617, {high,low};}


	
	{fma.rn.f16x2 %r619,%r611,%r617,%r614;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r619;
mov.b32 %r623, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r619;
mov.b32 %r625, {high,high};}


	
	{mul.f16x2 %r627,%r395,%r625;
}

	
	{xor.b32 %r630,%r627,0x80008000;
}

	
	{fma.rn.f16x2 %r632,%r392,%r623,%r630;
}

	
	{mul.f16x2 %r636,%r392,%r625;
}

	
	{fma.rn.f16x2 %r639,%r395,%r623,%r636;
}

	shl.b32 %r661, %r660, 15;
mov.u32 %r662, smem_full;
add.s32 %r72, %r662, %r661;
barrier.sync 0;
shl.b32 %r663, %r57, 6;
add.s32 %r664, %r72, %r663;
st.shared.u32 [%r664], %r350;
st.shared.u32 [%r664+4], %r353;
st.shared.u32 [%r664+8], %r410;
st.shared.u32 [%r664+12], %r417;
st.shared.u32 [%r664+16], %r447;
st.shared.u32 [%r664+20], %r454;
st.shared.u32 [%r664+24], %r484;
st.shared.u32 [%r664+28], %r491;
st.shared.u32 [%r664+32], %r521;
st.shared.u32 [%r664+36], %r528;
st.shared.u32 [%r664+40], %r558;
st.shared.u32 [%r664+44], %r565;
st.shared.u32 [%r664+48], %r595;
st.shared.u32 [%r664+52], %r602;
st.shared.u32 [%r664+56], %r632;
st.shared.u32 [%r664+60], %r639;
barrier.sync 0;
shl.b32 %r1123, %r57, 3;
add.s32 %r1124, %r72, %r1123;
ld.shared.u32 %r666, [%r1124];
ld.shared.u32 %r669, [%r1124+4];
ld.shared.u32 %r716, [%r1124+4096];
ld.shared.u32 %r719, [%r1124+4100];
ld.shared.u32 %r678, [%r1124+8192];
ld.shared.u32 %r681, [%r1124+8196];
ld.shared.u32 %r728, [%r1124+12288];
ld.shared.u32 %r731, [%r1124+12292];
ld.shared.u32 %r667, [%r1124+16384];
ld.shared.u32 %r670, [%r1124+16388];
ld.shared.u32 %r717, [%r1124+20480];
ld.shared.u32 %r720, [%r1124+20484];
ld.shared.u32 %r679, [%r1124+24576];
ld.shared.u32 %r682, [%r1124+24580];
ld.shared.u32 %r729, [%r1124+28672];
ld.shared.u32 %r732, [%r1124+28676];

	{add.f16x2 %r665,%r666,%r667;
}

	
	{add.f16x2 %r668,%r669,%r670;
}

	
	{sub.f16x2 %r671,%r666,%r667;
}

	
	{sub.f16x2 %r674,%r669,%r670;
}

	
	{add.f16x2 %r677,%r678,%r679;
}

	
	{add.f16x2 %r680,%r681,%r682;
}

	
	{sub.f16x2 %r683,%r678,%r679;
}

	
	{sub.f16x2 %r686,%r681,%r682;
}

	
	{xor.b32 %r689,%r683,0x80008000;
}

	
	{add.f16x2 %r691,%r665,%r677;
}

	
	{add.f16x2 %r694,%r668,%r680;
}

	
	{sub.f16x2 %r697,%r665,%r677;
}

	
	{sub.f16x2 %r700,%r668,%r680;
}

	
	{add.f16x2 %r703,%r671,%r686;
}

	
	{add.f16x2 %r706,%r674,%r689;
}

	
	{sub.f16x2 %r709,%r671,%r686;
}

	
	{sub.f16x2 %r712,%r674,%r689;
}

	
	{add.f16x2 %r715,%r716,%r717;
}

	
	{add.f16x2 %r718,%r719,%r720;
}

	
	{sub.f16x2 %r721,%r716,%r717;
}

	
	{sub.f16x2 %r724,%r719,%r720;
}

	
	{add.f16x2 %r727,%r728,%r729;
}

	
	{add.f16x2 %r730,%r731,%r732;
}

	
	{sub.f16x2 %r733,%r728,%r729;
}

	
	{sub.f16x2 %r736,%r731,%r732;
}

	
	{xor.b32 %r739,%r733,0x80008000;
}

	
	{add.f16x2 %r741,%r715,%r727;
}

	
	{add.f16x2 %r744,%r718,%r730;
}

	
	{sub.f16x2 %r747,%r715,%r727;
}

	
	{sub.f16x2 %r750,%r718,%r730;
}

	
	{add.f16x2 %r753,%r721,%r736;
}

	
	{add.f16x2 %r756,%r724,%r739;
}

	
	{sub.f16x2 %r759,%r721,%r736;
}

	
	{sub.f16x2 %r762,%r724,%r739;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r765, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r766, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r769, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r770, {low,high};}


	
	{mul.f16x2 %r779,%r753,%r765;
}

	
	{mul.f16x2 %r782,%r756,%r766;
}

	
	{sub.f16x2 %r785,%r779,%r782;
}

	
	{mul.f16x2 %r788,%r753,%r766;
}

	
	{fma.rn.f16x2 %r791,%r756,%r765,%r788;
}

	
	{xor.b32 %r795,%r747,0x80008000;
}

	
	{mul.f16x2 %r797,%r759,%r769;
}

	
	{mul.f16x2 %r800,%r762,%r770;
}

	
	{sub.f16x2 %r803,%r797,%r800;
}

	
	{mul.f16x2 %r806,%r759,%r770;
}

	
	{fma.rn.f16x2 %r809,%r762,%r769,%r806;
}

	
	{add.f16x2 %r813,%r691,%r741;
}

	
	{add.f16x2 %r816,%r694,%r744;
}

	
	{sub.f16x2 %r819,%r691,%r741;
}

	
	{sub.f16x2 %r822,%r694,%r744;
}

	
	{add.f16x2 %r825,%r703,%r785;
}

	
	{add.f16x2 %r828,%r706,%r791;
}

	
	{sub.f16x2 %r831,%r703,%r785;
}

	
	{sub.f16x2 %r834,%r706,%r791;
}

	
	{add.f16x2 %r837,%r697,%r750;
}

	
	{add.f16x2 %r840,%r700,%r795;
}

	
	{sub.f16x2 %r843,%r697,%r750;
}

	
	{sub.f16x2 %r846,%r700,%r795;
}

	
	{add.f16x2 %r849,%r709,%r803;
}

	
	{add.f16x2 %r852,%r712,%r809;
}

	
	{sub.f16x2 %r855,%r709,%r803;
}

	
	{sub.f16x2 %r858,%r712,%r809;
}

	and.b32 %r76, %r5, 504;
bfe.u32 %r1125, %r5, 3, 6;
and.b32 %r1126, %r5, 7;
add.s32 %r1127, %r56, %r1126;
cvt.rn.f32.u32	%f97, %r1125;
mul.f32 %f98, %f97, 0f3C490FDB;
cos.approx.f32 %f79, %f98;
sin.approx.f32 %f99, %f98;
neg.f32 %f80, %f99;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f79;
cvt.rn.f16.f32 high, %f80;
mov.b32 %r861, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r864, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r866, {high,high};}


	
	{mul.f16x2 %r868,%r828,%r866;
}

	
	{xor.b32 %r871,%r868,0x80008000;
}

	
	{fma.rn.f16x2 %r873,%r825,%r864,%r871;
}

	
	{mul.f16x2 %r877,%r825,%r866;
}

	
	{fma.rn.f16x2 %r880,%r828,%r864,%r877;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r884, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r886, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r888, {low,high};}


	
	{mul.f16x2 %r889,%r886,%r888;
}

	
	{mul.f16x2 %r892,%r861,%r884;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r895, {high,low};}


	
	{fma.rn.f16x2 %r897,%r889,%r895,%r892;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r897;
mov.b32 %r901, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r897;
mov.b32 %r903, {high,high};}


	
	{mul.f16x2 %r905,%r840,%r903;
}

	
	{xor.b32 %r908,%r905,0x80008000;
}

	
	{fma.rn.f16x2 %r910,%r837,%r901,%r908;
}

	
	{mul.f16x2 %r914,%r837,%r903;
}

	
	{fma.rn.f16x2 %r917,%r840,%r901,%r914;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r921, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r923, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r925, {low,high};}


	
	{mul.f16x2 %r926,%r923,%r925;
}

	
	{mul.f16x2 %r929,%r897,%r921;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r897;
mov.b32 %r932, {high,low};}


	
	{fma.rn.f16x2 %r934,%r926,%r932,%r929;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r934;
mov.b32 %r938, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r934;
mov.b32 %r940, {high,high};}


	
	{mul.f16x2 %r942,%r852,%r940;
}

	
	{xor.b32 %r945,%r942,0x80008000;
}

	
	{fma.rn.f16x2 %r947,%r849,%r938,%r945;
}

	
	{mul.f16x2 %r951,%r849,%r940;
}

	
	{fma.rn.f16x2 %r954,%r852,%r938,%r951;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r958, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r960, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r962, {low,high};}


	
	{mul.f16x2 %r963,%r960,%r962;
}

	
	{mul.f16x2 %r966,%r934,%r958;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r934;
mov.b32 %r969, {high,low};}


	
	{fma.rn.f16x2 %r971,%r963,%r969,%r966;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r971;
mov.b32 %r975, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r971;
mov.b32 %r977, {high,high};}


	
	{mul.f16x2 %r979,%r822,%r977;
}

	
	{xor.b32 %r982,%r979,0x80008000;
}

	
	{fma.rn.f16x2 %r984,%r819,%r975,%r982;
}

	
	{mul.f16x2 %r988,%r819,%r977;
}

	
	{fma.rn.f16x2 %r991,%r822,%r975,%r988;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r995, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r997, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r999, {low,high};}


	
	{mul.f16x2 %r1000,%r997,%r999;
}

	
	{mul.f16x2 %r1003,%r971,%r995;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r971;
mov.b32 %r1006, {high,low};}


	
	{fma.rn.f16x2 %r1008,%r1000,%r1006,%r1003;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1008;
mov.b32 %r1012, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1008;
mov.b32 %r1014, {high,high};}


	
	{mul.f16x2 %r1016,%r834,%r1014;
}

	
	{xor.b32 %r1019,%r1016,0x80008000;
}

	
	{fma.rn.f16x2 %r1021,%r831,%r1012,%r1019;
}

	
	{mul.f16x2 %r1025,%r831,%r1014;
}

	
	{fma.rn.f16x2 %r1028,%r834,%r1012,%r1025;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r1032, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r1034, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1036, {low,high};}


	
	{mul.f16x2 %r1037,%r1034,%r1036;
}

	
	{mul.f16x2 %r1040,%r1008,%r1032;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1008;
mov.b32 %r1043, {high,low};}


	
	{fma.rn.f16x2 %r1045,%r1037,%r1043,%r1040;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1045;
mov.b32 %r1049, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1045;
mov.b32 %r1051, {high,high};}


	
	{mul.f16x2 %r1053,%r846,%r1051;
}

	
	{xor.b32 %r1056,%r1053,0x80008000;
}

	
	{fma.rn.f16x2 %r1058,%r843,%r1049,%r1056;
}

	
	{mul.f16x2 %r1062,%r843,%r1051;
}

	
	{fma.rn.f16x2 %r1065,%r846,%r1049,%r1062;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r1069, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r1071, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1073, {low,high};}


	
	{mul.f16x2 %r1074,%r1071,%r1073;
}

	
	{mul.f16x2 %r1077,%r1045,%r1069;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1045;
mov.b32 %r1080, {high,low};}


	
	{fma.rn.f16x2 %r1082,%r1074,%r1080,%r1077;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1082;
mov.b32 %r1086, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1082;
mov.b32 %r1088, {high,high};}


	
	{mul.f16x2 %r1090,%r858,%r1088;
}

	
	{xor.b32 %r1093,%r1090,0x80008000;
}

	
	{fma.rn.f16x2 %r1095,%r855,%r1086,%r1093;
}

	
	{mul.f16x2 %r1099,%r855,%r1088;
}

	
	{fma.rn.f16x2 %r1102,%r858,%r1086,%r1099;
}

	shl.b32 %r1128, %r1127, 3;
add.s32 %r91, %r662, %r1128;
barrier.sync 0;
shl.b32 %r1130, %r76, 6;
add.s32 %r1131, %r91, %r1130;
st.shared.u32 [%r1131], %r813;
st.shared.u32 [%r1131+4], %r816;
st.shared.u32 [%r1131+64], %r873;
st.shared.u32 [%r1131+68], %r880;
st.shared.u32 [%r1131+128], %r910;
st.shared.u32 [%r1131+132], %r917;
st.shared.u32 [%r1131+192], %r947;
st.shared.u32 [%r1131+196], %r954;
st.shared.u32 [%r1131+256], %r984;
st.shared.u32 [%r1131+260], %r991;
st.shared.u32 [%r1131+320], %r1021;
st.shared.u32 [%r1131+324], %r1028;
st.shared.u32 [%r1131+384], %r1058;
st.shared.u32 [%r1131+388], %r1065;
st.shared.u32 [%r1131+448], %r1095;
st.shared.u32 [%r1131+452], %r1102;
barrier.sync 0;
shl.b32 %r1590, %r76, 3;
add.s32 %r1591, %r91, %r1590;
ld.shared.u32 %r1133, [%r1591];
ld.shared.u32 %r1136, [%r1591+4];
ld.shared.u32 %r1183, [%r1591+4096];
ld.shared.u32 %r1186, [%r1591+4100];
ld.shared.u32 %r1145, [%r1591+8192];
ld.shared.u32 %r1148, [%r1591+8196];
ld.shared.u32 %r1195, [%r1591+12288];
ld.shared.u32 %r1198, [%r1591+12292];
ld.shared.u32 %r1134, [%r1591+16384];
ld.shared.u32 %r1137, [%r1591+16388];
ld.shared.u32 %r1184, [%r1591+20480];
ld.shared.u32 %r1187, [%r1591+20484];
ld.shared.u32 %r1146, [%r1591+24576];
ld.shared.u32 %r1149, [%r1591+24580];
ld.shared.u32 %r1196, [%r1591+28672];
ld.shared.u32 %r1199, [%r1591+28676];

	{add.f16x2 %r1132,%r1133,%r1134;
}

	
	{add.f16x2 %r1135,%r1136,%r1137;
}

	
	{sub.f16x2 %r1138,%r1133,%r1134;
}

	
	{sub.f16x2 %r1141,%r1136,%r1137;
}

	
	{add.f16x2 %r1144,%r1145,%r1146;
}

	
	{add.f16x2 %r1147,%r1148,%r1149;
}

	
	{sub.f16x2 %r1150,%r1145,%r1146;
}

	
	{sub.f16x2 %r1153,%r1148,%r1149;
}

	
	{xor.b32 %r1156,%r1150,0x80008000;
}

	
	{add.f16x2 %r1158,%r1132,%r1144;
}

	
	{add.f16x2 %r1161,%r1135,%r1147;
}

	
	{sub.f16x2 %r1164,%r1132,%r1144;
}

	
	{sub.f16x2 %r1167,%r1135,%r1147;
}

	
	{add.f16x2 %r1170,%r1138,%r1153;
}

	
	{add.f16x2 %r1173,%r1141,%r1156;
}

	
	{sub.f16x2 %r1176,%r1138,%r1153;
}

	
	{sub.f16x2 %r1179,%r1141,%r1156;
}

	
	{add.f16x2 %r1182,%r1183,%r1184;
}

	
	{add.f16x2 %r1185,%r1186,%r1187;
}

	
	{sub.f16x2 %r1188,%r1183,%r1184;
}

	
	{sub.f16x2 %r1191,%r1186,%r1187;
}

	
	{add.f16x2 %r1194,%r1195,%r1196;
}

	
	{add.f16x2 %r1197,%r1198,%r1199;
}

	
	{sub.f16x2 %r1200,%r1195,%r1196;
}

	
	{sub.f16x2 %r1203,%r1198,%r1199;
}

	
	{xor.b32 %r1206,%r1200,0x80008000;
}

	
	{add.f16x2 %r1208,%r1182,%r1194;
}

	
	{add.f16x2 %r1211,%r1185,%r1197;
}

	
	{sub.f16x2 %r1214,%r1182,%r1194;
}

	
	{sub.f16x2 %r1217,%r1185,%r1197;
}

	
	{add.f16x2 %r1220,%r1188,%r1203;
}

	
	{add.f16x2 %r1223,%r1191,%r1206;
}

	
	{sub.f16x2 %r1226,%r1188,%r1203;
}

	
	{sub.f16x2 %r1229,%r1191,%r1206;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r1232, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1233, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1236, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1237, {low,high};}


	
	{mul.f16x2 %r1246,%r1220,%r1232;
}

	
	{mul.f16x2 %r1249,%r1223,%r1233;
}

	
	{sub.f16x2 %r1252,%r1246,%r1249;
}

	
	{mul.f16x2 %r1255,%r1220,%r1233;
}

	
	{fma.rn.f16x2 %r1258,%r1223,%r1232,%r1255;
}

	
	{xor.b32 %r1262,%r1214,0x80008000;
}

	
	{mul.f16x2 %r1264,%r1226,%r1236;
}

	
	{mul.f16x2 %r1267,%r1229,%r1237;
}

	
	{sub.f16x2 %r1270,%r1264,%r1267;
}

	
	{mul.f16x2 %r1273,%r1226,%r1237;
}

	
	{fma.rn.f16x2 %r1276,%r1229,%r1236,%r1273;
}

	
	{add.f16x2 %r1280,%r1158,%r1208;
}

	
	{add.f16x2 %r1283,%r1161,%r1211;
}

	
	{sub.f16x2 %r1286,%r1158,%r1208;
}

	
	{sub.f16x2 %r1289,%r1161,%r1211;
}

	
	{add.f16x2 %r1292,%r1170,%r1252;
}

	
	{add.f16x2 %r1295,%r1173,%r1258;
}

	
	{sub.f16x2 %r1298,%r1170,%r1252;
}

	
	{sub.f16x2 %r1301,%r1173,%r1258;
}

	
	{add.f16x2 %r1304,%r1164,%r1217;
}

	
	{add.f16x2 %r1307,%r1167,%r1262;
}

	
	{sub.f16x2 %r1310,%r1164,%r1217;
}

	
	{sub.f16x2 %r1313,%r1167,%r1262;
}

	
	{add.f16x2 %r1316,%r1176,%r1270;
}

	
	{add.f16x2 %r1319,%r1179,%r1276;
}

	
	{sub.f16x2 %r1322,%r1176,%r1270;
}

	
	{sub.f16x2 %r1325,%r1179,%r1276;
}

	and.b32 %r95, %r5, 448;
bfe.u32 %r1592, %r5, 6, 3;
and.b32 %r1593, %r5, 63;
add.s32 %r1594, %r56, %r1593;
cvt.rn.f32.u32	%f146, %r1592;
mul.f32 %f147, %f146, 0f3DC90FDB;
cos.approx.f32 %f128, %f147;
sin.approx.f32 %f148, %f147;
neg.f32 %f129, %f148;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f128;
cvt.rn.f16.f32 high, %f129;
mov.b32 %r1328, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1331, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1333, {high,high};}


	
	{mul.f16x2 %r1335,%r1295,%r1333;
}

	
	{xor.b32 %r1338,%r1335,0x80008000;
}

	
	{fma.rn.f16x2 %r1340,%r1292,%r1331,%r1338;
}

	
	{mul.f16x2 %r1344,%r1292,%r1333;
}

	
	{fma.rn.f16x2 %r1347,%r1295,%r1331,%r1344;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1351, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1353, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1355, {low,high};}


	
	{mul.f16x2 %r1356,%r1353,%r1355;
}

	
	{mul.f16x2 %r1359,%r1328,%r1351;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1362, {high,low};}


	
	{fma.rn.f16x2 %r1364,%r1356,%r1362,%r1359;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1364;
mov.b32 %r1368, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1364;
mov.b32 %r1370, {high,high};}


	
	{mul.f16x2 %r1372,%r1307,%r1370;
}

	
	{xor.b32 %r1375,%r1372,0x80008000;
}

	
	{fma.rn.f16x2 %r1377,%r1304,%r1368,%r1375;
}

	
	{mul.f16x2 %r1381,%r1304,%r1370;
}

	
	{fma.rn.f16x2 %r1384,%r1307,%r1368,%r1381;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1388, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1390, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1392, {low,high};}


	
	{mul.f16x2 %r1393,%r1390,%r1392;
}

	
	{mul.f16x2 %r1396,%r1364,%r1388;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1364;
mov.b32 %r1399, {high,low};}


	
	{fma.rn.f16x2 %r1401,%r1393,%r1399,%r1396;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1401;
mov.b32 %r1405, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1401;
mov.b32 %r1407, {high,high};}


	
	{mul.f16x2 %r1409,%r1319,%r1407;
}

	
	{xor.b32 %r1412,%r1409,0x80008000;
}

	
	{fma.rn.f16x2 %r1414,%r1316,%r1405,%r1412;
}

	
	{mul.f16x2 %r1418,%r1316,%r1407;
}

	
	{fma.rn.f16x2 %r1421,%r1319,%r1405,%r1418;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1425, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1427, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1429, {low,high};}


	
	{mul.f16x2 %r1430,%r1427,%r1429;
}

	
	{mul.f16x2 %r1433,%r1401,%r1425;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1401;
mov.b32 %r1436, {high,low};}


	
	{fma.rn.f16x2 %r1438,%r1430,%r1436,%r1433;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1438;
mov.b32 %r1442, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1438;
mov.b32 %r1444, {high,high};}


	
	{mul.f16x2 %r1446,%r1289,%r1444;
}

	
	{xor.b32 %r1449,%r1446,0x80008000;
}

	
	{fma.rn.f16x2 %r1451,%r1286,%r1442,%r1449;
}

	
	{mul.f16x2 %r1455,%r1286,%r1444;
}

	
	{fma.rn.f16x2 %r1458,%r1289,%r1442,%r1455;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1462, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1464, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1466, {low,high};}


	
	{mul.f16x2 %r1467,%r1464,%r1466;
}

	
	{mul.f16x2 %r1470,%r1438,%r1462;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1438;
mov.b32 %r1473, {high,low};}


	
	{fma.rn.f16x2 %r1475,%r1467,%r1473,%r1470;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1475;
mov.b32 %r1479, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1475;
mov.b32 %r1481, {high,high};}


	
	{mul.f16x2 %r1483,%r1301,%r1481;
}

	
	{xor.b32 %r1486,%r1483,0x80008000;
}

	
	{fma.rn.f16x2 %r1488,%r1298,%r1479,%r1486;
}

	
	{mul.f16x2 %r1492,%r1298,%r1481;
}

	
	{fma.rn.f16x2 %r1495,%r1301,%r1479,%r1492;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1499, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1501, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1503, {low,high};}


	
	{mul.f16x2 %r1504,%r1501,%r1503;
}

	
	{mul.f16x2 %r1507,%r1475,%r1499;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1475;
mov.b32 %r1510, {high,low};}


	
	{fma.rn.f16x2 %r1512,%r1504,%r1510,%r1507;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1512;
mov.b32 %r1516, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1512;
mov.b32 %r1518, {high,high};}


	
	{mul.f16x2 %r1520,%r1313,%r1518;
}

	
	{xor.b32 %r1523,%r1520,0x80008000;
}

	
	{fma.rn.f16x2 %r1525,%r1310,%r1516,%r1523;
}

	
	{mul.f16x2 %r1529,%r1310,%r1518;
}

	
	{fma.rn.f16x2 %r1532,%r1313,%r1516,%r1529;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1536, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1538, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1540, {low,high};}


	
	{mul.f16x2 %r1541,%r1538,%r1540;
}

	
	{mul.f16x2 %r1544,%r1512,%r1536;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1512;
mov.b32 %r1547, {high,low};}


	
	{fma.rn.f16x2 %r1549,%r1541,%r1547,%r1544;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1549;
mov.b32 %r1553, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1549;
mov.b32 %r1555, {high,high};}


	
	{mul.f16x2 %r1557,%r1325,%r1555;
}

	
	{xor.b32 %r1560,%r1557,0x80008000;
}

	
	{fma.rn.f16x2 %r1562,%r1322,%r1553,%r1560;
}

	
	{mul.f16x2 %r1566,%r1322,%r1555;
}

	
	{fma.rn.f16x2 %r1569,%r1325,%r1553,%r1566;
}

	shl.b32 %r1595, %r1594, 3;
add.s32 %r110, %r662, %r1595;
barrier.sync 0;
shl.b32 %r1597, %r95, 6;
add.s32 %r1598, %r110, %r1597;
st.shared.u32 [%r1598], %r1280;
st.shared.u32 [%r1598+4], %r1283;
st.shared.u32 [%r1598+512], %r1340;
st.shared.u32 [%r1598+516], %r1347;
st.shared.u32 [%r1598+1024], %r1377;
st.shared.u32 [%r1598+1028], %r1384;
st.shared.u32 [%r1598+1536], %r1414;
st.shared.u32 [%r1598+1540], %r1421;
st.shared.u32 [%r1598+2048], %r1451;
st.shared.u32 [%r1598+2052], %r1458;
st.shared.u32 [%r1598+2560], %r1488;
st.shared.u32 [%r1598+2564], %r1495;
st.shared.u32 [%r1598+3072], %r1525;
st.shared.u32 [%r1598+3076], %r1532;
st.shared.u32 [%r1598+3584], %r1562;
st.shared.u32 [%r1598+3588], %r1569;
barrier.sync 0;
shl.b32 %r1843, %r95, 3;
add.s32 %r1844, %r110, %r1843;
ld.shared.u32 %r1600, [%r1844];
ld.shared.u32 %r1603, [%r1844+4];
ld.shared.u32 %r1650, [%r1844+4096];
ld.shared.u32 %r1653, [%r1844+4100];
ld.shared.u32 %r1612, [%r1844+8192];
ld.shared.u32 %r1615, [%r1844+8196];
ld.shared.u32 %r1662, [%r1844+12288];
ld.shared.u32 %r1665, [%r1844+12292];
ld.shared.u32 %r1601, [%r1844+16384];
ld.shared.u32 %r1604, [%r1844+16388];
ld.shared.u32 %r1651, [%r1844+20480];
ld.shared.u32 %r1654, [%r1844+20484];
ld.shared.u32 %r1613, [%r1844+24576];
ld.shared.u32 %r1616, [%r1844+24580];
ld.shared.u32 %r1663, [%r1844+28672];
ld.shared.u32 %r1666, [%r1844+28676];

	{add.f16x2 %r1599,%r1600,%r1601;
}

	
	{add.f16x2 %r1602,%r1603,%r1604;
}

	
	{sub.f16x2 %r1605,%r1600,%r1601;
}

	
	{sub.f16x2 %r1608,%r1603,%r1604;
}

	
	{add.f16x2 %r1611,%r1612,%r1613;
}

	
	{add.f16x2 %r1614,%r1615,%r1616;
}

	
	{sub.f16x2 %r1617,%r1612,%r1613;
}

	
	{sub.f16x2 %r1620,%r1615,%r1616;
}

	
	{xor.b32 %r1623,%r1617,0x80008000;
}

	
	{add.f16x2 %r1625,%r1599,%r1611;
}

	
	{add.f16x2 %r1628,%r1602,%r1614;
}

	
	{sub.f16x2 %r1631,%r1599,%r1611;
}

	
	{sub.f16x2 %r1634,%r1602,%r1614;
}

	
	{add.f16x2 %r1637,%r1605,%r1620;
}

	
	{add.f16x2 %r1640,%r1608,%r1623;
}

	
	{sub.f16x2 %r1643,%r1605,%r1620;
}

	
	{sub.f16x2 %r1646,%r1608,%r1623;
}

	
	{add.f16x2 %r1649,%r1650,%r1651;
}

	
	{add.f16x2 %r1652,%r1653,%r1654;
}

	
	{sub.f16x2 %r1655,%r1650,%r1651;
}

	
	{sub.f16x2 %r1658,%r1653,%r1654;
}

	
	{add.f16x2 %r1661,%r1662,%r1663;
}

	
	{add.f16x2 %r1664,%r1665,%r1666;
}

	
	{sub.f16x2 %r1667,%r1662,%r1663;
}

	
	{sub.f16x2 %r1670,%r1665,%r1666;
}

	
	{xor.b32 %r1673,%r1667,0x80008000;
}

	
	{add.f16x2 %r1675,%r1649,%r1661;
}

	
	{add.f16x2 %r1678,%r1652,%r1664;
}

	
	{sub.f16x2 %r1681,%r1649,%r1661;
}

	
	{sub.f16x2 %r1684,%r1652,%r1664;
}

	
	{add.f16x2 %r1687,%r1655,%r1670;
}

	
	{add.f16x2 %r1690,%r1658,%r1673;
}

	
	{sub.f16x2 %r1693,%r1655,%r1670;
}

	
	{sub.f16x2 %r1696,%r1658,%r1673;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r1699, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1700, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1703, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1704, {low,high};}


	
	{mul.f16x2 %r1713,%r1687,%r1699;
}

	
	{mul.f16x2 %r1716,%r1690,%r1700;
}

	
	{sub.f16x2 %r1719,%r1713,%r1716;
}

	
	{mul.f16x2 %r1722,%r1687,%r1700;
}

	
	{fma.rn.f16x2 %r1725,%r1690,%r1699,%r1722;
}

	
	{xor.b32 %r1729,%r1681,0x80008000;
}

	
	{mul.f16x2 %r1731,%r1693,%r1703;
}

	
	{mul.f16x2 %r1734,%r1696,%r1704;
}

	
	{sub.f16x2 %r1737,%r1731,%r1734;
}

	
	{mul.f16x2 %r1740,%r1693,%r1704;
}

	
	{fma.rn.f16x2 %r1743,%r1696,%r1703,%r1740;
}

	
	{add.f16x2 %r1747,%r1625,%r1675;
}

	
	{add.f16x2 %r1750,%r1628,%r1678;
}

	
	{sub.f16x2 %r1753,%r1625,%r1675;
}

	
	{sub.f16x2 %r1756,%r1628,%r1678;
}

	
	{add.f16x2 %r1759,%r1637,%r1719;
}

	
	{add.f16x2 %r1762,%r1640,%r1725;
}

	
	{sub.f16x2 %r1765,%r1637,%r1719;
}

	
	{sub.f16x2 %r1768,%r1640,%r1725;
}

	
	{add.f16x2 %r1771,%r1631,%r1684;
}

	
	{add.f16x2 %r1774,%r1634,%r1729;
}

	
	{sub.f16x2 %r1777,%r1631,%r1684;
}

	
	{sub.f16x2 %r1780,%r1634,%r1729;
}

	
	{add.f16x2 %r1783,%r1643,%r1737;
}

	
	{add.f16x2 %r1786,%r1646,%r1743;
}

	
	{sub.f16x2 %r1789,%r1643,%r1737;
}

	
	{sub.f16x2 %r1792,%r1646,%r1743;
}

	selp.b32	%r1842, %r1789, %r1792, %p1;
selp.b32	%r1841, %r1792, %r1789, %p1;
selp.b32	%r1836, %r1777, %r1780, %p1;
selp.b32	%r1835, %r1780, %r1777, %p1;
selp.b32	%r1830, %r1765, %r1768, %p1;
selp.b32	%r1829, %r1768, %r1765, %p1;
selp.b32	%r1824, %r1753, %r1756, %p1;
selp.b32	%r1823, %r1756, %r1753, %p1;
selp.b32	%r1818, %r1783, %r1786, %p1;
selp.b32	%r1817, %r1786, %r1783, %p1;
selp.b32	%r1812, %r1771, %r1774, %p1;
selp.b32	%r1811, %r1774, %r1771, %p1;
selp.b32	%r1806, %r1759, %r1762, %p1;
selp.b32	%r1805, %r1762, %r1759, %p1;
selp.b32	%r1800, %r1747, %r1750, %p1;
selp.b32	%r1799, %r1750, %r1747, %p1;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1799;
mov.b32 {blow,bhigh}, %r1800;
mov.b32 %r1795, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1799;
mov.b32 {blow,bhigh}, %r1800;
mov.b32 %r1798, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1805;
mov.b32 {blow,bhigh}, %r1806;
mov.b32 %r1801, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1805;
mov.b32 {blow,bhigh}, %r1806;
mov.b32 %r1804, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1811;
mov.b32 {blow,bhigh}, %r1812;
mov.b32 %r1807, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1811;
mov.b32 {blow,bhigh}, %r1812;
mov.b32 %r1810, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1817;
mov.b32 {blow,bhigh}, %r1818;
mov.b32 %r1813, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1817;
mov.b32 {blow,bhigh}, %r1818;
mov.b32 %r1816, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1823;
mov.b32 {blow,bhigh}, %r1824;
mov.b32 %r1819, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1823;
mov.b32 {blow,bhigh}, %r1824;
mov.b32 %r1822, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1829;
mov.b32 {blow,bhigh}, %r1830;
mov.b32 %r1825, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1829;
mov.b32 {blow,bhigh}, %r1830;
mov.b32 %r1828, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1835;
mov.b32 {blow,bhigh}, %r1836;
mov.b32 %r1831, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1835;
mov.b32 {blow,bhigh}, %r1836;
mov.b32 %r1834, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1841;
mov.b32 {blow,bhigh}, %r1842;
mov.b32 %r1837, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1841;
mov.b32 {blow,bhigh}, %r1842;
mov.b32 %r1840, {ahigh,bhigh};}


	cvta.to.global.u64 %rd8, %rd5;
shl.b64 %rd9, %rd1, 2;
add.s64 %rd3, %rd8, %rd9;
@%p2 bra BB11_6;
bra.uni BB11_5;

BB11_6:
st.global.u32 [%rd3], %r1795;
st.global.u32 [%rd3+2048], %r1801;
st.global.u32 [%rd3+4096], %r1807;
st.global.u32 [%rd3+6144], %r1813;
st.global.u32 [%rd3+8192], %r1819;
st.global.u32 [%rd3+10240], %r1825;
st.global.u32 [%rd3+12288], %r1831;
st.global.u32 [%rd3+14336], %r1837;
bra.uni BB11_7;

BB11_5:
shl.b32 %r1845, %r3, 1;
add.s32 %r1846, %r1845, -1;
st.global.u32 [%rd3], %r1795;
st.global.u32 [%rd3+2048], %r1801;
st.global.u32 [%rd3+4096], %r1807;
st.global.u32 [%rd3+6144], %r1813;
st.global.u32 [%rd3+8192], %r1819;
st.global.u32 [%rd3+10240], %r1825;
st.global.u32 [%rd3+12288], %r1831;
st.global.u32 [%rd3+14336], %r1837;
setp.ge.u32	%p5, %r1846, %r1;
@%p5 bra BB11_8;

BB11_7:
st.global.u32 [%rd3+16384], %r1798;
st.global.u32 [%rd3+18432], %r1804;
st.global.u32 [%rd3+20480], %r1810;
st.global.u32 [%rd3+22528], %r1816;
st.global.u32 [%rd3+24576], %r1822;
st.global.u32 [%rd3+26624], %r1828;
st.global.u32 [%rd3+28672], %r1834;
st.global.u32 [%rd3+30720], %r1840;

BB11_8:
ret;
}


.weak .entry _Z10vector_fftILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z10vector_fftILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 1024, 1, 1
{
.reg .pred %p<6>;
.reg .b16 %rs<9>;
.reg .f32 %f<198>;
.reg .b32 %r<2238>;
.reg .f64 %fd<2>;
.reg .b64 %rd<10>;


ld.param.u32 %r1, [_Z10vector_fftILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u32 %r182, [_Z10vector_fftILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+16];
ld.param.u64 %rd5, [_Z10vector_fftILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z10vector_fftILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
mov.u32 %r2, %ctaid.x;
shl.b32 %r203, %r2, 14;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
add.s32 %r204, %r203, %r5;
cvt.u64.u32	%rd1, %r204;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r204, 4;
add.s64 %rd2, %rd6, %rd7;
@%p2 bra BB12_2;
bra.uni BB12_1;

BB12_2:
ld.global.u32 %r2237, [%rd2];
ld.global.u32 %r2235, [%rd2+4096];
ld.global.u32 %r2233, [%rd2+8192];
ld.global.u32 %r2231, [%rd2+12288];
ld.global.u32 %r2229, [%rd2+16384];
ld.global.u32 %r2227, [%rd2+20480];
ld.global.u32 %r2225, [%rd2+24576];
ld.global.u32 %r2223, [%rd2+28672];
bra.uni BB12_3;

BB12_1:
shl.b32 %r206, %r3, 1;
add.s32 %r207, %r206, -1;
ld.global.u32 %r2237, [%rd2];
ld.global.u32 %r2235, [%rd2+4096];
ld.global.u32 %r2233, [%rd2+8192];
ld.global.u32 %r2231, [%rd2+12288];
ld.global.u32 %r2229, [%rd2+16384];
ld.global.u32 %r2227, [%rd2+20480];
ld.global.u32 %r2225, [%rd2+24576];
ld.global.u32 %r2223, [%rd2+28672];
setp.ge.u32	%p3, %r207, %r1;
@%p3 bra BB12_4;

BB12_3:
ld.global.u32 %r2236, [%rd2+32768];
ld.global.u32 %r2234, [%rd2+36864];
ld.global.u32 %r2232, [%rd2+40960];
ld.global.u32 %r2230, [%rd2+45056];
ld.global.u32 %r2228, [%rd2+49152];
ld.global.u32 %r2226, [%rd2+53248];
ld.global.u32 %r2224, [%rd2+57344];
ld.global.u32 %r2222, [%rd2+61440];

BB12_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2237;
mov.b32 {blow,bhigh}, %r2236;
mov.b32 %r208, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2237;
mov.b32 {blow,bhigh}, %r2236;
mov.b32 %r211, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2235;
mov.b32 {blow,bhigh}, %r2234;
mov.b32 %r214, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2235;
mov.b32 {blow,bhigh}, %r2234;
mov.b32 %r217, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2233;
mov.b32 {blow,bhigh}, %r2232;
mov.b32 %r220, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2233;
mov.b32 {blow,bhigh}, %r2232;
mov.b32 %r223, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2231;
mov.b32 {blow,bhigh}, %r2230;
mov.b32 %r226, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2231;
mov.b32 {blow,bhigh}, %r2230;
mov.b32 %r229, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2229;
mov.b32 {blow,bhigh}, %r2228;
mov.b32 %r232, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2229;
mov.b32 {blow,bhigh}, %r2228;
mov.b32 %r235, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2227;
mov.b32 {blow,bhigh}, %r2226;
mov.b32 %r238, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2227;
mov.b32 {blow,bhigh}, %r2226;
mov.b32 %r241, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2225;
mov.b32 {blow,bhigh}, %r2224;
mov.b32 %r244, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2225;
mov.b32 {blow,bhigh}, %r2224;
mov.b32 %r247, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2223;
mov.b32 {blow,bhigh}, %r2222;
mov.b32 %r250, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2223;
mov.b32 {blow,bhigh}, %r2222;
mov.b32 %r253, {ahigh,bhigh};}


	setp.eq.s32	%p1, %r182, 1;
selp.b32	%r329, %r250, %r253, %p1;
selp.b32	%r326, %r253, %r250, %p1;
selp.b32	%r279, %r244, %r247, %p1;
selp.b32	%r276, %r247, %r244, %p1;
selp.b32	%r317, %r238, %r241, %p1;
selp.b32	%r314, %r241, %r238, %p1;
selp.b32	%r267, %r232, %r235, %p1;
selp.b32	%r264, %r235, %r232, %p1;
selp.b32	%r328, %r226, %r229, %p1;
selp.b32	%r325, %r229, %r226, %p1;
selp.b32	%r278, %r220, %r223, %p1;
selp.b32	%r275, %r223, %r220, %p1;
selp.b32	%r316, %r214, %r217, %p1;
selp.b32	%r313, %r217, %r214, %p1;
selp.b32	%r266, %r208, %r211, %p1;
selp.b32	%r263, %r211, %r208, %p1;

	{add.f16x2 %r256,%r263,%r264;
}

	
	{add.f16x2 %r259,%r266,%r267;
}

	
	{sub.f16x2 %r262,%r263,%r264;
}

	
	{sub.f16x2 %r265,%r266,%r267;
}

	
	{add.f16x2 %r268,%r275,%r276;
}

	
	{add.f16x2 %r271,%r278,%r279;
}

	
	{sub.f16x2 %r274,%r275,%r276;
}

	
	{sub.f16x2 %r277,%r278,%r279;
}

	
	{xor.b32 %r280,%r274,0x80008000;
}

	
	{add.f16x2 %r282,%r256,%r268;
}

	
	{add.f16x2 %r285,%r259,%r271;
}

	
	{sub.f16x2 %r288,%r256,%r268;
}

	
	{sub.f16x2 %r291,%r259,%r271;
}

	
	{add.f16x2 %r294,%r262,%r277;
}

	
	{add.f16x2 %r297,%r265,%r280;
}

	
	{sub.f16x2 %r300,%r262,%r277;
}

	
	{sub.f16x2 %r303,%r265,%r280;
}

	
	{add.f16x2 %r306,%r313,%r314;
}

	
	{add.f16x2 %r309,%r316,%r317;
}

	
	{sub.f16x2 %r312,%r313,%r314;
}

	
	{sub.f16x2 %r315,%r316,%r317;
}

	
	{add.f16x2 %r318,%r325,%r326;
}

	
	{add.f16x2 %r321,%r328,%r329;
}

	
	{sub.f16x2 %r324,%r325,%r326;
}

	
	{sub.f16x2 %r327,%r328,%r329;
}

	
	{xor.b32 %r330,%r324,0x80008000;
}

	
	{add.f16x2 %r332,%r306,%r318;
}

	
	{add.f16x2 %r335,%r309,%r321;
}

	
	{sub.f16x2 %r338,%r306,%r318;
}

	
	{sub.f16x2 %r341,%r309,%r321;
}

	
	{add.f16x2 %r344,%r312,%r327;
}

	
	{add.f16x2 %r347,%r315,%r330;
}

	
	{sub.f16x2 %r350,%r312,%r327;
}

	
	{sub.f16x2 %r353,%r315,%r330;
}

	mov.f32 %f3, 0f3F3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r356, {low,high};}


	mov.f32 %f13, 0fBF3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r357, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r360, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r361, {low,high};}


	
	{mul.f16x2 %r370,%r344,%r356;
}

	
	{mul.f16x2 %r373,%r347,%r357;
}

	
	{sub.f16x2 %r376,%r370,%r373;
}

	
	{mul.f16x2 %r379,%r344,%r357;
}

	
	{fma.rn.f16x2 %r382,%r347,%r356,%r379;
}

	
	{xor.b32 %r386,%r338,0x80008000;
}

	
	{mul.f16x2 %r388,%r350,%r360;
}

	
	{mul.f16x2 %r391,%r353,%r361;
}

	
	{sub.f16x2 %r394,%r388,%r391;
}

	
	{mul.f16x2 %r397,%r350,%r361;
}

	
	{fma.rn.f16x2 %r400,%r353,%r360,%r397;
}

	
	{add.f16x2 %r404,%r282,%r332;
}

	
	{add.f16x2 %r407,%r285,%r335;
}

	
	{sub.f16x2 %r410,%r282,%r332;
}

	
	{sub.f16x2 %r413,%r285,%r335;
}

	
	{add.f16x2 %r416,%r294,%r376;
}

	
	{add.f16x2 %r419,%r297,%r382;
}

	
	{sub.f16x2 %r422,%r294,%r376;
}

	
	{sub.f16x2 %r425,%r297,%r382;
}

	
	{add.f16x2 %r428,%r288,%r341;
}

	
	{add.f16x2 %r431,%r291,%r386;
}

	
	{sub.f16x2 %r434,%r288,%r341;
}

	
	{sub.f16x2 %r437,%r291,%r386;
}

	
	{add.f16x2 %r440,%r300,%r394;
}

	
	{add.f16x2 %r443,%r303,%r400;
}

	
	{sub.f16x2 %r446,%r300,%r394;
}

	
	{sub.f16x2 %r449,%r303,%r400;
}

	and.b32 %r56, %r5, 1023;
cvt.rn.f32.u32	%f48, %r56;
mul.f32 %f49, %f48, 0f3A490FDB;
cos.approx.f32 %f30, %f49;
sin.approx.f32 %f50, %f49;
neg.f32 %f31, %f50;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f30;
cvt.rn.f16.f32 high, %f31;
mov.b32 %r452, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r455, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r457, {high,high};}


	
	{mul.f16x2 %r459,%r419,%r457;
}

	
	{xor.b32 %r462,%r459,0x80008000;
}

	
	{fma.rn.f16x2 %r464,%r416,%r455,%r462;
}

	
	{mul.f16x2 %r468,%r416,%r457;
}

	
	{fma.rn.f16x2 %r471,%r419,%r455,%r468;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r475, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r477, {high,high};}


	mov.f32 %f44, 0fBF800000;
mov.f32 %f45, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r479, {low,high};}


	
	{mul.f16x2 %r480,%r477,%r479;
}

	
	{mul.f16x2 %r483,%r452,%r475;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r486, {high,low};}


	
	{fma.rn.f16x2 %r488,%r480,%r486,%r483;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r488;
mov.b32 %r492, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r488;
mov.b32 %r494, {high,high};}


	
	{mul.f16x2 %r496,%r431,%r494;
}

	
	{xor.b32 %r499,%r496,0x80008000;
}

	
	{fma.rn.f16x2 %r501,%r428,%r492,%r499;
}

	
	{mul.f16x2 %r505,%r428,%r494;
}

	
	{fma.rn.f16x2 %r508,%r431,%r492,%r505;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r512, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r514, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r516, {low,high};}


	
	{mul.f16x2 %r517,%r514,%r516;
}

	
	{mul.f16x2 %r520,%r488,%r512;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r488;
mov.b32 %r523, {high,low};}


	
	{fma.rn.f16x2 %r525,%r517,%r523,%r520;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r525;
mov.b32 %r529, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r525;
mov.b32 %r531, {high,high};}


	
	{mul.f16x2 %r533,%r443,%r531;
}

	
	{xor.b32 %r536,%r533,0x80008000;
}

	
	{fma.rn.f16x2 %r538,%r440,%r529,%r536;
}

	
	{mul.f16x2 %r542,%r440,%r531;
}

	
	{fma.rn.f16x2 %r545,%r443,%r529,%r542;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r549, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r551, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r553, {low,high};}


	
	{mul.f16x2 %r554,%r551,%r553;
}

	
	{mul.f16x2 %r557,%r525,%r549;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r525;
mov.b32 %r560, {high,low};}


	
	{fma.rn.f16x2 %r562,%r554,%r560,%r557;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r562;
mov.b32 %r566, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r562;
mov.b32 %r568, {high,high};}


	
	{mul.f16x2 %r570,%r413,%r568;
}

	
	{xor.b32 %r573,%r570,0x80008000;
}

	
	{fma.rn.f16x2 %r575,%r410,%r566,%r573;
}

	
	{mul.f16x2 %r579,%r410,%r568;
}

	
	{fma.rn.f16x2 %r582,%r413,%r566,%r579;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r586, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r588, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r590, {low,high};}


	
	{mul.f16x2 %r591,%r588,%r590;
}

	
	{mul.f16x2 %r594,%r562,%r586;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r562;
mov.b32 %r597, {high,low};}


	
	{fma.rn.f16x2 %r599,%r591,%r597,%r594;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r599;
mov.b32 %r603, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r599;
mov.b32 %r605, {high,high};}


	
	{mul.f16x2 %r607,%r425,%r605;
}

	
	{xor.b32 %r610,%r607,0x80008000;
}

	
	{fma.rn.f16x2 %r612,%r422,%r603,%r610;
}

	
	{mul.f16x2 %r616,%r422,%r605;
}

	
	{fma.rn.f16x2 %r619,%r425,%r603,%r616;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r623, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r625, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r627, {low,high};}


	
	{mul.f16x2 %r628,%r625,%r627;
}

	
	{mul.f16x2 %r631,%r599,%r623;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r599;
mov.b32 %r634, {high,low};}


	
	{fma.rn.f16x2 %r636,%r628,%r634,%r631;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r636;
mov.b32 %r640, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r636;
mov.b32 %r642, {high,high};}


	
	{mul.f16x2 %r644,%r437,%r642;
}

	
	{xor.b32 %r647,%r644,0x80008000;
}

	
	{fma.rn.f16x2 %r649,%r434,%r640,%r647;
}

	
	{mul.f16x2 %r653,%r434,%r642;
}

	
	{fma.rn.f16x2 %r656,%r437,%r640,%r653;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r660, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r662, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r664, {low,high};}


	
	{mul.f16x2 %r665,%r662,%r664;
}

	
	{mul.f16x2 %r668,%r636,%r660;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r636;
mov.b32 %r671, {high,low};}


	
	{fma.rn.f16x2 %r673,%r665,%r671,%r668;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r673;
mov.b32 %r677, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r673;
mov.b32 %r679, {high,high};}


	
	{mul.f16x2 %r681,%r449,%r679;
}

	
	{xor.b32 %r684,%r681,0x80008000;
}

	
	{fma.rn.f16x2 %r686,%r446,%r677,%r684;
}

	
	{mul.f16x2 %r690,%r446,%r679;
}

	
	{fma.rn.f16x2 %r693,%r449,%r677,%r690;
}

	shl.b32 %r714, %r5, 3;
and.b32 %r71, %r714, -8192;
barrier.sync 0;
shl.b32 %r715, %r56, 3;
add.s32 %r716, %r715, %r71;
shl.b32 %r717, %r716, 2;
mov.u32 %r718, smem_full;
add.s32 %r72, %r718, %r717;
st.shared.u32 [%r72], %r404;
st.shared.u32 [%r72+4], %r464;
st.shared.u32 [%r72+8], %r501;
st.shared.u32 [%r72+12], %r538;
st.shared.u32 [%r72+16], %r575;
st.shared.u32 [%r72+20], %r612;
st.shared.u32 [%r72+24], %r649;
st.shared.u32 [%r72+28], %r686;
barrier.sync 0;
add.s32 %r719, %r56, %r71;
shl.b32 %r720, %r719, 2;
add.s32 %r73, %r718, %r720;
ld.shared.u32 %r74, [%r73];
ld.shared.u32 %r75, [%r73+4096];
ld.shared.u32 %r76, [%r73+8192];
ld.shared.u32 %r77, [%r73+12288];
ld.shared.u32 %r78, [%r73+16384];
ld.shared.u32 %r79, [%r73+20480];
ld.shared.u32 %r80, [%r73+24576];
ld.shared.u32 %r81, [%r73+28672];
barrier.sync 0;
st.shared.u32 [%r72], %r407;
st.shared.u32 [%r72+4], %r471;
st.shared.u32 [%r72+8], %r508;
st.shared.u32 [%r72+12], %r545;
st.shared.u32 [%r72+16], %r582;
st.shared.u32 [%r72+20], %r619;
st.shared.u32 [%r72+24], %r656;
st.shared.u32 [%r72+28], %r693;
barrier.sync 0;
ld.shared.u32 %r726, [%r73];
ld.shared.u32 %r776, [%r73+4096];
ld.shared.u32 %r738, [%r73+8192];
ld.shared.u32 %r788, [%r73+12288];
ld.shared.u32 %r727, [%r73+16384];
ld.shared.u32 %r777, [%r73+20480];
ld.shared.u32 %r739, [%r73+24576];
ld.shared.u32 %r789, [%r73+28672];

	{add.f16x2 %r722,%r74,%r78;
}

	
	{add.f16x2 %r725,%r726,%r727;
}

	
	{sub.f16x2 %r728,%r74,%r78;
}

	
	{sub.f16x2 %r731,%r726,%r727;
}

	
	{add.f16x2 %r734,%r76,%r80;
}

	
	{add.f16x2 %r737,%r738,%r739;
}

	
	{sub.f16x2 %r740,%r76,%r80;
}

	
	{sub.f16x2 %r743,%r738,%r739;
}

	
	{xor.b32 %r746,%r740,0x80008000;
}

	
	{add.f16x2 %r748,%r722,%r734;
}

	
	{add.f16x2 %r751,%r725,%r737;
}

	
	{sub.f16x2 %r754,%r722,%r734;
}

	
	{sub.f16x2 %r757,%r725,%r737;
}

	
	{add.f16x2 %r760,%r728,%r743;
}

	
	{add.f16x2 %r763,%r731,%r746;
}

	
	{sub.f16x2 %r766,%r728,%r743;
}

	
	{sub.f16x2 %r769,%r731,%r746;
}

	
	{add.f16x2 %r772,%r75,%r79;
}

	
	{add.f16x2 %r775,%r776,%r777;
}

	
	{sub.f16x2 %r778,%r75,%r79;
}

	
	{sub.f16x2 %r781,%r776,%r777;
}

	
	{add.f16x2 %r784,%r77,%r81;
}

	
	{add.f16x2 %r787,%r788,%r789;
}

	
	{sub.f16x2 %r790,%r77,%r81;
}

	
	{sub.f16x2 %r793,%r788,%r789;
}

	
	{xor.b32 %r796,%r790,0x80008000;
}

	
	{add.f16x2 %r798,%r772,%r784;
}

	
	{add.f16x2 %r801,%r775,%r787;
}

	
	{sub.f16x2 %r804,%r772,%r784;
}

	
	{sub.f16x2 %r807,%r775,%r787;
}

	
	{add.f16x2 %r810,%r778,%r793;
}

	
	{add.f16x2 %r813,%r781,%r796;
}

	
	{sub.f16x2 %r816,%r778,%r793;
}

	
	{sub.f16x2 %r819,%r781,%r796;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r822, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r823, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r826, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r827, {low,high};}


	
	{mul.f16x2 %r836,%r810,%r822;
}

	
	{mul.f16x2 %r839,%r813,%r823;
}

	
	{sub.f16x2 %r842,%r836,%r839;
}

	
	{mul.f16x2 %r845,%r810,%r823;
}

	
	{fma.rn.f16x2 %r848,%r813,%r822,%r845;
}

	
	{xor.b32 %r852,%r804,0x80008000;
}

	
	{mul.f16x2 %r854,%r816,%r826;
}

	
	{mul.f16x2 %r857,%r819,%r827;
}

	
	{sub.f16x2 %r860,%r854,%r857;
}

	
	{mul.f16x2 %r863,%r816,%r827;
}

	
	{fma.rn.f16x2 %r866,%r819,%r826,%r863;
}

	
	{add.f16x2 %r870,%r748,%r798;
}

	
	{add.f16x2 %r873,%r751,%r801;
}

	
	{sub.f16x2 %r876,%r748,%r798;
}

	
	{sub.f16x2 %r879,%r751,%r801;
}

	
	{add.f16x2 %r882,%r760,%r842;
}

	
	{add.f16x2 %r885,%r763,%r848;
}

	
	{sub.f16x2 %r888,%r760,%r842;
}

	
	{sub.f16x2 %r891,%r763,%r848;
}

	
	{add.f16x2 %r894,%r754,%r807;
}

	
	{add.f16x2 %r897,%r757,%r852;
}

	
	{sub.f16x2 %r900,%r754,%r807;
}

	
	{sub.f16x2 %r903,%r757,%r852;
}

	
	{add.f16x2 %r906,%r766,%r860;
}

	
	{add.f16x2 %r909,%r769,%r866;
}

	
	{sub.f16x2 %r912,%r766,%r860;
}

	
	{sub.f16x2 %r915,%r769,%r866;
}

	and.b32 %r84, %r5, 1016;
bfe.u32 %r1180, %r5, 3, 7;
cvt.rn.f32.u32	%f97, %r1180;
mul.f32 %f98, %f97, 0f3BC90FDB;
cos.approx.f32 %f79, %f98;
sin.approx.f32 %f99, %f98;
neg.f32 %f80, %f99;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f79;
cvt.rn.f16.f32 high, %f80;
mov.b32 %r918, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r921, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r923, {high,high};}


	
	{mul.f16x2 %r925,%r885,%r923;
}

	
	{xor.b32 %r928,%r925,0x80008000;
}

	
	{fma.rn.f16x2 %r930,%r882,%r921,%r928;
}

	
	{mul.f16x2 %r934,%r882,%r923;
}

	
	{fma.rn.f16x2 %r937,%r885,%r921,%r934;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r941, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r943, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r945, {low,high};}


	
	{mul.f16x2 %r946,%r943,%r945;
}

	
	{mul.f16x2 %r949,%r918,%r941;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r952, {high,low};}


	
	{fma.rn.f16x2 %r954,%r946,%r952,%r949;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r954;
mov.b32 %r958, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r954;
mov.b32 %r960, {high,high};}


	
	{mul.f16x2 %r962,%r897,%r960;
}

	
	{xor.b32 %r965,%r962,0x80008000;
}

	
	{fma.rn.f16x2 %r967,%r894,%r958,%r965;
}

	
	{mul.f16x2 %r971,%r894,%r960;
}

	
	{fma.rn.f16x2 %r974,%r897,%r958,%r971;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r978, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r980, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r982, {low,high};}


	
	{mul.f16x2 %r983,%r980,%r982;
}

	
	{mul.f16x2 %r986,%r954,%r978;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r954;
mov.b32 %r989, {high,low};}


	
	{fma.rn.f16x2 %r991,%r983,%r989,%r986;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r991;
mov.b32 %r995, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r991;
mov.b32 %r997, {high,high};}


	
	{mul.f16x2 %r999,%r909,%r997;
}

	
	{xor.b32 %r1002,%r999,0x80008000;
}

	
	{fma.rn.f16x2 %r1004,%r906,%r995,%r1002;
}

	
	{mul.f16x2 %r1008,%r906,%r997;
}

	
	{fma.rn.f16x2 %r1011,%r909,%r995,%r1008;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r1015, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r1017, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1019, {low,high};}


	
	{mul.f16x2 %r1020,%r1017,%r1019;
}

	
	{mul.f16x2 %r1023,%r991,%r1015;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r991;
mov.b32 %r1026, {high,low};}


	
	{fma.rn.f16x2 %r1028,%r1020,%r1026,%r1023;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1028;
mov.b32 %r1032, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1028;
mov.b32 %r1034, {high,high};}


	
	{mul.f16x2 %r1036,%r879,%r1034;
}

	
	{xor.b32 %r1039,%r1036,0x80008000;
}

	
	{fma.rn.f16x2 %r1041,%r876,%r1032,%r1039;
}

	
	{mul.f16x2 %r1045,%r876,%r1034;
}

	
	{fma.rn.f16x2 %r1048,%r879,%r1032,%r1045;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r1052, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r1054, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1056, {low,high};}


	
	{mul.f16x2 %r1057,%r1054,%r1056;
}

	
	{mul.f16x2 %r1060,%r1028,%r1052;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1028;
mov.b32 %r1063, {high,low};}


	
	{fma.rn.f16x2 %r1065,%r1057,%r1063,%r1060;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1065;
mov.b32 %r1069, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1065;
mov.b32 %r1071, {high,high};}


	
	{mul.f16x2 %r1073,%r891,%r1071;
}

	
	{xor.b32 %r1076,%r1073,0x80008000;
}

	
	{fma.rn.f16x2 %r1078,%r888,%r1069,%r1076;
}

	
	{mul.f16x2 %r1082,%r888,%r1071;
}

	
	{fma.rn.f16x2 %r1085,%r891,%r1069,%r1082;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r1089, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r1091, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1093, {low,high};}


	
	{mul.f16x2 %r1094,%r1091,%r1093;
}

	
	{mul.f16x2 %r1097,%r1065,%r1089;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1065;
mov.b32 %r1100, {high,low};}


	
	{fma.rn.f16x2 %r1102,%r1094,%r1100,%r1097;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1102;
mov.b32 %r1106, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1102;
mov.b32 %r1108, {high,high};}


	
	{mul.f16x2 %r1110,%r903,%r1108;
}

	
	{xor.b32 %r1113,%r1110,0x80008000;
}

	
	{fma.rn.f16x2 %r1115,%r900,%r1106,%r1113;
}

	
	{mul.f16x2 %r1119,%r900,%r1108;
}

	
	{fma.rn.f16x2 %r1122,%r903,%r1106,%r1119;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r1126, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r1128, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1130, {low,high};}


	
	{mul.f16x2 %r1131,%r1128,%r1130;
}

	
	{mul.f16x2 %r1134,%r1102,%r1126;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1102;
mov.b32 %r1137, {high,low};}


	
	{fma.rn.f16x2 %r1139,%r1131,%r1137,%r1134;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1139;
mov.b32 %r1143, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1139;
mov.b32 %r1145, {high,high};}


	
	{mul.f16x2 %r1147,%r915,%r1145;
}

	
	{xor.b32 %r1150,%r1147,0x80008000;
}

	
	{fma.rn.f16x2 %r1152,%r912,%r1143,%r1150;
}

	
	{mul.f16x2 %r1156,%r912,%r1145;
}

	
	{fma.rn.f16x2 %r1159,%r915,%r1143,%r1156;
}

	and.b32 %r1181, %r5, 7;
add.s32 %r99, %r71, %r1181;
barrier.sync 0;
shl.b32 %r1182, %r84, 3;
add.s32 %r1183, %r1182, %r99;
shl.b32 %r1184, %r1183, 2;
add.s32 %r100, %r718, %r1184;
st.shared.u32 [%r100], %r870;
st.shared.u32 [%r100+32], %r930;
st.shared.u32 [%r100+64], %r967;
st.shared.u32 [%r100+96], %r1004;
st.shared.u32 [%r100+128], %r1041;
st.shared.u32 [%r100+160], %r1078;
st.shared.u32 [%r100+192], %r1115;
st.shared.u32 [%r100+224], %r1152;
barrier.sync 0;
add.s32 %r1186, %r84, %r99;
shl.b32 %r1187, %r1186, 2;
add.s32 %r101, %r718, %r1187;
ld.shared.u32 %r102, [%r101];
ld.shared.u32 %r103, [%r101+4096];
ld.shared.u32 %r104, [%r101+8192];
ld.shared.u32 %r105, [%r101+12288];
ld.shared.u32 %r106, [%r101+16384];
ld.shared.u32 %r107, [%r101+20480];
ld.shared.u32 %r108, [%r101+24576];
ld.shared.u32 %r109, [%r101+28672];
barrier.sync 0;
st.shared.u32 [%r100], %r873;
st.shared.u32 [%r100+32], %r937;
st.shared.u32 [%r100+64], %r974;
st.shared.u32 [%r100+96], %r1011;
st.shared.u32 [%r100+128], %r1048;
st.shared.u32 [%r100+160], %r1085;
st.shared.u32 [%r100+192], %r1122;
st.shared.u32 [%r100+224], %r1159;
barrier.sync 0;
ld.shared.u32 %r1193, [%r101];
ld.shared.u32 %r1243, [%r101+4096];
ld.shared.u32 %r1205, [%r101+8192];
ld.shared.u32 %r1255, [%r101+12288];
ld.shared.u32 %r1194, [%r101+16384];
ld.shared.u32 %r1244, [%r101+20480];
ld.shared.u32 %r1206, [%r101+24576];
ld.shared.u32 %r1256, [%r101+28672];

	{add.f16x2 %r1189,%r102,%r106;
}

	
	{add.f16x2 %r1192,%r1193,%r1194;
}

	
	{sub.f16x2 %r1195,%r102,%r106;
}

	
	{sub.f16x2 %r1198,%r1193,%r1194;
}

	
	{add.f16x2 %r1201,%r104,%r108;
}

	
	{add.f16x2 %r1204,%r1205,%r1206;
}

	
	{sub.f16x2 %r1207,%r104,%r108;
}

	
	{sub.f16x2 %r1210,%r1205,%r1206;
}

	
	{xor.b32 %r1213,%r1207,0x80008000;
}

	
	{add.f16x2 %r1215,%r1189,%r1201;
}

	
	{add.f16x2 %r1218,%r1192,%r1204;
}

	
	{sub.f16x2 %r1221,%r1189,%r1201;
}

	
	{sub.f16x2 %r1224,%r1192,%r1204;
}

	
	{add.f16x2 %r1227,%r1195,%r1210;
}

	
	{add.f16x2 %r1230,%r1198,%r1213;
}

	
	{sub.f16x2 %r1233,%r1195,%r1210;
}

	
	{sub.f16x2 %r1236,%r1198,%r1213;
}

	
	{add.f16x2 %r1239,%r103,%r107;
}

	
	{add.f16x2 %r1242,%r1243,%r1244;
}

	
	{sub.f16x2 %r1245,%r103,%r107;
}

	
	{sub.f16x2 %r1248,%r1243,%r1244;
}

	
	{add.f16x2 %r1251,%r105,%r109;
}

	
	{add.f16x2 %r1254,%r1255,%r1256;
}

	
	{sub.f16x2 %r1257,%r105,%r109;
}

	
	{sub.f16x2 %r1260,%r1255,%r1256;
}

	
	{xor.b32 %r1263,%r1257,0x80008000;
}

	
	{add.f16x2 %r1265,%r1239,%r1251;
}

	
	{add.f16x2 %r1268,%r1242,%r1254;
}

	
	{sub.f16x2 %r1271,%r1239,%r1251;
}

	
	{sub.f16x2 %r1274,%r1242,%r1254;
}

	
	{add.f16x2 %r1277,%r1245,%r1260;
}

	
	{add.f16x2 %r1280,%r1248,%r1263;
}

	
	{sub.f16x2 %r1283,%r1245,%r1260;
}

	
	{sub.f16x2 %r1286,%r1248,%r1263;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r1289, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1290, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1293, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1294, {low,high};}


	
	{mul.f16x2 %r1303,%r1277,%r1289;
}

	
	{mul.f16x2 %r1306,%r1280,%r1290;
}

	
	{sub.f16x2 %r1309,%r1303,%r1306;
}

	
	{mul.f16x2 %r1312,%r1277,%r1290;
}

	
	{fma.rn.f16x2 %r1315,%r1280,%r1289,%r1312;
}

	
	{xor.b32 %r1319,%r1271,0x80008000;
}

	
	{mul.f16x2 %r1321,%r1283,%r1293;
}

	
	{mul.f16x2 %r1324,%r1286,%r1294;
}

	
	{sub.f16x2 %r1327,%r1321,%r1324;
}

	
	{mul.f16x2 %r1330,%r1283,%r1294;
}

	
	{fma.rn.f16x2 %r1333,%r1286,%r1293,%r1330;
}

	
	{add.f16x2 %r1337,%r1215,%r1265;
}

	
	{add.f16x2 %r1340,%r1218,%r1268;
}

	
	{sub.f16x2 %r1343,%r1215,%r1265;
}

	
	{sub.f16x2 %r1346,%r1218,%r1268;
}

	
	{add.f16x2 %r1349,%r1227,%r1309;
}

	
	{add.f16x2 %r1352,%r1230,%r1315;
}

	
	{sub.f16x2 %r1355,%r1227,%r1309;
}

	
	{sub.f16x2 %r1358,%r1230,%r1315;
}

	
	{add.f16x2 %r1361,%r1221,%r1274;
}

	
	{add.f16x2 %r1364,%r1224,%r1319;
}

	
	{sub.f16x2 %r1367,%r1221,%r1274;
}

	
	{sub.f16x2 %r1370,%r1224,%r1319;
}

	
	{add.f16x2 %r1373,%r1233,%r1327;
}

	
	{add.f16x2 %r1376,%r1236,%r1333;
}

	
	{sub.f16x2 %r1379,%r1233,%r1327;
}

	
	{sub.f16x2 %r1382,%r1236,%r1333;
}

	and.b32 %r112, %r5, 960;
bfe.u32 %r1647, %r5, 6, 4;
cvt.rn.f32.u32	%f146, %r1647;
mul.f32 %f147, %f146, 0f3D490FDB;
cos.approx.f32 %f128, %f147;
sin.approx.f32 %f148, %f147;
neg.f32 %f129, %f148;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f128;
cvt.rn.f16.f32 high, %f129;
mov.b32 %r1385, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1388, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1390, {high,high};}


	
	{mul.f16x2 %r1392,%r1352,%r1390;
}

	
	{xor.b32 %r1395,%r1392,0x80008000;
}

	
	{fma.rn.f16x2 %r1397,%r1349,%r1388,%r1395;
}

	
	{mul.f16x2 %r1401,%r1349,%r1390;
}

	
	{fma.rn.f16x2 %r1404,%r1352,%r1388,%r1401;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1408, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1410, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1412, {low,high};}


	
	{mul.f16x2 %r1413,%r1410,%r1412;
}

	
	{mul.f16x2 %r1416,%r1385,%r1408;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1419, {high,low};}


	
	{fma.rn.f16x2 %r1421,%r1413,%r1419,%r1416;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1421;
mov.b32 %r1425, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1421;
mov.b32 %r1427, {high,high};}


	
	{mul.f16x2 %r1429,%r1364,%r1427;
}

	
	{xor.b32 %r1432,%r1429,0x80008000;
}

	
	{fma.rn.f16x2 %r1434,%r1361,%r1425,%r1432;
}

	
	{mul.f16x2 %r1438,%r1361,%r1427;
}

	
	{fma.rn.f16x2 %r1441,%r1364,%r1425,%r1438;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1445, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1447, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1449, {low,high};}


	
	{mul.f16x2 %r1450,%r1447,%r1449;
}

	
	{mul.f16x2 %r1453,%r1421,%r1445;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1421;
mov.b32 %r1456, {high,low};}


	
	{fma.rn.f16x2 %r1458,%r1450,%r1456,%r1453;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1458;
mov.b32 %r1462, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1458;
mov.b32 %r1464, {high,high};}


	
	{mul.f16x2 %r1466,%r1376,%r1464;
}

	
	{xor.b32 %r1469,%r1466,0x80008000;
}

	
	{fma.rn.f16x2 %r1471,%r1373,%r1462,%r1469;
}

	
	{mul.f16x2 %r1475,%r1373,%r1464;
}

	
	{fma.rn.f16x2 %r1478,%r1376,%r1462,%r1475;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1482, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1484, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1486, {low,high};}


	
	{mul.f16x2 %r1487,%r1484,%r1486;
}

	
	{mul.f16x2 %r1490,%r1458,%r1482;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1458;
mov.b32 %r1493, {high,low};}


	
	{fma.rn.f16x2 %r1495,%r1487,%r1493,%r1490;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1495;
mov.b32 %r1499, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1495;
mov.b32 %r1501, {high,high};}


	
	{mul.f16x2 %r1503,%r1346,%r1501;
}

	
	{xor.b32 %r1506,%r1503,0x80008000;
}

	
	{fma.rn.f16x2 %r1508,%r1343,%r1499,%r1506;
}

	
	{mul.f16x2 %r1512,%r1343,%r1501;
}

	
	{fma.rn.f16x2 %r1515,%r1346,%r1499,%r1512;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1519, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1521, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1523, {low,high};}


	
	{mul.f16x2 %r1524,%r1521,%r1523;
}

	
	{mul.f16x2 %r1527,%r1495,%r1519;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1495;
mov.b32 %r1530, {high,low};}


	
	{fma.rn.f16x2 %r1532,%r1524,%r1530,%r1527;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1532;
mov.b32 %r1536, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1532;
mov.b32 %r1538, {high,high};}


	
	{mul.f16x2 %r1540,%r1358,%r1538;
}

	
	{xor.b32 %r1543,%r1540,0x80008000;
}

	
	{fma.rn.f16x2 %r1545,%r1355,%r1536,%r1543;
}

	
	{mul.f16x2 %r1549,%r1355,%r1538;
}

	
	{fma.rn.f16x2 %r1552,%r1358,%r1536,%r1549;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1556, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1558, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1560, {low,high};}


	
	{mul.f16x2 %r1561,%r1558,%r1560;
}

	
	{mul.f16x2 %r1564,%r1532,%r1556;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1532;
mov.b32 %r1567, {high,low};}


	
	{fma.rn.f16x2 %r1569,%r1561,%r1567,%r1564;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1569;
mov.b32 %r1573, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1569;
mov.b32 %r1575, {high,high};}


	
	{mul.f16x2 %r1577,%r1370,%r1575;
}

	
	{xor.b32 %r1580,%r1577,0x80008000;
}

	
	{fma.rn.f16x2 %r1582,%r1367,%r1573,%r1580;
}

	
	{mul.f16x2 %r1586,%r1367,%r1575;
}

	
	{fma.rn.f16x2 %r1589,%r1370,%r1573,%r1586;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1593, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1595, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1597, {low,high};}


	
	{mul.f16x2 %r1598,%r1595,%r1597;
}

	
	{mul.f16x2 %r1601,%r1569,%r1593;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1569;
mov.b32 %r1604, {high,low};}


	
	{fma.rn.f16x2 %r1606,%r1598,%r1604,%r1601;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1606;
mov.b32 %r1610, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1606;
mov.b32 %r1612, {high,high};}


	
	{mul.f16x2 %r1614,%r1382,%r1612;
}

	
	{xor.b32 %r1617,%r1614,0x80008000;
}

	
	{fma.rn.f16x2 %r1619,%r1379,%r1610,%r1617;
}

	
	{mul.f16x2 %r1623,%r1379,%r1612;
}

	
	{fma.rn.f16x2 %r1626,%r1382,%r1610,%r1623;
}

	and.b32 %r1648, %r5, 63;
add.s32 %r127, %r71, %r1648;
barrier.sync 0;
shl.b32 %r1649, %r112, 3;
add.s32 %r1650, %r1649, %r127;
shl.b32 %r1651, %r1650, 2;
add.s32 %r128, %r718, %r1651;
st.shared.u32 [%r128], %r1337;
st.shared.u32 [%r128+256], %r1397;
st.shared.u32 [%r128+512], %r1434;
st.shared.u32 [%r128+768], %r1471;
st.shared.u32 [%r128+1024], %r1508;
st.shared.u32 [%r128+1280], %r1545;
st.shared.u32 [%r128+1536], %r1582;
st.shared.u32 [%r128+1792], %r1619;
barrier.sync 0;
add.s32 %r1653, %r112, %r127;
shl.b32 %r1654, %r1653, 2;
add.s32 %r129, %r718, %r1654;
ld.shared.u32 %r130, [%r129];
ld.shared.u32 %r131, [%r129+4096];
ld.shared.u32 %r132, [%r129+8192];
ld.shared.u32 %r133, [%r129+12288];
ld.shared.u32 %r134, [%r129+16384];
ld.shared.u32 %r135, [%r129+20480];
ld.shared.u32 %r136, [%r129+24576];
ld.shared.u32 %r137, [%r129+28672];
barrier.sync 0;
st.shared.u32 [%r128], %r1340;
st.shared.u32 [%r128+256], %r1404;
st.shared.u32 [%r128+512], %r1441;
st.shared.u32 [%r128+768], %r1478;
st.shared.u32 [%r128+1024], %r1515;
st.shared.u32 [%r128+1280], %r1552;
st.shared.u32 [%r128+1536], %r1589;
st.shared.u32 [%r128+1792], %r1626;
barrier.sync 0;
ld.shared.u32 %r1660, [%r129];
ld.shared.u32 %r1710, [%r129+4096];
ld.shared.u32 %r1672, [%r129+8192];
ld.shared.u32 %r1722, [%r129+12288];
ld.shared.u32 %r1661, [%r129+16384];
ld.shared.u32 %r1711, [%r129+20480];
ld.shared.u32 %r1673, [%r129+24576];
ld.shared.u32 %r1723, [%r129+28672];

	{add.f16x2 %r1656,%r130,%r134;
}

	
	{add.f16x2 %r1659,%r1660,%r1661;
}

	
	{sub.f16x2 %r1662,%r130,%r134;
}

	
	{sub.f16x2 %r1665,%r1660,%r1661;
}

	
	{add.f16x2 %r1668,%r132,%r136;
}

	
	{add.f16x2 %r1671,%r1672,%r1673;
}

	
	{sub.f16x2 %r1674,%r132,%r136;
}

	
	{sub.f16x2 %r1677,%r1672,%r1673;
}

	
	{xor.b32 %r1680,%r1674,0x80008000;
}

	
	{add.f16x2 %r1682,%r1656,%r1668;
}

	
	{add.f16x2 %r1685,%r1659,%r1671;
}

	
	{sub.f16x2 %r1688,%r1656,%r1668;
}

	
	{sub.f16x2 %r1691,%r1659,%r1671;
}

	
	{add.f16x2 %r1694,%r1662,%r1677;
}

	
	{add.f16x2 %r1697,%r1665,%r1680;
}

	
	{sub.f16x2 %r1700,%r1662,%r1677;
}

	
	{sub.f16x2 %r1703,%r1665,%r1680;
}

	
	{add.f16x2 %r1706,%r131,%r135;
}

	
	{add.f16x2 %r1709,%r1710,%r1711;
}

	
	{sub.f16x2 %r1712,%r131,%r135;
}

	
	{sub.f16x2 %r1715,%r1710,%r1711;
}

	
	{add.f16x2 %r1718,%r133,%r137;
}

	
	{add.f16x2 %r1721,%r1722,%r1723;
}

	
	{sub.f16x2 %r1724,%r133,%r137;
}

	
	{sub.f16x2 %r1727,%r1722,%r1723;
}

	
	{xor.b32 %r1730,%r1724,0x80008000;
}

	
	{add.f16x2 %r1732,%r1706,%r1718;
}

	
	{add.f16x2 %r1735,%r1709,%r1721;
}

	
	{sub.f16x2 %r1738,%r1706,%r1718;
}

	
	{sub.f16x2 %r1741,%r1709,%r1721;
}

	
	{add.f16x2 %r1744,%r1712,%r1727;
}

	
	{add.f16x2 %r1747,%r1715,%r1730;
}

	
	{sub.f16x2 %r1750,%r1712,%r1727;
}

	
	{sub.f16x2 %r1753,%r1715,%r1730;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r1756, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1757, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1760, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1761, {low,high};}


	
	{mul.f16x2 %r1770,%r1744,%r1756;
}

	
	{mul.f16x2 %r1773,%r1747,%r1757;
}

	
	{sub.f16x2 %r1776,%r1770,%r1773;
}

	
	{mul.f16x2 %r1779,%r1744,%r1757;
}

	
	{fma.rn.f16x2 %r1782,%r1747,%r1756,%r1779;
}

	
	{xor.b32 %r1786,%r1738,0x80008000;
}

	
	{mul.f16x2 %r1788,%r1750,%r1760;
}

	
	{mul.f16x2 %r1791,%r1753,%r1761;
}

	
	{sub.f16x2 %r1794,%r1788,%r1791;
}

	
	{mul.f16x2 %r1797,%r1750,%r1761;
}

	
	{fma.rn.f16x2 %r1800,%r1753,%r1760,%r1797;
}

	
	{add.f16x2 %r1804,%r1682,%r1732;
}

	
	{add.f16x2 %r1807,%r1685,%r1735;
}

	
	{sub.f16x2 %r1810,%r1682,%r1732;
}

	
	{sub.f16x2 %r1813,%r1685,%r1735;
}

	
	{add.f16x2 %r1816,%r1694,%r1776;
}

	
	{add.f16x2 %r1819,%r1697,%r1782;
}

	
	{sub.f16x2 %r1822,%r1694,%r1776;
}

	
	{sub.f16x2 %r1825,%r1697,%r1782;
}

	
	{add.f16x2 %r1828,%r1688,%r1741;
}

	
	{add.f16x2 %r1831,%r1691,%r1786;
}

	
	{sub.f16x2 %r1834,%r1688,%r1741;
}

	
	{sub.f16x2 %r1837,%r1691,%r1786;
}

	
	{add.f16x2 %r1840,%r1700,%r1794;
}

	
	{add.f16x2 %r1843,%r1703,%r1800;
}

	
	{sub.f16x2 %r1846,%r1700,%r1794;
}

	
	{sub.f16x2 %r1849,%r1703,%r1800;
}

	and.b32 %r140, %r5, 512;
bfe.u32 %r2114, %r5, 9, 1;
cvt.rn.f32.u32	%f195, %r2114;
mul.f32 %f196, %f195, 0f3EC90FDB;
cos.approx.f32 %f177, %f196;
sin.approx.f32 %f197, %f196;
neg.f32 %f178, %f197;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f177;
cvt.rn.f16.f32 high, %f178;
mov.b32 %r1852, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1855, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1857, {high,high};}


	
	{mul.f16x2 %r1859,%r1819,%r1857;
}

	
	{xor.b32 %r1862,%r1859,0x80008000;
}

	
	{fma.rn.f16x2 %r1864,%r1816,%r1855,%r1862;
}

	
	{mul.f16x2 %r1868,%r1816,%r1857;
}

	
	{fma.rn.f16x2 %r1871,%r1819,%r1855,%r1868;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1875, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1877, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1879, {low,high};}


	
	{mul.f16x2 %r1880,%r1877,%r1879;
}

	
	{mul.f16x2 %r1883,%r1852,%r1875;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1886, {high,low};}


	
	{fma.rn.f16x2 %r1888,%r1880,%r1886,%r1883;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1888;
mov.b32 %r1892, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1888;
mov.b32 %r1894, {high,high};}


	
	{mul.f16x2 %r1896,%r1831,%r1894;
}

	
	{xor.b32 %r1899,%r1896,0x80008000;
}

	
	{fma.rn.f16x2 %r1901,%r1828,%r1892,%r1899;
}

	
	{mul.f16x2 %r1905,%r1828,%r1894;
}

	
	{fma.rn.f16x2 %r1908,%r1831,%r1892,%r1905;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1912, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1914, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1916, {low,high};}


	
	{mul.f16x2 %r1917,%r1914,%r1916;
}

	
	{mul.f16x2 %r1920,%r1888,%r1912;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1888;
mov.b32 %r1923, {high,low};}


	
	{fma.rn.f16x2 %r1925,%r1917,%r1923,%r1920;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1925;
mov.b32 %r1929, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1925;
mov.b32 %r1931, {high,high};}


	
	{mul.f16x2 %r1933,%r1843,%r1931;
}

	
	{xor.b32 %r1936,%r1933,0x80008000;
}

	
	{fma.rn.f16x2 %r1938,%r1840,%r1929,%r1936;
}

	
	{mul.f16x2 %r1942,%r1840,%r1931;
}

	
	{fma.rn.f16x2 %r1945,%r1843,%r1929,%r1942;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1949, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1951, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1953, {low,high};}


	
	{mul.f16x2 %r1954,%r1951,%r1953;
}

	
	{mul.f16x2 %r1957,%r1925,%r1949;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1925;
mov.b32 %r1960, {high,low};}


	
	{fma.rn.f16x2 %r1962,%r1954,%r1960,%r1957;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1962;
mov.b32 %r1966, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1962;
mov.b32 %r1968, {high,high};}


	
	{mul.f16x2 %r1970,%r1813,%r1968;
}

	
	{xor.b32 %r1973,%r1970,0x80008000;
}

	
	{fma.rn.f16x2 %r1975,%r1810,%r1966,%r1973;
}

	
	{mul.f16x2 %r1979,%r1810,%r1968;
}

	
	{fma.rn.f16x2 %r1982,%r1813,%r1966,%r1979;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1986, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1988, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1990, {low,high};}


	
	{mul.f16x2 %r1991,%r1988,%r1990;
}

	
	{mul.f16x2 %r1994,%r1962,%r1986;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1962;
mov.b32 %r1997, {high,low};}


	
	{fma.rn.f16x2 %r1999,%r1991,%r1997,%r1994;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1999;
mov.b32 %r2003, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1999;
mov.b32 %r2005, {high,high};}


	
	{mul.f16x2 %r2007,%r1825,%r2005;
}

	
	{xor.b32 %r2010,%r2007,0x80008000;
}

	
	{fma.rn.f16x2 %r2012,%r1822,%r2003,%r2010;
}

	
	{mul.f16x2 %r2016,%r1822,%r2005;
}

	
	{fma.rn.f16x2 %r2019,%r1825,%r2003,%r2016;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r2023, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r2025, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r2027, {low,high};}


	
	{mul.f16x2 %r2028,%r2025,%r2027;
}

	
	{mul.f16x2 %r2031,%r1999,%r2023;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1999;
mov.b32 %r2034, {high,low};}


	
	{fma.rn.f16x2 %r2036,%r2028,%r2034,%r2031;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2036;
mov.b32 %r2040, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2036;
mov.b32 %r2042, {high,high};}


	
	{mul.f16x2 %r2044,%r1837,%r2042;
}

	
	{xor.b32 %r2047,%r2044,0x80008000;
}

	
	{fma.rn.f16x2 %r2049,%r1834,%r2040,%r2047;
}

	
	{mul.f16x2 %r2053,%r1834,%r2042;
}

	
	{fma.rn.f16x2 %r2056,%r1837,%r2040,%r2053;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r2060, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r2062, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r2064, {low,high};}


	
	{mul.f16x2 %r2065,%r2062,%r2064;
}

	
	{mul.f16x2 %r2068,%r2036,%r2060;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2036;
mov.b32 %r2071, {high,low};}


	
	{fma.rn.f16x2 %r2073,%r2065,%r2071,%r2068;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2073;
mov.b32 %r2077, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2073;
mov.b32 %r2079, {high,high};}


	
	{mul.f16x2 %r2081,%r1849,%r2079;
}

	
	{xor.b32 %r2084,%r2081,0x80008000;
}

	
	{fma.rn.f16x2 %r2086,%r1846,%r2077,%r2084;
}

	
	{mul.f16x2 %r2090,%r1846,%r2079;
}

	
	{fma.rn.f16x2 %r2093,%r1849,%r2077,%r2090;
}

	and.b32 %r2115, %r5, 511;
add.s32 %r155, %r71, %r2115;
barrier.sync 0;
shl.b32 %r2116, %r140, 3;
add.s32 %r2117, %r2116, %r155;
shl.b32 %r2118, %r2117, 2;
add.s32 %r156, %r718, %r2118;
st.shared.u32 [%r156], %r1804;
st.shared.u32 [%r156+2048], %r1864;
st.shared.u32 [%r156+4096], %r1901;
st.shared.u32 [%r156+6144], %r1938;
st.shared.u32 [%r156+8192], %r1975;
st.shared.u32 [%r156+10240], %r2012;
st.shared.u32 [%r156+12288], %r2049;
st.shared.u32 [%r156+14336], %r2086;
barrier.sync 0;
add.s32 %r2120, %r140, %r155;
shl.b32 %r2121, %r2120, 2;
add.s32 %r157, %r718, %r2121;
ld.shared.u32 %r158, [%r157];
ld.shared.u32 %r159, [%r157+4096];
ld.shared.u32 %r160, [%r157+8192];
ld.shared.u32 %r161, [%r157+12288];
ld.shared.u32 %r162, [%r157+16384];
ld.shared.u32 %r163, [%r157+20480];
ld.shared.u32 %r164, [%r157+24576];
ld.shared.u32 %r165, [%r157+28672];
barrier.sync 0;
st.shared.u32 [%r156], %r1807;
st.shared.u32 [%r156+2048], %r1871;
st.shared.u32 [%r156+4096], %r1908;
st.shared.u32 [%r156+6144], %r1945;
st.shared.u32 [%r156+8192], %r1982;
st.shared.u32 [%r156+10240], %r2019;
st.shared.u32 [%r156+12288], %r2056;
st.shared.u32 [%r156+14336], %r2093;
barrier.sync 0;
ld.shared.u32 %r2127, [%r157];
ld.shared.u32 %r2139, [%r157+4096];
ld.shared.u32 %r2151, [%r157+8192];
ld.shared.u32 %r2163, [%r157+12288];
ld.shared.u32 %r2128, [%r157+16384];
ld.shared.u32 %r2140, [%r157+20480];
ld.shared.u32 %r2152, [%r157+24576];
ld.shared.u32 %r2164, [%r157+28672];

	{add.f16x2 %r2123,%r158,%r162;
}

	
	{add.f16x2 %r2126,%r2127,%r2128;
}

	
	{sub.f16x2 %r2129,%r158,%r162;
}

	
	{sub.f16x2 %r2132,%r2127,%r2128;
}

	
	{add.f16x2 %r2135,%r159,%r163;
}

	
	{add.f16x2 %r2138,%r2139,%r2140;
}

	
	{sub.f16x2 %r2141,%r159,%r163;
}

	
	{sub.f16x2 %r2144,%r2139,%r2140;
}

	
	{add.f16x2 %r2147,%r160,%r164;
}

	
	{add.f16x2 %r2150,%r2151,%r2152;
}

	
	{sub.f16x2 %r2153,%r160,%r164;
}

	
	{sub.f16x2 %r2156,%r2151,%r2152;
}

	
	{add.f16x2 %r2159,%r161,%r165;
}

	
	{add.f16x2 %r2162,%r2163,%r2164;
}

	
	{sub.f16x2 %r2165,%r161,%r165;
}

	
	{sub.f16x2 %r2168,%r2163,%r2164;
}

	selp.b32	%r2218, %r2165, %r2168, %p1;
selp.b32	%r2217, %r2168, %r2165, %p1;
selp.b32	%r2212, %r2153, %r2156, %p1;
selp.b32	%r2211, %r2156, %r2153, %p1;
selp.b32	%r2206, %r2141, %r2144, %p1;
selp.b32	%r2205, %r2144, %r2141, %p1;
selp.b32	%r2200, %r2129, %r2132, %p1;
selp.b32	%r2199, %r2132, %r2129, %p1;
selp.b32	%r2194, %r2159, %r2162, %p1;
selp.b32	%r2193, %r2162, %r2159, %p1;
selp.b32	%r2188, %r2147, %r2150, %p1;
selp.b32	%r2187, %r2150, %r2147, %p1;
selp.b32	%r2182, %r2135, %r2138, %p1;
selp.b32	%r2181, %r2138, %r2135, %p1;
selp.b32	%r2176, %r2123, %r2126, %p1;
selp.b32	%r2175, %r2126, %r2123, %p1;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2175;
mov.b32 {blow,bhigh}, %r2176;
mov.b32 %r2171, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2175;
mov.b32 {blow,bhigh}, %r2176;
mov.b32 %r2174, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2181;
mov.b32 {blow,bhigh}, %r2182;
mov.b32 %r2177, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2181;
mov.b32 {blow,bhigh}, %r2182;
mov.b32 %r2180, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2187;
mov.b32 {blow,bhigh}, %r2188;
mov.b32 %r2183, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2187;
mov.b32 {blow,bhigh}, %r2188;
mov.b32 %r2186, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2193;
mov.b32 {blow,bhigh}, %r2194;
mov.b32 %r2189, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2193;
mov.b32 {blow,bhigh}, %r2194;
mov.b32 %r2192, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2199;
mov.b32 {blow,bhigh}, %r2200;
mov.b32 %r2195, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2199;
mov.b32 {blow,bhigh}, %r2200;
mov.b32 %r2198, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2205;
mov.b32 {blow,bhigh}, %r2206;
mov.b32 %r2201, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2205;
mov.b32 {blow,bhigh}, %r2206;
mov.b32 %r2204, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2211;
mov.b32 {blow,bhigh}, %r2212;
mov.b32 %r2207, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2211;
mov.b32 {blow,bhigh}, %r2212;
mov.b32 %r2210, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2217;
mov.b32 {blow,bhigh}, %r2218;
mov.b32 %r2213, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2217;
mov.b32 {blow,bhigh}, %r2218;
mov.b32 %r2216, {ahigh,bhigh};}


	cvta.to.global.u64 %rd8, %rd5;
shl.b64 %rd9, %rd1, 2;
add.s64 %rd3, %rd8, %rd9;
@%p2 bra BB12_6;
bra.uni BB12_5;

BB12_6:
st.global.u32 [%rd3], %r2171;
st.global.u32 [%rd3+4096], %r2177;
st.global.u32 [%rd3+8192], %r2183;
st.global.u32 [%rd3+12288], %r2189;
st.global.u32 [%rd3+16384], %r2195;
st.global.u32 [%rd3+20480], %r2201;
st.global.u32 [%rd3+24576], %r2207;
st.global.u32 [%rd3+28672], %r2213;
bra.uni BB12_7;

BB12_5:
shl.b32 %r2220, %r3, 1;
add.s32 %r2221, %r2220, -1;
st.global.u32 [%rd3], %r2171;
st.global.u32 [%rd3+4096], %r2177;
st.global.u32 [%rd3+8192], %r2183;
st.global.u32 [%rd3+12288], %r2189;
st.global.u32 [%rd3+16384], %r2195;
st.global.u32 [%rd3+20480], %r2201;
st.global.u32 [%rd3+24576], %r2207;
st.global.u32 [%rd3+28672], %r2213;
setp.ge.u32	%p5, %r2221, %r1;
@%p5 bra BB12_8;

BB12_7:
st.global.u32 [%rd3+32768], %r2174;
st.global.u32 [%rd3+36864], %r2180;
st.global.u32 [%rd3+40960], %r2186;
st.global.u32 [%rd3+45056], %r2192;
st.global.u32 [%rd3+49152], %r2198;
st.global.u32 [%rd3+53248], %r2204;
st.global.u32 [%rd3+57344], %r2210;
st.global.u32 [%rd3+61440], %r2216;

BB12_8:
ret;
}


