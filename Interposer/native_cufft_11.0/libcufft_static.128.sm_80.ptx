







.version 7.0
.target sm_80
.address_size 64


.func (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
.param .b64 __internal_trig_reduction_slowpathd_param_0,
.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry _Z6calc__P7double2jjd(
.param .u64 _Z6calc__P7double2jjd_param_0,
.param .u32 _Z6calc__P7double2jjd_param_1,
.param .u32 _Z6calc__P7double2jjd_param_2,
.param .f64 _Z6calc__P7double2jjd_param_3
)
{
.local .align 4 .b8 __local_depot0[8];
.reg .b64 %SP;
.reg .b64 %SPL;
.reg .pred %p<14>;
.reg .b32 %r<38>;
.reg .f64 %fd<84>;
.reg .b64 %rd<25>;


mov.u64 %SPL, __local_depot0;
cvta.local.u64 %SP, %SPL;
ld.param.u64 %rd4, [_Z6calc__P7double2jjd_param_0];
ld.param.u32 %r15, [_Z6calc__P7double2jjd_param_1];
ld.param.u32 %r17, [_Z6calc__P7double2jjd_param_2];
ld.param.f64 %fd26, [_Z6calc__P7double2jjd_param_3];
cvta.to.global.u64 %rd5, %rd4;
mov.u32 %r18, %tid.x;
shl.b32 %r34, %r18, 3;
mul.wide.s32 %rd6, %r34, 16;
add.s64 %rd24, %rd5, %rd6;
sub.s32 %r2, %r17, %r34;
mov.u32 %r35, 0;

BB0_1:
setp.ge.u32	%p1, %r34, %r15;
@%p1 bra BB0_23;

mul.lo.s32 %r19, %r34, %r34;
cvt.rn.f64.s32	%fd27, %r19;
mul.f64 %fd80, %fd27, %fd26;
{
.reg .b32 %temp; 
mov.b64 {%temp, %r20}, %fd80;
}
and.b32 %r5, %r20, 2147483647;
setp.ne.s32	%p2, %r5, 2146435072;
mov.f64 %fd76, %fd80;
@%p2 bra BB0_5;

{
.reg .b32 %temp; 
mov.b64 {%r21, %temp}, %fd80;
}
setp.ne.s32	%p3, %r21, 0;
mov.f64 %fd76, %fd80;
@%p3 bra BB0_5;

mov.f64 %fd28, 0d0000000000000000;
mul.rn.f64 %fd76, %fd80, %fd28;

BB0_5:
mul.f64 %fd29, %fd76, 0d3FE45F306DC9C883;
cvt.rni.s32.f64	%r36, %fd29;
add.u64 %rd7, %SP, 4;
add.u64 %rd8, %SPL, 4;
st.local.u32 [%rd8], %r36;
cvt.rn.f64.s32	%fd30, %r36;
neg.f64 %fd31, %fd30;
mov.f64 %fd32, 0d3FF921FB54442D18;
fma.rn.f64 %fd33, %fd31, %fd32, %fd76;
mov.f64 %fd34, 0d3C91A62633145C00;
fma.rn.f64 %fd35, %fd31, %fd34, %fd33;
mov.f64 %fd36, 0d397B839A252049C0;
fma.rn.f64 %fd77, %fd31, %fd36, %fd35;
{
.reg .b32 %temp; 
mov.b64 {%temp, %r22}, %fd76;
}
and.b32 %r23, %r22, 2145386496;
setp.lt.u32	%p4, %r23, 1105199104;
@%p4 bra BB0_7;


	{
.reg .b32 temp_param_reg;

	.param .b64 param0;
st.param.f64	[param0+0], %fd76;
.param .b64 param1;
st.param.b64	[param1+0], %rd7;
.param .b64 retval0;
call.uni (retval0), 
__internal_trig_reduction_slowpathd, 
(
param0, 
param1
);
ld.param.f64	%fd77, [retval0+0];


	}
	ld.local.u32 %r36, [%rd8];

BB0_7:
add.s32 %r9, %r36, 1;
and.b32 %r24, %r9, 1;
shl.b32 %r25, %r24, 3;
setp.eq.s32	%p5, %r24, 0;
selp.f64	%fd37, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
mul.wide.u32 %rd11, %r25, 8;
mov.u64 %rd12, __cudart_sin_cos_coeffs;
add.s64 %rd13, %rd11, %rd12;
ld.const.f64 %fd38, [%rd13+8];
mul.rn.f64 %fd7, %fd77, %fd77;
fma.rn.f64 %fd39, %fd37, %fd7, %fd38;
ld.const.f64 %fd40, [%rd13+16];
fma.rn.f64 %fd41, %fd39, %fd7, %fd40;
ld.const.f64 %fd42, [%rd13+24];
fma.rn.f64 %fd43, %fd41, %fd7, %fd42;
ld.const.f64 %fd44, [%rd13+32];
fma.rn.f64 %fd45, %fd43, %fd7, %fd44;
ld.const.f64 %fd46, [%rd13+40];
fma.rn.f64 %fd47, %fd45, %fd7, %fd46;
ld.const.f64 %fd48, [%rd13+48];
fma.rn.f64 %fd8, %fd47, %fd7, %fd48;
fma.rn.f64 %fd78, %fd8, %fd77, %fd77;
@%p5 bra BB0_9;

mov.f64 %fd49, 0d3FF0000000000000;
fma.rn.f64 %fd78, %fd8, %fd7, %fd49;

BB0_9:
and.b32 %r26, %r9, 2;
setp.eq.s32	%p6, %r26, 0;
@%p6 bra BB0_11;

mov.f64 %fd50, 0d0000000000000000;
mov.f64 %fd51, 0dBFF0000000000000;
fma.rn.f64 %fd78, %fd78, %fd51, %fd50;

BB0_11:
@%p2 bra BB0_14;

{
.reg .b32 %temp; 
mov.b64 {%r27, %temp}, %fd80;
}
setp.ne.s32	%p8, %r27, 0;
@%p8 bra BB0_14;

mov.f64 %fd52, 0d0000000000000000;
mul.rn.f64 %fd80, %fd80, %fd52;

BB0_14:
mul.f64 %fd53, %fd80, 0d3FE45F306DC9C883;
cvt.rni.s32.f64	%r37, %fd53;
add.u64 %rd14, %SP, 0;
add.u64 %rd15, %SPL, 0;
st.local.u32 [%rd15], %r37;
cvt.rn.f64.s32	%fd54, %r37;
neg.f64 %fd55, %fd54;
fma.rn.f64 %fd57, %fd55, %fd32, %fd80;
fma.rn.f64 %fd59, %fd55, %fd34, %fd57;
fma.rn.f64 %fd81, %fd55, %fd36, %fd59;
{
.reg .b32 %temp; 
mov.b64 {%temp, %r28}, %fd80;
}
and.b32 %r29, %r28, 2145386496;
setp.lt.u32	%p9, %r29, 1105199104;
@%p9 bra BB0_16;


	{
.reg .b32 temp_param_reg;

	.param .b64 param0;
st.param.f64	[param0+0], %fd80;
.param .b64 param1;
st.param.b64	[param1+0], %rd14;
.param .b64 retval0;
call.uni (retval0), 
__internal_trig_reduction_slowpathd, 
(
param0, 
param1
);
ld.param.f64	%fd81, [retval0+0];


	}
	ld.local.u32 %r37, [%rd15];

BB0_16:
and.b32 %r30, %r37, 1;
shl.b32 %r31, %r30, 3;
setp.eq.s32	%p10, %r30, 0;
selp.f64	%fd61, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p10;
mul.wide.u32 %rd18, %r31, 8;
add.s64 %rd20, %rd18, %rd12;
ld.const.f64 %fd62, [%rd20+8];
mul.rn.f64 %fd19, %fd81, %fd81;
fma.rn.f64 %fd63, %fd61, %fd19, %fd62;
ld.const.f64 %fd64, [%rd20+16];
fma.rn.f64 %fd65, %fd63, %fd19, %fd64;
ld.const.f64 %fd66, [%rd20+24];
fma.rn.f64 %fd67, %fd65, %fd19, %fd66;
ld.const.f64 %fd68, [%rd20+32];
fma.rn.f64 %fd69, %fd67, %fd19, %fd68;
ld.const.f64 %fd70, [%rd20+40];
fma.rn.f64 %fd71, %fd69, %fd19, %fd70;
ld.const.f64 %fd72, [%rd20+48];
fma.rn.f64 %fd20, %fd71, %fd19, %fd72;
fma.rn.f64 %fd82, %fd20, %fd81, %fd81;
@%p10 bra BB0_18;

mov.f64 %fd73, 0d3FF0000000000000;
fma.rn.f64 %fd82, %fd20, %fd19, %fd73;

BB0_18:
and.b32 %r32, %r37, 2;
setp.eq.s32	%p11, %r32, 0;
@%p11 bra BB0_20;

mov.f64 %fd74, 0d0000000000000000;
mov.f64 %fd75, 0dBFF0000000000000;
fma.rn.f64 %fd82, %fd82, %fd75, %fd74;

BB0_20:
st.global.v2.f64 [%rd24], {%fd78, %fd82};
setp.lt.s32	%p12, %r34, 1;
@%p12 bra BB0_22;

sub.s32 %r33, %r2, %r35;
mul.wide.u32 %rd22, %r33, 16;
add.s64 %rd23, %rd5, %rd22;
st.global.v2.f64 [%rd23], {%fd78, %fd82};

BB0_22:
add.s32 %r35, %r35, 1;
add.s64 %rd24, %rd24, 16;
add.s32 %r34, %r34, 1;
setp.lt.u32	%p13, %r35, 8;
@%p13 bra BB0_1;

BB0_23:
ret;
}


.visible .entry _Z14postprocessDSEILj16E7double2EvPT0_(
.param .u64 _Z14postprocessDSEILj16E7double2EvPT0__param_0
)
{
.reg .b32 %r<3>;
.reg .f64 %fd<33>;
.reg .b64 %rd<5>;


ld.param.u64 %rd1, [_Z14postprocessDSEILj16E7double2EvPT0__param_0];
cvta.to.global.u64 %rd2, %rd1;
mov.u32 %r1, %tid.x;
shl.b32 %r2, %r1, 4;
mul.wide.u32 %rd3, %r2, 16;
add.s64 %rd4, %rd2, %rd3;
ld.global.f64 %fd1, [%rd4+8];
neg.f64 %fd2, %fd1;
st.global.f64 [%rd4+8], %fd2;
ld.global.f64 %fd3, [%rd4+24];
neg.f64 %fd4, %fd3;
st.global.f64 [%rd4+24], %fd4;
ld.global.f64 %fd5, [%rd4+40];
neg.f64 %fd6, %fd5;
st.global.f64 [%rd4+40], %fd6;
ld.global.f64 %fd7, [%rd4+56];
neg.f64 %fd8, %fd7;
st.global.f64 [%rd4+56], %fd8;
ld.global.f64 %fd9, [%rd4+72];
neg.f64 %fd10, %fd9;
st.global.f64 [%rd4+72], %fd10;
ld.global.f64 %fd11, [%rd4+88];
neg.f64 %fd12, %fd11;
st.global.f64 [%rd4+88], %fd12;
ld.global.f64 %fd13, [%rd4+104];
neg.f64 %fd14, %fd13;
st.global.f64 [%rd4+104], %fd14;
ld.global.f64 %fd15, [%rd4+120];
neg.f64 %fd16, %fd15;
st.global.f64 [%rd4+120], %fd16;
ld.global.f64 %fd17, [%rd4+136];
neg.f64 %fd18, %fd17;
st.global.f64 [%rd4+136], %fd18;
ld.global.f64 %fd19, [%rd4+152];
neg.f64 %fd20, %fd19;
st.global.f64 [%rd4+152], %fd20;
ld.global.f64 %fd21, [%rd4+168];
neg.f64 %fd22, %fd21;
st.global.f64 [%rd4+168], %fd22;
ld.global.f64 %fd23, [%rd4+184];
neg.f64 %fd24, %fd23;
st.global.f64 [%rd4+184], %fd24;
ld.global.f64 %fd25, [%rd4+200];
neg.f64 %fd26, %fd25;
st.global.f64 [%rd4+200], %fd26;
ld.global.f64 %fd27, [%rd4+216];
neg.f64 %fd28, %fd27;
st.global.f64 [%rd4+216], %fd28;
ld.global.f64 %fd29, [%rd4+232];
neg.f64 %fd30, %fd29;
st.global.f64 [%rd4+232], %fd30;
ld.global.f64 %fd31, [%rd4+248];
neg.f64 %fd32, %fd31;
st.global.f64 [%rd4+248], %fd32;
ret;
}


.visible .entry _Z7convertILj0ELj16EEvPvS0_(
.param .u64 _Z7convertILj0ELj16EEvPvS0__param_0,
.param .u64 _Z7convertILj0ELj16EEvPvS0__param_1
)
{
.reg .f32 %f<33>;
.reg .b32 %r<3>;
.reg .f64 %fd<33>;
.reg .b64 %rd<9>;


ld.param.u64 %rd1, [_Z7convertILj0ELj16EEvPvS0__param_0];
ld.param.u64 %rd2, [_Z7convertILj0ELj16EEvPvS0__param_1];
cvta.to.global.u64 %rd3, %rd1;
mov.u32 %r1, %tid.x;
shl.b32 %r2, %r1, 4;
cvta.to.global.u64 %rd4, %rd2;
mul.wide.u32 %rd5, %r2, 16;
add.s64 %rd6, %rd3, %rd5;
ld.global.f64 %fd1, [%rd6];
cvt.rn.f32.f64	%f1, %fd1;
mul.wide.u32 %rd7, %r2, 8;
add.s64 %rd8, %rd4, %rd7;
st.global.f32 [%rd8], %f1;
ld.global.f64 %fd2, [%rd6+8];
cvt.rn.f32.f64	%f2, %fd2;
st.global.f32 [%rd8+4], %f2;
ld.global.f64 %fd3, [%rd6+16];
cvt.rn.f32.f64	%f3, %fd3;
st.global.f32 [%rd8+8], %f3;
ld.global.f64 %fd4, [%rd6+24];
cvt.rn.f32.f64	%f4, %fd4;
st.global.f32 [%rd8+12], %f4;
ld.global.f64 %fd5, [%rd6+32];
cvt.rn.f32.f64	%f5, %fd5;
st.global.f32 [%rd8+16], %f5;
ld.global.f64 %fd6, [%rd6+40];
cvt.rn.f32.f64	%f6, %fd6;
st.global.f32 [%rd8+20], %f6;
ld.global.f64 %fd7, [%rd6+48];
cvt.rn.f32.f64	%f7, %fd7;
st.global.f32 [%rd8+24], %f7;
ld.global.f64 %fd8, [%rd6+56];
cvt.rn.f32.f64	%f8, %fd8;
st.global.f32 [%rd8+28], %f8;
ld.global.f64 %fd9, [%rd6+64];
cvt.rn.f32.f64	%f9, %fd9;
st.global.f32 [%rd8+32], %f9;
ld.global.f64 %fd10, [%rd6+72];
cvt.rn.f32.f64	%f10, %fd10;
st.global.f32 [%rd8+36], %f10;
ld.global.f64 %fd11, [%rd6+80];
cvt.rn.f32.f64	%f11, %fd11;
st.global.f32 [%rd8+40], %f11;
ld.global.f64 %fd12, [%rd6+88];
cvt.rn.f32.f64	%f12, %fd12;
st.global.f32 [%rd8+44], %f12;
ld.global.f64 %fd13, [%rd6+96];
cvt.rn.f32.f64	%f13, %fd13;
st.global.f32 [%rd8+48], %f13;
ld.global.f64 %fd14, [%rd6+104];
cvt.rn.f32.f64	%f14, %fd14;
st.global.f32 [%rd8+52], %f14;
ld.global.f64 %fd15, [%rd6+112];
cvt.rn.f32.f64	%f15, %fd15;
st.global.f32 [%rd8+56], %f15;
ld.global.f64 %fd16, [%rd6+120];
cvt.rn.f32.f64	%f16, %fd16;
st.global.f32 [%rd8+60], %f16;
ld.global.f64 %fd17, [%rd6+128];
cvt.rn.f32.f64	%f17, %fd17;
st.global.f32 [%rd8+64], %f17;
ld.global.f64 %fd18, [%rd6+136];
cvt.rn.f32.f64	%f18, %fd18;
st.global.f32 [%rd8+68], %f18;
ld.global.f64 %fd19, [%rd6+144];
cvt.rn.f32.f64	%f19, %fd19;
st.global.f32 [%rd8+72], %f19;
ld.global.f64 %fd20, [%rd6+152];
cvt.rn.f32.f64	%f20, %fd20;
st.global.f32 [%rd8+76], %f20;
ld.global.f64 %fd21, [%rd6+160];
cvt.rn.f32.f64	%f21, %fd21;
st.global.f32 [%rd8+80], %f21;
ld.global.f64 %fd22, [%rd6+168];
cvt.rn.f32.f64	%f22, %fd22;
st.global.f32 [%rd8+84], %f22;
ld.global.f64 %fd23, [%rd6+176];
cvt.rn.f32.f64	%f23, %fd23;
st.global.f32 [%rd8+88], %f23;
ld.global.f64 %fd24, [%rd6+184];
cvt.rn.f32.f64	%f24, %fd24;
st.global.f32 [%rd8+92], %f24;
ld.global.f64 %fd25, [%rd6+192];
cvt.rn.f32.f64	%f25, %fd25;
st.global.f32 [%rd8+96], %f25;
ld.global.f64 %fd26, [%rd6+200];
cvt.rn.f32.f64	%f26, %fd26;
st.global.f32 [%rd8+100], %f26;
ld.global.f64 %fd27, [%rd6+208];
cvt.rn.f32.f64	%f27, %fd27;
st.global.f32 [%rd8+104], %f27;
ld.global.f64 %fd28, [%rd6+216];
cvt.rn.f32.f64	%f28, %fd28;
st.global.f32 [%rd8+108], %f28;
ld.global.f64 %fd29, [%rd6+224];
cvt.rn.f32.f64	%f29, %fd29;
st.global.f32 [%rd8+112], %f29;
ld.global.f64 %fd30, [%rd6+232];
cvt.rn.f32.f64	%f30, %fd30;
st.global.f32 [%rd8+116], %f30;
ld.global.f64 %fd31, [%rd6+240];
cvt.rn.f32.f64	%f31, %fd31;
st.global.f32 [%rd8+120], %f31;
ld.global.f64 %fd32, [%rd6+248];
cvt.rn.f32.f64	%f32, %fd32;
st.global.f32 [%rd8+124], %f32;
ret;
}


.visible .entry _Z7convertILj2ELj16EEvPvS0_(
.param .u64 _Z7convertILj2ELj16EEvPvS0__param_0,
.param .u64 _Z7convertILj2ELj16EEvPvS0__param_1
)
{
.reg .f32 %f<33>;
.reg .b32 %r<19>;
.reg .f64 %fd<65>;
.reg .b64 %rd<9>;


ld.param.u64 %rd1, [_Z7convertILj2ELj16EEvPvS0__param_0];
ld.param.u64 %rd2, [_Z7convertILj2ELj16EEvPvS0__param_1];
cvta.to.global.u64 %rd3, %rd1;
mov.u32 %r17, %tid.x;
shl.b32 %r18, %r17, 4;
cvta.to.global.u64 %rd4, %rd2;
mul.wide.u32 %rd5, %r18, 4;
add.s64 %rd6, %rd4, %rd5;
mul.wide.u32 %rd7, %r18, 16;
add.s64 %rd8, %rd3, %rd7;
ld.global.v2.f64 {%fd1, %fd2}, [%rd8];
cvt.rn.f32.f64	%f1, %fd1;
cvt.rn.f32.f64	%f2, %fd2;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f1;
cvt.rn.f16.f32 high, %f2;
mov.b32 %r1, {low,high};}


	st.global.u32 [%rd6], %r1;
ld.global.v2.f64 {%fd5, %fd6}, [%rd8+16];
cvt.rn.f32.f64	%f3, %fd5;
cvt.rn.f32.f64	%f4, %fd6;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f4;
mov.b32 %r2, {low,high};}


	st.global.u32 [%rd6+4], %r2;
ld.global.v2.f64 {%fd9, %fd10}, [%rd8+32];
cvt.rn.f32.f64	%f5, %fd9;
cvt.rn.f32.f64	%f6, %fd10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f5;
cvt.rn.f16.f32 high, %f6;
mov.b32 %r3, {low,high};}


	st.global.u32 [%rd6+8], %r3;
ld.global.v2.f64 {%fd13, %fd14}, [%rd8+48];
cvt.rn.f32.f64	%f7, %fd13;
cvt.rn.f32.f64	%f8, %fd14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f7;
cvt.rn.f16.f32 high, %f8;
mov.b32 %r4, {low,high};}


	st.global.u32 [%rd6+12], %r4;
ld.global.v2.f64 {%fd17, %fd18}, [%rd8+64];
cvt.rn.f32.f64	%f9, %fd17;
cvt.rn.f32.f64	%f10, %fd18;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f9;
cvt.rn.f16.f32 high, %f10;
mov.b32 %r5, {low,high};}


	st.global.u32 [%rd6+16], %r5;
ld.global.v2.f64 {%fd21, %fd22}, [%rd8+80];
cvt.rn.f32.f64	%f11, %fd21;
cvt.rn.f32.f64	%f12, %fd22;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r6, {low,high};}


	st.global.u32 [%rd6+20], %r6;
ld.global.v2.f64 {%fd25, %fd26}, [%rd8+96];
cvt.rn.f32.f64	%f13, %fd25;
cvt.rn.f32.f64	%f14, %fd26;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r7, {low,high};}


	st.global.u32 [%rd6+24], %r7;
ld.global.v2.f64 {%fd29, %fd30}, [%rd8+112];
cvt.rn.f32.f64	%f15, %fd29;
cvt.rn.f32.f64	%f16, %fd30;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r8, {low,high};}


	st.global.u32 [%rd6+28], %r8;
ld.global.v2.f64 {%fd33, %fd34}, [%rd8+128];
cvt.rn.f32.f64	%f17, %fd33;
cvt.rn.f32.f64	%f18, %fd34;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f17;
cvt.rn.f16.f32 high, %f18;
mov.b32 %r9, {low,high};}


	st.global.u32 [%rd6+32], %r9;
ld.global.v2.f64 {%fd37, %fd38}, [%rd8+144];
cvt.rn.f32.f64	%f19, %fd37;
cvt.rn.f32.f64	%f20, %fd38;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f19;
cvt.rn.f16.f32 high, %f20;
mov.b32 %r10, {low,high};}


	st.global.u32 [%rd6+36], %r10;
ld.global.v2.f64 {%fd41, %fd42}, [%rd8+160];
cvt.rn.f32.f64	%f21, %fd41;
cvt.rn.f32.f64	%f22, %fd42;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f21;
cvt.rn.f16.f32 high, %f22;
mov.b32 %r11, {low,high};}


	st.global.u32 [%rd6+40], %r11;
ld.global.v2.f64 {%fd45, %fd46}, [%rd8+176];
cvt.rn.f32.f64	%f23, %fd45;
cvt.rn.f32.f64	%f24, %fd46;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f23;
cvt.rn.f16.f32 high, %f24;
mov.b32 %r12, {low,high};}


	st.global.u32 [%rd6+44], %r12;
ld.global.v2.f64 {%fd49, %fd50}, [%rd8+192];
cvt.rn.f32.f64	%f25, %fd49;
cvt.rn.f32.f64	%f26, %fd50;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f25;
cvt.rn.f16.f32 high, %f26;
mov.b32 %r13, {low,high};}


	st.global.u32 [%rd6+48], %r13;
ld.global.v2.f64 {%fd53, %fd54}, [%rd8+208];
cvt.rn.f32.f64	%f27, %fd53;
cvt.rn.f32.f64	%f28, %fd54;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f27;
cvt.rn.f16.f32 high, %f28;
mov.b32 %r14, {low,high};}


	st.global.u32 [%rd6+52], %r14;
ld.global.v2.f64 {%fd57, %fd58}, [%rd8+224];
cvt.rn.f32.f64	%f29, %fd57;
cvt.rn.f32.f64	%f30, %fd58;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f29;
cvt.rn.f16.f32 high, %f30;
mov.b32 %r15, {low,high};}


	st.global.u32 [%rd6+56], %r15;
ld.global.v2.f64 {%fd61, %fd62}, [%rd8+240];
cvt.rn.f32.f64	%f31, %fd61;
cvt.rn.f32.f64	%f32, %fd62;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f31;
cvt.rn.f16.f32 high, %f32;
mov.b32 %r16, {low,high};}


	st.global.u32 [%rd6+60], %r16;
ret;
}

.func (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
.param .b64 __internal_trig_reduction_slowpathd_param_0,
.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
.local .align 8 .b8 __local_depot4[40];
.reg .b64 %SP;
.reg .b64 %SPL;
.reg .pred %p<9>;
.reg .b32 %r<42>;
.reg .f64 %fd<5>;
.reg .b64 %rd<101>;


mov.u64 %SPL, __local_depot4;
ld.param.f64 %fd4, [__internal_trig_reduction_slowpathd_param_0];
ld.param.u64 %rd37, [__internal_trig_reduction_slowpathd_param_1];
add.u64 %rd1, %SPL, 0;
{
.reg .b32 %temp; 
mov.b64 {%temp, %r1}, %fd4;
}
and.b32 %r40, %r1, -2147483648;
shr.u32 %r3, %r1, 20;
bfe.u32 %r4, %r1, 20, 11;
setp.eq.s32	%p1, %r4, 2047;
@%p1 bra BB4_13;

add.s32 %r15, %r4, -1024;
shr.u32 %r16, %r15, 6;
mov.u32 %r17, 15;
sub.s32 %r5, %r17, %r16;
mov.u32 %r18, 19;
sub.s32 %r19, %r18, %r16;
mov.u32 %r20, 18;
min.s32 %r6, %r20, %r19;
mov.u64 %rd94, 0;
setp.ge.s32	%p2, %r5, %r6;
mov.u64 %rd93, %rd1;
@%p2 bra BB4_4;

bfe.u32 %r21, %r1, 20, 11;
add.s32 %r22, %r21, -1024;
shr.u32 %r23, %r22, 6;
sub.s32 %r25, %r17, %r23;
mul.wide.s32 %rd41, %r25, 8;
mov.u64 %rd42, __cudart_i2opi_d;
add.s64 %rd89, %rd42, %rd41;
mov.b64 %rd43, %fd4;
shl.b64 %rd44, %rd43, 11;
or.b64 %rd5, %rd44, -9223372036854775808;
mov.u64 %rd94, 0;
mov.u64 %rd93, %rd1;
mov.u64 %rd91, %rd1;
mov.u32 %r39, %r5;

BB4_3:
.pragma "nounroll";
ld.const.u64 %rd47, [%rd89];

	{
.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
mov.b64 {alo,ahi}, %rd47; 
mov.b64 {blo,bhi}, %rd5; 
mov.b64 {clo,chi}, %rd94; 
mad.lo.cc.u32 r0, alo, blo, clo;
madc.hi.cc.u32 r1, alo, blo, chi;
madc.hi.u32 r2, alo, bhi, 0;
mad.lo.cc.u32 r1, alo, bhi, r1;
madc.hi.cc.u32 r2, ahi, blo, r2;
madc.hi.u32 r3, ahi, bhi, 0;
mad.lo.cc.u32 r1, ahi, blo, r1;
madc.lo.cc.u32 r2, ahi, bhi, r2;
addc.u32 r3, r3, 0; 
mov.b64 %rd45, {r0,r1}; 
mov.b64 %rd94, {r2,r3}; 
}

	st.local.u64 [%rd91], %rd45;
add.s32 %r39, %r39, 1;
sub.s32 %r26, %r39, %r5;
mul.wide.s32 %rd50, %r26, 8;
add.s64 %rd91, %rd1, %rd50;
add.s64 %rd93, %rd93, 8;
add.s64 %rd89, %rd89, 8;
setp.lt.s32	%p3, %r39, %r6;
@%p3 bra BB4_3;

BB4_4:
st.local.u64 [%rd93], %rd94;
ld.local.u64 %rd95, [%rd1+16];
ld.local.u64 %rd96, [%rd1+24];
and.b32 %r9, %r3, 63;
setp.eq.s32	%p4, %r9, 0;
@%p4 bra BB4_6;

mov.u32 %r27, 64;
sub.s32 %r28, %r27, %r9;
shl.b64 %rd51, %rd96, %r9;
shr.u64 %rd52, %rd95, %r28;
or.b64 %rd96, %rd51, %rd52;
shl.b64 %rd53, %rd95, %r9;
ld.local.u64 %rd54, [%rd1+8];
shr.u64 %rd55, %rd54, %r28;
or.b64 %rd95, %rd55, %rd53;

BB4_6:
shr.u64 %rd56, %rd96, 62;
cvt.u32.u64	%r29, %rd56;
shr.u64 %rd57, %rd95, 62;
shl.b64 %rd58, %rd96, 2;
or.b64 %rd98, %rd58, %rd57;
shl.b64 %rd97, %rd95, 2;
shr.u64 %rd59, %rd96, 61;
cvt.u32.u64	%r30, %rd59;
and.b32 %r31, %r30, 1;
add.s32 %r32, %r31, %r29;
neg.s32 %r33, %r32;
setp.eq.s32	%p5, %r40, 0;
selp.b32	%r34, %r32, %r33, %p5;
cvta.to.local.u64 %rd60, %rd37;
st.local.u32 [%rd60], %r34;
setp.eq.s32	%p6, %r31, 0;
@%p6 bra BB4_8;

mov.u64 %rd64, 0;

	{
.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
mov.b64 {a0,a1}, %rd64;
mov.b64 {a2,a3}, %rd64;
mov.b64 {b0,b1}, %rd97;
mov.b64 {b2,b3}, %rd98;
sub.cc.u32 r0, a0, b0; 
subc.cc.u32 r1, a1, b1; 
subc.cc.u32 r2, a2, b2; 
subc.u32 r3, a3, b3; 
mov.b64 %rd97, {r0,r1};
mov.b64 %rd98, {r2,r3};
}

	xor.b32 %r40, %r40, -2147483648;

BB4_8:
clz.b64 %r41, %rd98;
setp.eq.s32	%p7, %r41, 0;
@%p7 bra BB4_10;

shl.b64 %rd67, %rd98, %r41;
mov.u32 %r35, 64;
sub.s32 %r36, %r35, %r41;
shr.u64 %rd68, %rd97, %r36;
or.b64 %rd98, %rd68, %rd67;

BB4_10:
mov.u64 %rd72, -3958705157555305931;

	{
.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
mov.b64 {alo,ahi}, %rd98; 
mov.b64 {blo,bhi}, %rd72; 
mul.lo.u32 r0, alo, blo; 
mul.hi.u32 r1, alo, blo; 
mad.lo.cc.u32 r1, alo, bhi, r1;
madc.hi.u32 r2, alo, bhi, 0;
mad.lo.cc.u32 r1, ahi, blo, r1;
madc.hi.cc.u32 r2, ahi, blo, r2;
madc.hi.u32 r3, ahi, bhi, 0;
mad.lo.cc.u32 r2, ahi, bhi, r2;
addc.u32 r3, r3, 0; 
mov.b64 %rd69, {r0,r1}; 
mov.b64 %rd100, {r2,r3}; 
}

	setp.lt.s64	%p8, %rd100, 1;
@%p8 bra BB4_12;


	{
.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
mov.b64 {a0,a1}, %rd69;
mov.b64 {a2,a3}, %rd100;
mov.b64 {b0,b1}, %rd69;
mov.b64 {b2,b3}, %rd100;
add.cc.u32 r0, a0, b0; 
addc.cc.u32 r1, a1, b1; 
addc.cc.u32 r2, a2, b2; 
addc.u32 r3, a3, b3; 
mov.b64 %rd73, {r0,r1};
mov.b64 %rd100, {r2,r3};
}

	add.s32 %r41, %r41, 1;

BB4_12:
cvt.u64.u32	%rd79, %r40;
shl.b64 %rd80, %rd79, 32;
mov.u32 %r37, 1022;
sub.s32 %r38, %r37, %r41;
cvt.u64.u32	%rd81, %r38;
shl.b64 %rd82, %rd81, 52;
add.s64 %rd83, %rd100, 1;
shr.u64 %rd84, %rd83, 10;
add.s64 %rd85, %rd84, 1;
shr.u64 %rd86, %rd85, 1;
add.s64 %rd87, %rd86, %rd82;
or.b64 %rd88, %rd87, %rd80;
mov.b64 %fd4, %rd88;

BB4_13:
st.param.f64	[func_retval0+0], %fd4;
ret;
}


