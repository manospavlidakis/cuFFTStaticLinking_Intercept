







.version 7.0
.target sm_80
.address_size 64


.global .align 4 .u32 _ZZN75_INTERNAL_53_half_32bit_vector_r2c_RT_SM70_plus_compute_80_cpp1_ii_2509c6dc18cooperative_groups4__v17details17_binary_partitionINS1_15coalesced_groupEEES4_RKT_bE8fullMask = -1;
.extern .shared .align 4 .b8 smem_full[];

.weak .entry _Z14vector_fft_r2cILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<13>;
.reg .b16 %rs<10>;
.reg .f32 %f<2>;
.reg .b32 %r<110>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u64 %rd6, [_Z14vector_fft_r2cILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_r2cILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj2ELj2ELj64EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r50, %r2, 5;
mov.u32 %r3, %tid.y;
add.s32 %r51, %r50, %r3;
shl.b32 %r4, %r51, 1;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB0_6;

setp.eq.s16	%p2, %rs1, 0;
selp.b32	%r52, 1, 2, %p2;
shl.b32 %r5, %r3, 1;
shl.b32 %r53, %r2, 6;
add.s32 %r54, %r5, %r53;
add.s32 %r55, %r54, 1;
mov.u32 %r6, %nctaid.x;
add.s32 %r56, %r6, -1;
setp.lt.u32	%p3, %r2, %r56;
mov.u32 %r57, %tid.x;
mad.lo.s32 %r58, %r54, %r52, %r57;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r58, 4;
add.s64 %rd1, %rd7, %rd8;
mad.lo.s32 %r59, %r55, %r52, %r57;
mul.wide.u32 %rd9, %r59, 4;
add.s64 %rd2, %rd7, %rd9;
@%p3 bra BB0_3;
bra.uni BB0_2;

BB0_3:
ld.global.u32 %r107, [%rd1];
bra.uni BB0_4;

BB0_2:
shl.b32 %r61, %r6, 6;
add.s32 %r62, %r5, %r61;
add.s32 %r63, %r62, -63;
ld.global.u32 %r107, [%rd1];
setp.ge.u32	%p4, %r63, %r1;
@%p4 bra BB0_5;

BB0_4:
ld.global.u32 %r106, [%rd2];

BB0_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r107;
mov.b32 {blow,bhigh}, %r106;
mov.b32 %r109, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r107;
mov.b32 {blow,bhigh}, %r106;
mov.b32 %r108, {ahigh,bhigh};}



BB0_6:

	{add.f16x2 %r70,%r109,%r108;
}

	mov.u32 %r80, 0;

	{add.f16x2 %r73,%r80,%r82;
}

	
	{sub.f16x2 %r76,%r109,%r108;
}

	
	{sub.f16x2 %r79,%r80,%r83;
}

	@%p1 bra BB0_18;

mov.u32 %r96, %tid.x;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r70;
mov.b32 {blow,bhigh}, %r73;
mov.b32 %r84, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r70;
mov.b32 {blow,bhigh}, %r73;
mov.b32 %r87, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r76;
mov.b32 {blow,bhigh}, %r79;
mov.b32 %r90, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r76;
mov.b32 {blow,bhigh}, %r79;
mov.b32 %r93, {ahigh,bhigh};}


	shl.b32 %r25, %r3, 1;
shl.b32 %r97, %r2, 6;
add.s32 %r98, %r25, %r97;
shl.b32 %r99, %r98, 1;
add.s32 %r100, %r96, %r99;
mov.u32 %r26, %nctaid.x;
add.s32 %r101, %r26, -1;
setp.lt.u32	%p6, %r2, %r101;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r100, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r27, %r96, 1;
add.s32 %r102, %r100, 2;
mul.wide.u32 %rd12, %r102, 4;
add.s64 %rd4, %rd10, %rd12;
@%p6 bra BB0_14;
bra.uni BB0_8;

BB0_14:
st.global.u32 [%rd3], %r84;
setp.gt.u32	%p11, %r27, 1;
@%p11 bra BB0_16;

st.global.u32 [%rd3+4], %r90;

BB0_16:
st.global.u32 [%rd4], %r87;
@%p11 bra BB0_18;
bra.uni BB0_17;

BB0_8:
shl.b32 %r103, %r26, 6;
add.s32 %r104, %r25, %r103;
add.s32 %r105, %r104, -63;
setp.lt.u32	%p7, %r105, %r1;
st.global.u32 [%rd3], %r84;
@%p7 bra BB0_11;
bra.uni BB0_9;

BB0_11:
setp.gt.u32	%p9, %r27, 1;
@%p9 bra BB0_13;

st.global.u32 [%rd3+4], %r90;

BB0_13:
st.global.u32 [%rd4], %r87;
@%p9 bra BB0_18;

BB0_17:
st.global.u32 [%rd4+4], %r93;
bra.uni BB0_18;

BB0_9:
setp.gt.u32	%p8, %r27, 1;
@%p8 bra BB0_18;

st.global.u32 [%rd3+4], %r90;

BB0_18:
ret;
}


.weak .entry _Z14vector_fft_r2cILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<13>;
.reg .b16 %rs<10>;
.reg .f32 %f<11>;
.reg .b32 %r<188>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u64 %rd6, [_Z14vector_fft_r2cILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_r2cILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj4ELj2ELj32EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r62, %r2, 4;
mov.u32 %r3, %tid.y;
add.s32 %r63, %r62, %r3;
shl.b32 %r4, %r63, 1;
mov.u32 %r5, %tid.x;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB1_6;

setp.eq.s16	%p2, %rs1, 0;
selp.b32	%r64, 2, 3, %p2;
shl.b32 %r6, %r3, 1;
shl.b32 %r65, %r2, 5;
add.s32 %r66, %r6, %r65;
add.s32 %r67, %r66, 1;
mov.u32 %r7, %nctaid.x;
add.s32 %r68, %r7, -1;
setp.lt.u32	%p3, %r2, %r68;
mad.lo.s32 %r69, %r66, %r64, %r5;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r69, 4;
add.s64 %rd1, %rd7, %rd8;
mad.lo.s32 %r70, %r67, %r64, %r5;
mul.wide.u32 %rd9, %r70, 4;
add.s64 %rd2, %rd7, %rd9;
@%p3 bra BB1_3;
bra.uni BB1_2;

BB1_3:
ld.global.u32 %r185, [%rd1];
bra.uni BB1_4;

BB1_2:
shl.b32 %r72, %r7, 5;
add.s32 %r73, %r6, %r72;
add.s32 %r74, %r73, -31;
ld.global.u32 %r185, [%rd1];
setp.ge.u32	%p4, %r74, %r1;
@%p4 bra BB1_5;

BB1_4:
ld.global.u32 %r184, [%rd2];

BB1_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r185;
mov.b32 {blow,bhigh}, %r184;
mov.b32 %r75, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r185;
mov.b32 {blow,bhigh}, %r184;
mov.b32 %r78, {ahigh,bhigh};}


	shl.b32 %r81, %r5, 1;
shl.b32 %r14, %r3, 2;
add.s32 %r82, %r81, %r14;
shl.b32 %r83, %r82, 2;
mov.u32 %r84, smem_full;
add.s32 %r85, %r84, %r83;
st.shared.u32 [%r85], %r75;
st.shared.u32 [%r85+4], %r78;
barrier.sync 0;
add.s32 %r86, %r14, %r5;
shl.b32 %r87, %r86, 2;
add.s32 %r89, %r84, %r87;
ld.shared.u32 %r187, [%r89];
ld.shared.u32 %r186, [%r89+8];
barrier.sync 0;

BB1_6:

	{add.f16x2 %r90,%r187,%r186;
}

	mov.u32 %r101, 0;

	{add.f16x2 %r93,%r101,%r101;
}

	
	{sub.f16x2 %r96,%r187,%r186;
}

	
	{sub.f16x2 %r99,%r101,%r101;
}

	and.b32 %r21, %r5, 1;
cvt.rn.f32.u32	%f8, %r21;
mul.f32 %f9, %f8, 0f3FC90FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r102, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r102;
mov.b32 %r105, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r102;
mov.b32 %r107, {high,high};}


	
	{mul.f16x2 %r109,%r99,%r107;
}

	
	{xor.b32 %r112,%r109,0x80008000;
}

	
	{fma.rn.f16x2 %r114,%r96,%r105,%r112;
}

	
	{mul.f16x2 %r118,%r96,%r107;
}

	
	{fma.rn.f16x2 %r121,%r99,%r105,%r118;
}

	shl.b32 %r142, %r5, 1;
and.b32 %r143, %r142, -4;
shl.b32 %r144, %r3, 2;
add.s32 %r24, %r143, %r144;
barrier.sync 0;
shl.b32 %r145, %r21, 1;
add.s32 %r146, %r145, %r24;
shl.b32 %r147, %r146, 2;
mov.u32 %r148, smem_full;
add.s32 %r25, %r148, %r147;
st.shared.u32 [%r25], %r90;
st.shared.u32 [%r25+4], %r114;
barrier.sync 0;
add.s32 %r149, %r21, %r24;
shl.b32 %r150, %r149, 2;
add.s32 %r26, %r148, %r150;
ld.shared.u32 %r27, [%r26];
ld.shared.u32 %r28, [%r26+8];
barrier.sync 0;
st.shared.u32 [%r25], %r93;
st.shared.u32 [%r25+4], %r121;
barrier.sync 0;
ld.shared.u32 %r156, [%r26];
ld.shared.u32 %r157, [%r26+8];

	{add.f16x2 %r152,%r27,%r28;
}

	
	{add.f16x2 %r155,%r156,%r157;
}

	
	{sub.f16x2 %r158,%r27,%r28;
}

	
	{sub.f16x2 %r161,%r156,%r157;
}

	@%p1 bra BB1_18;


	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r152;
mov.b32 {blow,bhigh}, %r155;
mov.b32 %r164, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r152;
mov.b32 {blow,bhigh}, %r155;
mov.b32 %r167, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r158;
mov.b32 {blow,bhigh}, %r161;
mov.b32 %r170, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r158;
mov.b32 {blow,bhigh}, %r161;
mov.b32 %r173, {ahigh,bhigh};}


	shl.b32 %r37, %r3, 1;
shl.b32 %r176, %r2, 5;
add.s32 %r177, %r37, %r176;
mad.lo.s32 %r178, %r177, 3, %r5;
mov.u32 %r38, %nctaid.x;
add.s32 %r179, %r38, -1;
setp.lt.u32	%p6, %r2, %r179;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r178, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r39, %r5, 2;
add.s32 %r180, %r178, 3;
mul.wide.u32 %rd12, %r180, 4;
add.s64 %rd4, %rd10, %rd12;
@%p6 bra BB1_14;
bra.uni BB1_8;

BB1_14:
st.global.u32 [%rd3], %r164;
setp.gt.u32	%p11, %r39, 2;
@%p11 bra BB1_16;

st.global.u32 [%rd3+8], %r170;

BB1_16:
st.global.u32 [%rd4], %r167;
@%p11 bra BB1_18;
bra.uni BB1_17;

BB1_8:
shl.b32 %r181, %r38, 5;
add.s32 %r182, %r37, %r181;
add.s32 %r183, %r182, -31;
setp.lt.u32	%p7, %r183, %r1;
st.global.u32 [%rd3], %r164;
@%p7 bra BB1_11;
bra.uni BB1_9;

BB1_11:
setp.gt.u32	%p9, %r39, 2;
@%p9 bra BB1_13;

st.global.u32 [%rd3+8], %r170;

BB1_13:
st.global.u32 [%rd4], %r167;
@%p9 bra BB1_18;

BB1_17:
st.global.u32 [%rd4+8], %r173;
bra.uni BB1_18;

BB1_9:
setp.gt.u32	%p8, %r39, 2;
@%p8 bra BB1_18;

st.global.u32 [%rd3+8], %r170;

BB1_18:
ret;
}


.weak .entry _Z14vector_fft_r2cILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<13>;
.reg .b16 %rs<10>;
.reg .f32 %f<20>;
.reg .b32 %r<259>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u64 %rd6, [_Z14vector_fft_r2cILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_r2cILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj8ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r72, %r2, 3;
mov.u32 %r3, %tid.y;
add.s32 %r73, %r72, %r3;
shl.b32 %r4, %r73, 1;
mov.u32 %r5, %tid.x;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB2_6;

setp.eq.s16	%p2, %rs1, 0;
selp.b32	%r74, 4, 5, %p2;
shl.b32 %r6, %r3, 1;
shl.b32 %r75, %r2, 4;
add.s32 %r76, %r6, %r75;
add.s32 %r77, %r76, 1;
mov.u32 %r7, %nctaid.x;
add.s32 %r78, %r7, -1;
setp.lt.u32	%p3, %r2, %r78;
mad.lo.s32 %r79, %r76, %r74, %r5;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r79, 4;
add.s64 %rd1, %rd7, %rd8;
mad.lo.s32 %r80, %r77, %r74, %r5;
mul.wide.u32 %rd9, %r80, 4;
add.s64 %rd2, %rd7, %rd9;
@%p3 bra BB2_3;
bra.uni BB2_2;

BB2_3:
ld.global.u32 %r256, [%rd1];
bra.uni BB2_4;

BB2_2:
shl.b32 %r82, %r7, 4;
add.s32 %r83, %r6, %r82;
add.s32 %r84, %r83, -15;
ld.global.u32 %r256, [%rd1];
setp.ge.u32	%p4, %r84, %r1;
@%p4 bra BB2_5;

BB2_4:
ld.global.u32 %r255, [%rd2];

BB2_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r256;
mov.b32 {blow,bhigh}, %r255;
mov.b32 %r85, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r256;
mov.b32 {blow,bhigh}, %r255;
mov.b32 %r88, {ahigh,bhigh};}


	shl.b32 %r91, %r5, 1;
shl.b32 %r14, %r3, 3;
add.s32 %r92, %r91, %r14;
shl.b32 %r93, %r92, 2;
mov.u32 %r94, smem_full;
add.s32 %r95, %r94, %r93;
st.shared.u32 [%r95], %r85;
st.shared.u32 [%r95+4], %r88;
barrier.sync 0;
add.s32 %r96, %r14, %r5;
shl.b32 %r97, %r96, 2;
add.s32 %r99, %r94, %r97;
ld.shared.u32 %r258, [%r99];
ld.shared.u32 %r257, [%r99+16];
barrier.sync 0;

BB2_6:

	{add.f16x2 %r100,%r258,%r257;
}

	mov.u32 %r111, 0;

	{add.f16x2 %r103,%r111,%r111;
}

	
	{sub.f16x2 %r106,%r258,%r257;
}

	
	{sub.f16x2 %r109,%r111,%r111;
}

	and.b32 %r21, %r5, 3;
cvt.rn.f32.u32	%f8, %r21;
mul.f32 %f9, %f8, 0f3F490FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r112, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r112;
mov.b32 %r115, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r112;
mov.b32 %r117, {high,high};}


	
	{mul.f16x2 %r119,%r109,%r117;
}

	
	{xor.b32 %r122,%r119,0x80008000;
}

	
	{fma.rn.f16x2 %r124,%r106,%r115,%r122;
}

	
	{mul.f16x2 %r128,%r106,%r117;
}

	
	{fma.rn.f16x2 %r131,%r109,%r115,%r128;
}

	shl.b32 %r152, %r5, 1;
and.b32 %r153, %r152, -8;
shl.b32 %r154, %r3, 3;
add.s32 %r24, %r153, %r154;
barrier.sync 0;
shl.b32 %r155, %r21, 1;
add.s32 %r156, %r155, %r24;
shl.b32 %r157, %r156, 2;
mov.u32 %r158, smem_full;
add.s32 %r25, %r158, %r157;
st.shared.u32 [%r25], %r100;
st.shared.u32 [%r25+4], %r124;
barrier.sync 0;
add.s32 %r159, %r21, %r24;
shl.b32 %r160, %r159, 2;
add.s32 %r26, %r158, %r160;
ld.shared.u32 %r27, [%r26];
ld.shared.u32 %r28, [%r26+16];
barrier.sync 0;
st.shared.u32 [%r25], %r103;
st.shared.u32 [%r25+4], %r131;
barrier.sync 0;
ld.shared.u32 %r166, [%r26];
ld.shared.u32 %r167, [%r26+16];

	{add.f16x2 %r162,%r27,%r28;
}

	
	{add.f16x2 %r165,%r166,%r167;
}

	
	{sub.f16x2 %r168,%r27,%r28;
}

	
	{sub.f16x2 %r171,%r166,%r167;
}

	and.b32 %r31, %r5, 2;
bfe.u32 %r214, %r5, 1, 1;
cvt.rn.f32.u32	%f17, %r214;
mul.f32 %f18, %f17, 0f3FC90FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r174, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r177, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r179, {high,high};}


	
	{mul.f16x2 %r181,%r171,%r179;
}

	
	{xor.b32 %r184,%r181,0x80008000;
}

	
	{fma.rn.f16x2 %r186,%r168,%r177,%r184;
}

	
	{mul.f16x2 %r190,%r168,%r179;
}

	
	{fma.rn.f16x2 %r193,%r171,%r177,%r190;
}

	and.b32 %r215, %r5, 1;
add.s32 %r34, %r24, %r215;
barrier.sync 0;
shl.b32 %r216, %r31, 1;
add.s32 %r217, %r216, %r34;
shl.b32 %r218, %r217, 2;
add.s32 %r35, %r158, %r218;
st.shared.u32 [%r35], %r162;
st.shared.u32 [%r35+8], %r186;
barrier.sync 0;
add.s32 %r220, %r31, %r34;
shl.b32 %r221, %r220, 2;
add.s32 %r36, %r158, %r221;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+16];
barrier.sync 0;
st.shared.u32 [%r35], %r165;
st.shared.u32 [%r35+8], %r193;
barrier.sync 0;
ld.shared.u32 %r227, [%r36];
ld.shared.u32 %r228, [%r36+16];

	{add.f16x2 %r223,%r37,%r38;
}

	
	{add.f16x2 %r226,%r227,%r228;
}

	
	{sub.f16x2 %r229,%r37,%r38;
}

	
	{sub.f16x2 %r232,%r227,%r228;
}

	@%p1 bra BB2_18;


	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r223;
mov.b32 {blow,bhigh}, %r226;
mov.b32 %r235, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r223;
mov.b32 {blow,bhigh}, %r226;
mov.b32 %r238, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r229;
mov.b32 {blow,bhigh}, %r232;
mov.b32 %r241, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r229;
mov.b32 {blow,bhigh}, %r232;
mov.b32 %r244, {ahigh,bhigh};}


	shl.b32 %r47, %r3, 1;
shl.b32 %r247, %r2, 4;
add.s32 %r248, %r47, %r247;
mad.lo.s32 %r249, %r248, 5, %r5;
mov.u32 %r48, %nctaid.x;
add.s32 %r250, %r48, -1;
setp.lt.u32	%p6, %r2, %r250;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r249, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r49, %r5, 4;
add.s32 %r251, %r249, 5;
mul.wide.u32 %rd12, %r251, 4;
add.s64 %rd4, %rd10, %rd12;
@%p6 bra BB2_14;
bra.uni BB2_8;

BB2_14:
st.global.u32 [%rd3], %r235;
setp.gt.u32	%p11, %r49, 4;
@%p11 bra BB2_16;

st.global.u32 [%rd3+16], %r241;

BB2_16:
st.global.u32 [%rd4], %r238;
@%p11 bra BB2_18;
bra.uni BB2_17;

BB2_8:
shl.b32 %r252, %r48, 4;
add.s32 %r253, %r47, %r252;
add.s32 %r254, %r253, -15;
setp.lt.u32	%p7, %r254, %r1;
st.global.u32 [%rd3], %r235;
@%p7 bra BB2_11;
bra.uni BB2_9;

BB2_11:
setp.gt.u32	%p9, %r49, 4;
@%p9 bra BB2_13;

st.global.u32 [%rd3+16], %r241;

BB2_13:
st.global.u32 [%rd4], %r238;
@%p9 bra BB2_18;

BB2_17:
st.global.u32 [%rd4+16], %r244;
bra.uni BB2_18;

BB2_9:
setp.gt.u32	%p8, %r49, 4;
@%p8 bra BB2_18;

st.global.u32 [%rd3+16], %r241;

BB2_18:
ret;
}


.weak .entry _Z14vector_fft_r2cILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<13>;
.reg .b16 %rs<10>;
.reg .f32 %f<29>;
.reg .b32 %r<330>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u64 %rd6, [_Z14vector_fft_r2cILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_r2cILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj16ELj2ELj8EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r82, %r2, 2;
mov.u32 %r3, %tid.y;
add.s32 %r83, %r82, %r3;
shl.b32 %r4, %r83, 1;
mov.u32 %r5, %tid.x;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB3_6;

setp.eq.s16	%p2, %rs1, 0;
selp.b32	%r84, 8, 9, %p2;
shl.b32 %r6, %r3, 1;
shl.b32 %r85, %r2, 3;
add.s32 %r86, %r6, %r85;
add.s32 %r87, %r86, 1;
mov.u32 %r7, %nctaid.x;
add.s32 %r88, %r7, -1;
setp.lt.u32	%p3, %r2, %r88;
mad.lo.s32 %r89, %r86, %r84, %r5;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r89, 4;
add.s64 %rd1, %rd7, %rd8;
mad.lo.s32 %r90, %r87, %r84, %r5;
mul.wide.u32 %rd9, %r90, 4;
add.s64 %rd2, %rd7, %rd9;
@%p3 bra BB3_3;
bra.uni BB3_2;

BB3_3:
ld.global.u32 %r327, [%rd1];
bra.uni BB3_4;

BB3_2:
shl.b32 %r92, %r7, 3;
add.s32 %r93, %r6, %r92;
add.s32 %r94, %r93, -7;
ld.global.u32 %r327, [%rd1];
setp.ge.u32	%p4, %r94, %r1;
@%p4 bra BB3_5;

BB3_4:
ld.global.u32 %r326, [%rd2];

BB3_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r327;
mov.b32 {blow,bhigh}, %r326;
mov.b32 %r95, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r327;
mov.b32 {blow,bhigh}, %r326;
mov.b32 %r98, {ahigh,bhigh};}


	shl.b32 %r101, %r5, 1;
shl.b32 %r14, %r3, 4;
add.s32 %r102, %r101, %r14;
shl.b32 %r103, %r102, 2;
mov.u32 %r104, smem_full;
add.s32 %r105, %r104, %r103;
st.shared.u32 [%r105], %r95;
st.shared.u32 [%r105+4], %r98;
barrier.sync 0;
add.s32 %r106, %r14, %r5;
shl.b32 %r107, %r106, 2;
add.s32 %r109, %r104, %r107;
ld.shared.u32 %r329, [%r109];
ld.shared.u32 %r328, [%r109+32];
barrier.sync 0;

BB3_6:

	{add.f16x2 %r110,%r329,%r328;
}

	mov.u32 %r121, 0;

	{add.f16x2 %r113,%r121,%r121;
}

	
	{sub.f16x2 %r116,%r329,%r328;
}

	
	{sub.f16x2 %r119,%r121,%r121;
}

	and.b32 %r21, %r5, 7;
cvt.rn.f32.u32	%f8, %r21;
mul.f32 %f9, %f8, 0f3EC90FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r122, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r122;
mov.b32 %r125, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r122;
mov.b32 %r127, {high,high};}


	
	{mul.f16x2 %r129,%r119,%r127;
}

	
	{xor.b32 %r132,%r129,0x80008000;
}

	
	{fma.rn.f16x2 %r134,%r116,%r125,%r132;
}

	
	{mul.f16x2 %r138,%r116,%r127;
}

	
	{fma.rn.f16x2 %r141,%r119,%r125,%r138;
}

	shl.b32 %r162, %r5, 1;
and.b32 %r163, %r162, -16;
shl.b32 %r164, %r3, 4;
add.s32 %r24, %r163, %r164;
barrier.sync 0;
shl.b32 %r165, %r21, 1;
add.s32 %r166, %r165, %r24;
shl.b32 %r167, %r166, 2;
mov.u32 %r168, smem_full;
add.s32 %r25, %r168, %r167;
st.shared.u32 [%r25], %r110;
st.shared.u32 [%r25+4], %r134;
barrier.sync 0;
add.s32 %r169, %r21, %r24;
shl.b32 %r170, %r169, 2;
add.s32 %r26, %r168, %r170;
ld.shared.u32 %r27, [%r26];
ld.shared.u32 %r28, [%r26+32];
barrier.sync 0;
st.shared.u32 [%r25], %r113;
st.shared.u32 [%r25+4], %r141;
barrier.sync 0;
ld.shared.u32 %r176, [%r26];
ld.shared.u32 %r177, [%r26+32];

	{add.f16x2 %r172,%r27,%r28;
}

	
	{add.f16x2 %r175,%r176,%r177;
}

	
	{sub.f16x2 %r178,%r27,%r28;
}

	
	{sub.f16x2 %r181,%r176,%r177;
}

	and.b32 %r31, %r5, 6;
bfe.u32 %r224, %r5, 1, 2;
cvt.rn.f32.u32	%f17, %r224;
mul.f32 %f18, %f17, 0f3F490FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r184, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r184;
mov.b32 %r187, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r184;
mov.b32 %r189, {high,high};}


	
	{mul.f16x2 %r191,%r181,%r189;
}

	
	{xor.b32 %r194,%r191,0x80008000;
}

	
	{fma.rn.f16x2 %r196,%r178,%r187,%r194;
}

	
	{mul.f16x2 %r200,%r178,%r189;
}

	
	{fma.rn.f16x2 %r203,%r181,%r187,%r200;
}

	and.b32 %r225, %r5, 1;
add.s32 %r34, %r24, %r225;
barrier.sync 0;
shl.b32 %r226, %r31, 1;
add.s32 %r227, %r226, %r34;
shl.b32 %r228, %r227, 2;
add.s32 %r35, %r168, %r228;
st.shared.u32 [%r35], %r172;
st.shared.u32 [%r35+8], %r196;
barrier.sync 0;
add.s32 %r230, %r31, %r34;
shl.b32 %r231, %r230, 2;
add.s32 %r36, %r168, %r231;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+32];
barrier.sync 0;
st.shared.u32 [%r35], %r175;
st.shared.u32 [%r35+8], %r203;
barrier.sync 0;
ld.shared.u32 %r237, [%r36];
ld.shared.u32 %r238, [%r36+32];

	{add.f16x2 %r233,%r37,%r38;
}

	
	{add.f16x2 %r236,%r237,%r238;
}

	
	{sub.f16x2 %r239,%r37,%r38;
}

	
	{sub.f16x2 %r242,%r237,%r238;
}

	and.b32 %r41, %r5, 4;
bfe.u32 %r285, %r5, 2, 1;
cvt.rn.f32.u32	%f26, %r285;
mul.f32 %f27, %f26, 0f3FC90FDB;
cos.approx.f32 %f20, %f27;
sin.approx.f32 %f28, %f27;
neg.f32 %f21, %f28;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f20;
cvt.rn.f16.f32 high, %f21;
mov.b32 %r245, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r245;
mov.b32 %r248, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r245;
mov.b32 %r250, {high,high};}


	
	{mul.f16x2 %r252,%r242,%r250;
}

	
	{xor.b32 %r255,%r252,0x80008000;
}

	
	{fma.rn.f16x2 %r257,%r239,%r248,%r255;
}

	
	{mul.f16x2 %r261,%r239,%r250;
}

	
	{fma.rn.f16x2 %r264,%r242,%r248,%r261;
}

	and.b32 %r286, %r5, 3;
add.s32 %r44, %r24, %r286;
barrier.sync 0;
shl.b32 %r287, %r41, 1;
add.s32 %r288, %r287, %r44;
shl.b32 %r289, %r288, 2;
add.s32 %r45, %r168, %r289;
st.shared.u32 [%r45], %r233;
st.shared.u32 [%r45+16], %r257;
barrier.sync 0;
add.s32 %r291, %r41, %r44;
shl.b32 %r292, %r291, 2;
add.s32 %r46, %r168, %r292;
ld.shared.u32 %r47, [%r46];
ld.shared.u32 %r48, [%r46+32];
barrier.sync 0;
st.shared.u32 [%r45], %r236;
st.shared.u32 [%r45+16], %r264;
barrier.sync 0;
ld.shared.u32 %r298, [%r46];
ld.shared.u32 %r299, [%r46+32];

	{add.f16x2 %r294,%r47,%r48;
}

	
	{add.f16x2 %r297,%r298,%r299;
}

	
	{sub.f16x2 %r300,%r47,%r48;
}

	
	{sub.f16x2 %r303,%r298,%r299;
}

	@%p1 bra BB3_18;


	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r294;
mov.b32 {blow,bhigh}, %r297;
mov.b32 %r306, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r294;
mov.b32 {blow,bhigh}, %r297;
mov.b32 %r309, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r300;
mov.b32 {blow,bhigh}, %r303;
mov.b32 %r312, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r300;
mov.b32 {blow,bhigh}, %r303;
mov.b32 %r315, {ahigh,bhigh};}


	shl.b32 %r57, %r3, 1;
shl.b32 %r318, %r2, 3;
add.s32 %r319, %r57, %r318;
mad.lo.s32 %r320, %r319, 9, %r5;
mov.u32 %r58, %nctaid.x;
add.s32 %r321, %r58, -1;
setp.lt.u32	%p6, %r2, %r321;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r320, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r59, %r5, 8;
add.s32 %r322, %r320, 9;
mul.wide.u32 %rd12, %r322, 4;
add.s64 %rd4, %rd10, %rd12;
@%p6 bra BB3_14;
bra.uni BB3_8;

BB3_14:
st.global.u32 [%rd3], %r306;
setp.gt.u32	%p11, %r59, 8;
@%p11 bra BB3_16;

st.global.u32 [%rd3+32], %r312;

BB3_16:
st.global.u32 [%rd4], %r309;
@%p11 bra BB3_18;
bra.uni BB3_17;

BB3_8:
shl.b32 %r323, %r58, 3;
add.s32 %r324, %r57, %r323;
add.s32 %r325, %r324, -7;
setp.lt.u32	%p7, %r325, %r1;
st.global.u32 [%rd3], %r306;
@%p7 bra BB3_11;
bra.uni BB3_9;

BB3_11:
setp.gt.u32	%p9, %r59, 8;
@%p9 bra BB3_13;

st.global.u32 [%rd3+32], %r312;

BB3_13:
st.global.u32 [%rd4], %r309;
@%p9 bra BB3_18;

BB3_17:
st.global.u32 [%rd4+32], %r315;
bra.uni BB3_18;

BB3_9:
setp.gt.u32	%p8, %r59, 8;
@%p8 bra BB3_18;

st.global.u32 [%rd3+32], %r312;

BB3_18:
ret;
}


.weak .entry _Z14vector_fft_r2cILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 128, 1, 1
{
.reg .pred %p<13>;
.reg .b16 %rs<10>;
.reg .f32 %f<38>;
.reg .b32 %r<402>;
.reg .f64 %fd<2>;
.reg .b64 %rd<13>;


ld.param.u64 %rd6, [_Z14vector_fft_r2cILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u32 %r1, [_Z14vector_fft_r2cILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj32ELj2ELj16EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
mov.u32 %r2, %ctaid.x;
shl.b32 %r92, %r2, 3;
mov.u32 %r3, %tid.y;
add.s32 %r93, %r92, %r3;
shl.b32 %r4, %r93, 1;
mov.u32 %r5, %tid.x;
setp.ge.u32	%p1, %r4, %r1;
@%p1 bra BB4_6;

setp.eq.s16	%p2, %rs1, 0;
selp.b32	%r94, 16, 17, %p2;
shl.b32 %r6, %r3, 1;
shl.b32 %r95, %r2, 4;
add.s32 %r96, %r6, %r95;
add.s32 %r97, %r96, 1;
mov.u32 %r7, %nctaid.x;
add.s32 %r98, %r7, -1;
setp.lt.u32	%p3, %r2, %r98;
mad.lo.s32 %r99, %r96, %r94, %r5;
cvta.to.global.u64 %rd7, %rd5;
mul.wide.u32 %rd8, %r99, 4;
add.s64 %rd1, %rd7, %rd8;
mad.lo.s32 %r100, %r97, %r94, %r5;
mul.wide.u32 %rd9, %r100, 4;
add.s64 %rd2, %rd7, %rd9;
@%p3 bra BB4_3;
bra.uni BB4_2;

BB4_3:
ld.global.u32 %r399, [%rd1];
bra.uni BB4_4;

BB4_2:
shl.b32 %r102, %r7, 4;
add.s32 %r103, %r6, %r102;
add.s32 %r104, %r103, -15;
ld.global.u32 %r399, [%rd1];
setp.ge.u32	%p4, %r104, %r1;
@%p4 bra BB4_5;

BB4_4:
ld.global.u32 %r398, [%rd2];

BB4_5:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r399;
mov.b32 {blow,bhigh}, %r398;
mov.b32 %r105, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r399;
mov.b32 {blow,bhigh}, %r398;
mov.b32 %r108, {ahigh,bhigh};}


	shl.b32 %r111, %r5, 1;
shl.b32 %r14, %r3, 5;
add.s32 %r112, %r111, %r14;
shl.b32 %r113, %r112, 2;
mov.u32 %r114, smem_full;
add.s32 %r115, %r114, %r113;
st.shared.u32 [%r115], %r105;
st.shared.u32 [%r115+4], %r108;
barrier.sync 0;
add.s32 %r116, %r14, %r5;
shl.b32 %r117, %r116, 2;
add.s32 %r119, %r114, %r117;
ld.shared.u32 %r401, [%r119];
ld.shared.u32 %r400, [%r119+64];
barrier.sync 0;

BB4_6:

	{add.f16x2 %r120,%r401,%r400;
}

	mov.u32 %r131, 0;

	{add.f16x2 %r123,%r131,%r131;
}

	
	{sub.f16x2 %r126,%r401,%r400;
}

	
	{sub.f16x2 %r129,%r131,%r131;
}

	and.b32 %r21, %r5, 15;
cvt.rn.f32.u32	%f8, %r21;
mul.f32 %f9, %f8, 0f3E490FDB;
cos.approx.f32 %f2, %f9;
sin.approx.f32 %f10, %f9;
neg.f32 %f3, %f10;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r132, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r132;
mov.b32 %r135, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r132;
mov.b32 %r137, {high,high};}


	
	{mul.f16x2 %r139,%r129,%r137;
}

	
	{xor.b32 %r142,%r139,0x80008000;
}

	
	{fma.rn.f16x2 %r144,%r126,%r135,%r142;
}

	
	{mul.f16x2 %r148,%r126,%r137;
}

	
	{fma.rn.f16x2 %r151,%r129,%r135,%r148;
}

	shl.b32 %r172, %r5, 1;
and.b32 %r173, %r172, -32;
shl.b32 %r174, %r3, 5;
add.s32 %r24, %r173, %r174;
barrier.sync 0;
shl.b32 %r175, %r21, 1;
add.s32 %r176, %r175, %r24;
shl.b32 %r177, %r176, 2;
mov.u32 %r178, smem_full;
add.s32 %r25, %r178, %r177;
st.shared.u32 [%r25], %r120;
st.shared.u32 [%r25+4], %r144;
barrier.sync 0;
add.s32 %r179, %r21, %r24;
shl.b32 %r180, %r179, 2;
add.s32 %r26, %r178, %r180;
ld.shared.u32 %r27, [%r26];
ld.shared.u32 %r28, [%r26+64];
barrier.sync 0;
st.shared.u32 [%r25], %r123;
st.shared.u32 [%r25+4], %r151;
barrier.sync 0;
ld.shared.u32 %r186, [%r26];
ld.shared.u32 %r187, [%r26+64];

	{add.f16x2 %r182,%r27,%r28;
}

	
	{add.f16x2 %r185,%r186,%r187;
}

	
	{sub.f16x2 %r188,%r27,%r28;
}

	
	{sub.f16x2 %r191,%r186,%r187;
}

	and.b32 %r31, %r5, 14;
bfe.u32 %r234, %r5, 1, 3;
cvt.rn.f32.u32	%f17, %r234;
mul.f32 %f18, %f17, 0f3EC90FDB;
cos.approx.f32 %f11, %f18;
sin.approx.f32 %f19, %f18;
neg.f32 %f12, %f19;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f11;
cvt.rn.f16.f32 high, %f12;
mov.b32 %r194, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r194;
mov.b32 %r197, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r194;
mov.b32 %r199, {high,high};}


	
	{mul.f16x2 %r201,%r191,%r199;
}

	
	{xor.b32 %r204,%r201,0x80008000;
}

	
	{fma.rn.f16x2 %r206,%r188,%r197,%r204;
}

	
	{mul.f16x2 %r210,%r188,%r199;
}

	
	{fma.rn.f16x2 %r213,%r191,%r197,%r210;
}

	neg.s32 %r235, %r5;
and.b32 %r236, %r235, 1;
add.s32 %r34, %r24, %r236;
barrier.sync 0;
shl.b32 %r237, %r31, 1;
add.s32 %r238, %r237, %r34;
shl.b32 %r239, %r238, 2;
add.s32 %r35, %r178, %r239;
st.shared.u32 [%r35], %r182;
st.shared.u32 [%r35+8], %r206;
barrier.sync 0;
add.s32 %r241, %r31, %r34;
shl.b32 %r242, %r241, 2;
add.s32 %r36, %r178, %r242;
ld.shared.u32 %r37, [%r36];
ld.shared.u32 %r38, [%r36+64];
barrier.sync 0;
st.shared.u32 [%r35], %r185;
st.shared.u32 [%r35+8], %r213;
barrier.sync 0;
ld.shared.u32 %r248, [%r36];
ld.shared.u32 %r249, [%r36+64];

	{add.f16x2 %r244,%r37,%r38;
}

	
	{add.f16x2 %r247,%r248,%r249;
}

	
	{sub.f16x2 %r250,%r37,%r38;
}

	
	{sub.f16x2 %r253,%r248,%r249;
}

	and.b32 %r41, %r5, 12;
bfe.u32 %r296, %r5, 2, 2;
cvt.rn.f32.u32	%f26, %r296;
mul.f32 %f27, %f26, 0f3F490FDB;
cos.approx.f32 %f20, %f27;
sin.approx.f32 %f28, %f27;
neg.f32 %f21, %f28;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f20;
cvt.rn.f16.f32 high, %f21;
mov.b32 %r256, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r256;
mov.b32 %r259, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r256;
mov.b32 %r261, {high,high};}


	
	{mul.f16x2 %r263,%r253,%r261;
}

	
	{xor.b32 %r266,%r263,0x80008000;
}

	
	{fma.rn.f16x2 %r268,%r250,%r259,%r266;
}

	
	{mul.f16x2 %r272,%r250,%r261;
}

	
	{fma.rn.f16x2 %r275,%r253,%r259,%r272;
}

	and.b32 %r297, %r5, 3;
add.s32 %r44, %r24, %r297;
barrier.sync 0;
shl.b32 %r298, %r41, 1;
add.s32 %r299, %r298, %r44;
shl.b32 %r300, %r299, 2;
add.s32 %r45, %r178, %r300;
st.shared.u32 [%r45], %r244;
st.shared.u32 [%r45+16], %r268;
barrier.sync 0;
add.s32 %r302, %r41, %r44;
shl.b32 %r303, %r302, 2;
add.s32 %r46, %r178, %r303;
ld.shared.u32 %r47, [%r46];
ld.shared.u32 %r48, [%r46+64];
barrier.sync 0;
st.shared.u32 [%r45], %r247;
st.shared.u32 [%r45+16], %r275;
barrier.sync 0;
ld.shared.u32 %r309, [%r46];
ld.shared.u32 %r310, [%r46+64];

	{add.f16x2 %r305,%r47,%r48;
}

	
	{add.f16x2 %r308,%r309,%r310;
}

	
	{sub.f16x2 %r311,%r47,%r48;
}

	
	{sub.f16x2 %r314,%r309,%r310;
}

	and.b32 %r51, %r5, 8;
bfe.u32 %r357, %r5, 3, 1;
cvt.rn.f32.u32	%f35, %r357;
mul.f32 %f36, %f35, 0f3FC90FDB;
cos.approx.f32 %f29, %f36;
sin.approx.f32 %f37, %f36;
neg.f32 %f30, %f37;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f29;
cvt.rn.f16.f32 high, %f30;
mov.b32 %r317, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r317;
mov.b32 %r320, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r317;
mov.b32 %r322, {high,high};}


	
	{mul.f16x2 %r324,%r314,%r322;
}

	
	{xor.b32 %r327,%r324,0x80008000;
}

	
	{fma.rn.f16x2 %r329,%r311,%r320,%r327;
}

	
	{mul.f16x2 %r333,%r311,%r322;
}

	
	{fma.rn.f16x2 %r336,%r314,%r320,%r333;
}

	and.b32 %r358, %r5, 7;
add.s32 %r54, %r24, %r358;
barrier.sync 0;
shl.b32 %r359, %r51, 1;
add.s32 %r360, %r359, %r54;
shl.b32 %r361, %r360, 2;
add.s32 %r55, %r178, %r361;
st.shared.u32 [%r55], %r305;
st.shared.u32 [%r55+32], %r329;
barrier.sync 0;
add.s32 %r363, %r51, %r54;
shl.b32 %r364, %r363, 2;
add.s32 %r56, %r178, %r364;
ld.shared.u32 %r57, [%r56];
ld.shared.u32 %r58, [%r56+64];
barrier.sync 0;
st.shared.u32 [%r55], %r308;
st.shared.u32 [%r55+32], %r336;
barrier.sync 0;
ld.shared.u32 %r370, [%r56];
ld.shared.u32 %r371, [%r56+64];

	{add.f16x2 %r366,%r57,%r58;
}

	
	{add.f16x2 %r369,%r370,%r371;
}

	
	{sub.f16x2 %r372,%r57,%r58;
}

	
	{sub.f16x2 %r375,%r370,%r371;
}

	@%p1 bra BB4_18;


	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r366;
mov.b32 {blow,bhigh}, %r369;
mov.b32 %r378, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r366;
mov.b32 {blow,bhigh}, %r369;
mov.b32 %r381, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r372;
mov.b32 {blow,bhigh}, %r375;
mov.b32 %r384, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r372;
mov.b32 {blow,bhigh}, %r375;
mov.b32 %r387, {ahigh,bhigh};}


	shl.b32 %r67, %r3, 1;
shl.b32 %r390, %r2, 4;
add.s32 %r391, %r67, %r390;
mad.lo.s32 %r392, %r391, 17, %r5;
mov.u32 %r68, %nctaid.x;
add.s32 %r393, %r68, -1;
setp.lt.u32	%p6, %r2, %r393;
cvta.to.global.u64 %rd10, %rd6;
mul.wide.u32 %rd11, %r392, 4;
add.s64 %rd3, %rd10, %rd11;
add.s32 %r69, %r5, 16;
add.s32 %r394, %r392, 17;
mul.wide.u32 %rd12, %r394, 4;
add.s64 %rd4, %rd10, %rd12;
@%p6 bra BB4_14;
bra.uni BB4_8;

BB4_14:
st.global.u32 [%rd3], %r378;
setp.gt.u32	%p11, %r69, 16;
@%p11 bra BB4_16;

st.global.u32 [%rd3+64], %r384;

BB4_16:
st.global.u32 [%rd4], %r381;
@%p11 bra BB4_18;
bra.uni BB4_17;

BB4_8:
shl.b32 %r395, %r68, 4;
add.s32 %r396, %r67, %r395;
add.s32 %r397, %r396, -15;
setp.lt.u32	%p7, %r397, %r1;
st.global.u32 [%rd3], %r378;
@%p7 bra BB4_11;
bra.uni BB4_9;

BB4_11:
setp.gt.u32	%p9, %r69, 16;
@%p9 bra BB4_13;

st.global.u32 [%rd3+64], %r384;

BB4_13:
st.global.u32 [%rd4], %r381;
@%p9 bra BB4_18;

BB4_17:
st.global.u32 [%rd4+64], %r387;
bra.uni BB4_18;

BB4_9:
setp.gt.u32	%p8, %r69, 16;
@%p8 bra BB4_18;

st.global.u32 [%rd3+64], %r384;

BB4_18:
ret;
}


.weak .entry _Z14vector_fft_r2cILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 16, 1, 1
{
.reg .pred %p<11>;
.reg .b16 %rs<9>;
.reg .f32 %f<28>;
.reg .b32 %r<534>;
.reg .f64 %fd<2>;
.reg .b64 %rd<11>;


ld.param.u32 %r1, [_Z14vector_fft_r2cILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z14vector_fft_r2cILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj64ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
setp.eq.s16	%p1, %rs1, 0;
selp.b32	%r82, 32, 33, %p1;
mov.u32 %r2, %ctaid.x;
shl.b32 %r83, %r2, 1;
add.s32 %r84, %r83, 1;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
mad.lo.s32 %r85, %r83, %r82, %r5;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r85, 4;
add.s64 %rd1, %rd6, %rd7;
mad.lo.s32 %r86, %r84, %r82, %r5;
mul.wide.u32 %rd8, %r86, 4;
add.s64 %rd2, %rd6, %rd8;
@%p2 bra BB5_2;
bra.uni BB5_1;

BB5_2:
ld.global.u32 %r533, [%rd1];
ld.global.u32 %r531, [%rd1+64];
bra.uni BB5_3;

BB5_1:
shl.b32 %r88, %r3, 1;
add.s32 %r89, %r88, -1;
ld.global.u32 %r533, [%rd1];
ld.global.u32 %r531, [%rd1+64];
setp.ge.u32	%p3, %r89, %r1;
@%p3 bra BB5_4;

BB5_3:
ld.global.u32 %r532, [%rd2];
ld.global.u32 %r530, [%rd2+64];

BB5_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r533;
mov.b32 {blow,bhigh}, %r532;
mov.b32 %r90, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r533;
mov.b32 {blow,bhigh}, %r532;
mov.b32 %r93, {ahigh,bhigh};}


	shl.b32 %r102, %r5, 3;
mov.u32 %r103, smem_full;
add.s32 %r104, %r103, %r102;
st.shared.u32 [%r104], %r90;
st.shared.u32 [%r104+4], %r93;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r531;
mov.b32 {blow,bhigh}, %r530;
mov.b32 %r96, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r531;
mov.b32 {blow,bhigh}, %r530;
mov.b32 %r99, {ahigh,bhigh};}


	st.shared.u32 [%r104+128], %r96;
st.shared.u32 [%r104+132], %r99;
barrier.sync 0;
shl.b32 %r105, %r5, 2;
add.s32 %r107, %r103, %r105;
ld.shared.u32 %r18, [%r107];
ld.shared.u32 %r19, [%r107+64];
add.s32 %r20, %r5, 32;
ld.shared.u32 %r21, [%r107+128];
ld.shared.u32 %r22, [%r107+192];
barrier.sync 0;

	{add.f16x2 %r108,%r18,%r21;
}

	mov.u32 %r131, 0;

	{add.f16x2 %r111,%r131,%r131;
}

	
	{sub.f16x2 %r114,%r18,%r21;
}

	
	{sub.f16x2 %r117,%r131,%r131;
}

	
	{add.f16x2 %r120,%r19,%r22;
}

	
	{add.f16x2 %r123,%r131,%r131;
}

	
	{sub.f16x2 %r126,%r19,%r22;
}

	
	{sub.f16x2 %r129,%r131,%r131;
}

	
	{xor.b32 %r132,%r126,0x80008000;
}

	
	{add.f16x2 %r134,%r108,%r120;
}

	
	{add.f16x2 %r137,%r111,%r123;
}

	
	{sub.f16x2 %r140,%r108,%r120;
}

	
	{sub.f16x2 %r143,%r111,%r123;
}

	
	{add.f16x2 %r146,%r114,%r129;
}

	
	{add.f16x2 %r149,%r117,%r132;
}

	
	{sub.f16x2 %r152,%r114,%r129;
}

	
	{sub.f16x2 %r155,%r117,%r132;
}

	and.b32 %r25, %r5, 15;
cvt.rn.f32.u32	%f12, %r25;
mul.f32 %f13, %f12, 0f3DC90FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r158, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r158;
mov.b32 %r161, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r158;
mov.b32 %r163, {high,high};}


	
	{mul.f16x2 %r165,%r149,%r163;
}

	
	{xor.b32 %r168,%r165,0x80008000;
}

	
	{fma.rn.f16x2 %r170,%r146,%r161,%r168;
}

	
	{mul.f16x2 %r174,%r146,%r163;
}

	
	{fma.rn.f16x2 %r177,%r149,%r161,%r174;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r158;
mov.b32 %r181, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r158;
mov.b32 %r183, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r185, {low,high};}


	
	{mul.f16x2 %r186,%r183,%r185;
}

	
	{mul.f16x2 %r189,%r158,%r181;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r158;
mov.b32 %r192, {high,low};}


	
	{fma.rn.f16x2 %r194,%r186,%r192,%r189;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r194;
mov.b32 %r198, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r194;
mov.b32 %r200, {high,high};}


	
	{mul.f16x2 %r202,%r143,%r200;
}

	
	{xor.b32 %r205,%r202,0x80008000;
}

	
	{fma.rn.f16x2 %r207,%r140,%r198,%r205;
}

	
	{mul.f16x2 %r211,%r140,%r200;
}

	
	{fma.rn.f16x2 %r214,%r143,%r198,%r211;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r158;
mov.b32 %r218, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r158;
mov.b32 %r220, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r222, {low,high};}


	
	{mul.f16x2 %r223,%r220,%r222;
}

	
	{mul.f16x2 %r226,%r194,%r218;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r194;
mov.b32 %r229, {high,low};}


	
	{fma.rn.f16x2 %r231,%r223,%r229,%r226;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r231;
mov.b32 %r235, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r231;
mov.b32 %r237, {high,high};}


	
	{mul.f16x2 %r239,%r155,%r237;
}

	
	{xor.b32 %r242,%r239,0x80008000;
}

	
	{fma.rn.f16x2 %r244,%r152,%r235,%r242;
}

	
	{mul.f16x2 %r248,%r152,%r237;
}

	
	{fma.rn.f16x2 %r251,%r155,%r235,%r248;
}

	and.b32 %r32, %r105, -64;
barrier.sync 0;
shl.b32 %r273, %r25, 2;
add.s32 %r274, %r273, %r32;
shl.b32 %r275, %r274, 2;
add.s32 %r33, %r103, %r275;
st.shared.u32 [%r33], %r134;
st.shared.u32 [%r33+4], %r170;
st.shared.u32 [%r33+8], %r207;
st.shared.u32 [%r33+12], %r244;
barrier.sync 0;
add.s32 %r277, %r25, %r32;
shl.b32 %r278, %r277, 2;
add.s32 %r34, %r103, %r278;
ld.shared.u32 %r35, [%r34];
ld.shared.u32 %r36, [%r34+64];
ld.shared.u32 %r37, [%r34+128];
ld.shared.u32 %r38, [%r34+192];
barrier.sync 0;
st.shared.u32 [%r33], %r137;
st.shared.u32 [%r33+4], %r177;
st.shared.u32 [%r33+8], %r214;
st.shared.u32 [%r33+12], %r251;
barrier.sync 0;
ld.shared.u32 %r284, [%r34];
ld.shared.u32 %r296, [%r34+64];
ld.shared.u32 %r285, [%r34+128];
ld.shared.u32 %r297, [%r34+192];

	{add.f16x2 %r280,%r35,%r37;
}

	
	{add.f16x2 %r283,%r284,%r285;
}

	
	{sub.f16x2 %r286,%r35,%r37;
}

	
	{sub.f16x2 %r289,%r284,%r285;
}

	
	{add.f16x2 %r292,%r36,%r38;
}

	
	{add.f16x2 %r295,%r296,%r297;
}

	
	{sub.f16x2 %r298,%r36,%r38;
}

	
	{sub.f16x2 %r301,%r296,%r297;
}

	
	{xor.b32 %r304,%r298,0x80008000;
}

	
	{add.f16x2 %r306,%r280,%r292;
}

	
	{add.f16x2 %r309,%r283,%r295;
}

	
	{sub.f16x2 %r312,%r280,%r292;
}

	
	{sub.f16x2 %r315,%r283,%r295;
}

	
	{add.f16x2 %r318,%r286,%r301;
}

	
	{add.f16x2 %r321,%r289,%r304;
}

	
	{sub.f16x2 %r324,%r286,%r301;
}

	
	{sub.f16x2 %r327,%r289,%r304;
}

	and.b32 %r41, %r5, 12;
bfe.u32 %r444, %r5, 2, 2;
cvt.rn.f32.u32	%f25, %r444;
mul.f32 %f26, %f25, 0f3EC90FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r330, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r330;
mov.b32 %r333, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r330;
mov.b32 %r335, {high,high};}


	
	{mul.f16x2 %r337,%r321,%r335;
}

	
	{xor.b32 %r340,%r337,0x80008000;
}

	
	{fma.rn.f16x2 %r342,%r318,%r333,%r340;
}

	
	{mul.f16x2 %r346,%r318,%r335;
}

	
	{fma.rn.f16x2 %r349,%r321,%r333,%r346;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r330;
mov.b32 %r353, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r330;
mov.b32 %r355, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r357, {low,high};}


	
	{mul.f16x2 %r358,%r355,%r357;
}

	
	{mul.f16x2 %r361,%r330,%r353;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r330;
mov.b32 %r364, {high,low};}


	
	{fma.rn.f16x2 %r366,%r358,%r364,%r361;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r366;
mov.b32 %r370, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r366;
mov.b32 %r372, {high,high};}


	
	{mul.f16x2 %r374,%r315,%r372;
}

	
	{xor.b32 %r377,%r374,0x80008000;
}

	
	{fma.rn.f16x2 %r379,%r312,%r370,%r377;
}

	
	{mul.f16x2 %r383,%r312,%r372;
}

	
	{fma.rn.f16x2 %r386,%r315,%r370,%r383;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r330;
mov.b32 %r390, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r330;
mov.b32 %r392, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r394, {low,high};}


	
	{mul.f16x2 %r395,%r392,%r394;
}

	
	{mul.f16x2 %r398,%r366,%r390;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r366;
mov.b32 %r401, {high,low};}


	
	{fma.rn.f16x2 %r403,%r395,%r401,%r398;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r403;
mov.b32 %r407, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r403;
mov.b32 %r409, {high,high};}


	
	{mul.f16x2 %r411,%r327,%r409;
}

	
	{xor.b32 %r414,%r411,0x80008000;
}

	
	{fma.rn.f16x2 %r416,%r324,%r407,%r414;
}

	
	{mul.f16x2 %r420,%r324,%r409;
}

	
	{fma.rn.f16x2 %r423,%r327,%r407,%r420;
}

	and.b32 %r445, %r5, 3;
add.s32 %r48, %r32, %r445;
barrier.sync 0;
shl.b32 %r446, %r41, 2;
add.s32 %r447, %r446, %r48;
shl.b32 %r448, %r447, 2;
add.s32 %r49, %r103, %r448;
st.shared.u32 [%r49], %r306;
st.shared.u32 [%r49+16], %r342;
st.shared.u32 [%r49+32], %r379;
st.shared.u32 [%r49+48], %r416;
barrier.sync 0;
add.s32 %r450, %r41, %r48;
shl.b32 %r451, %r450, 2;
add.s32 %r50, %r103, %r451;
ld.shared.u32 %r51, [%r50];
ld.shared.u32 %r52, [%r50+64];
ld.shared.u32 %r53, [%r50+128];
ld.shared.u32 %r54, [%r50+192];
barrier.sync 0;
st.shared.u32 [%r49], %r309;
st.shared.u32 [%r49+16], %r349;
st.shared.u32 [%r49+32], %r386;
st.shared.u32 [%r49+48], %r423;
barrier.sync 0;
ld.shared.u32 %r457, [%r50];
ld.shared.u32 %r469, [%r50+64];
ld.shared.u32 %r458, [%r50+128];
ld.shared.u32 %r470, [%r50+192];

	{add.f16x2 %r453,%r51,%r53;
}

	
	{add.f16x2 %r456,%r457,%r458;
}

	
	{sub.f16x2 %r459,%r51,%r53;
}

	
	{sub.f16x2 %r462,%r457,%r458;
}

	
	{add.f16x2 %r465,%r52,%r54;
}

	
	{add.f16x2 %r468,%r469,%r470;
}

	
	{sub.f16x2 %r471,%r52,%r54;
}

	
	{sub.f16x2 %r474,%r469,%r470;
}

	
	{xor.b32 %r477,%r471,0x80008000;
}

	
	{add.f16x2 %r479,%r453,%r465;
}

	
	{add.f16x2 %r482,%r456,%r468;
}

	
	{sub.f16x2 %r485,%r453,%r465;
}

	
	{sub.f16x2 %r488,%r456,%r468;
}

	
	{add.f16x2 %r491,%r459,%r474;
}

	
	{add.f16x2 %r494,%r462,%r477;
}

	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r479;
mov.b32 {blow,bhigh}, %r482;
mov.b32 %r503, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r479;
mov.b32 {blow,bhigh}, %r482;
mov.b32 %r506, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r491;
mov.b32 {blow,bhigh}, %r494;
mov.b32 %r509, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r491;
mov.b32 {blow,bhigh}, %r494;
mov.b32 %r512, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r485;
mov.b32 {blow,bhigh}, %r488;
mov.b32 %r515, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r485;
mov.b32 {blow,bhigh}, %r488;
mov.b32 %r518, {ahigh,bhigh};}


	mad.lo.s32 %r527, %r2, 66, %r5;
cvta.to.global.u64 %rd9, %rd5;
mul.wide.u32 %rd10, %r527, 4;
add.s64 %rd3, %rd9, %rd10;
@%p2 bra BB5_11;
bra.uni BB5_5;

BB5_11:
st.global.u32 [%rd3], %r503;
st.global.u32 [%rd3+64], %r509;
setp.gt.u32	%p9, %r20, 32;
@%p9 bra BB5_13;

st.global.u32 [%rd3+128], %r515;

BB5_13:
st.global.u32 [%rd3+132], %r506;
st.global.u32 [%rd3+196], %r512;
@%p9 bra BB5_15;
bra.uni BB5_14;

BB5_5:
shl.b32 %r528, %r3, 1;
add.s32 %r529, %r528, -1;
setp.lt.u32	%p5, %r529, %r1;
st.global.u32 [%rd3], %r503;
st.global.u32 [%rd3+64], %r509;
@%p5 bra BB5_8;
bra.uni BB5_6;

BB5_8:
setp.gt.u32	%p7, %r20, 32;
@%p7 bra BB5_10;

st.global.u32 [%rd3+128], %r515;

BB5_10:
st.global.u32 [%rd3+132], %r506;
st.global.u32 [%rd3+196], %r512;
@%p7 bra BB5_15;

BB5_14:
st.global.u32 [%rd3+260], %r518;
bra.uni BB5_15;

BB5_6:
setp.gt.u32	%p6, %r20, 32;
@%p6 bra BB5_15;

st.global.u32 [%rd3+128], %r515;

BB5_15:
ret;
}


.weak .entry _Z14vector_fft_r2cILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 32, 1, 1
{
.reg .pred %p<11>;
.reg .b16 %rs<9>;
.reg .f32 %f<41>;
.reg .b32 %r<698>;
.reg .f64 %fd<2>;
.reg .b64 %rd<11>;


ld.param.u32 %r1, [_Z14vector_fft_r2cILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z14vector_fft_r2cILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj128ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
setp.eq.s16	%p1, %rs1, 0;
selp.b32	%r98, 64, 65, %p1;
mov.u32 %r2, %ctaid.x;
shl.b32 %r99, %r2, 1;
add.s32 %r100, %r99, 1;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
mad.lo.s32 %r101, %r99, %r98, %r5;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r101, 4;
add.s64 %rd1, %rd6, %rd7;
mad.lo.s32 %r102, %r100, %r98, %r5;
mul.wide.u32 %rd8, %r102, 4;
add.s64 %rd2, %rd6, %rd8;
@%p2 bra BB6_2;
bra.uni BB6_1;

BB6_2:
ld.global.u32 %r697, [%rd1];
ld.global.u32 %r695, [%rd1+128];
bra.uni BB6_3;

BB6_1:
shl.b32 %r104, %r3, 1;
add.s32 %r105, %r104, -1;
ld.global.u32 %r697, [%rd1];
ld.global.u32 %r695, [%rd1+128];
setp.ge.u32	%p3, %r105, %r1;
@%p3 bra BB6_4;

BB6_3:
ld.global.u32 %r696, [%rd2];
ld.global.u32 %r694, [%rd2+128];

BB6_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r697;
mov.b32 {blow,bhigh}, %r696;
mov.b32 %r106, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r697;
mov.b32 {blow,bhigh}, %r696;
mov.b32 %r109, {ahigh,bhigh};}


	shl.b32 %r118, %r5, 3;
mov.u32 %r119, smem_full;
add.s32 %r120, %r119, %r118;
st.shared.u32 [%r120], %r106;
st.shared.u32 [%r120+4], %r109;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r695;
mov.b32 {blow,bhigh}, %r694;
mov.b32 %r112, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r695;
mov.b32 {blow,bhigh}, %r694;
mov.b32 %r115, {ahigh,bhigh};}


	st.shared.u32 [%r120+256], %r112;
st.shared.u32 [%r120+260], %r115;
barrier.sync 0;
shl.b32 %r121, %r5, 2;
add.s32 %r123, %r119, %r121;
ld.shared.u32 %r18, [%r123];
ld.shared.u32 %r19, [%r123+128];
add.s32 %r20, %r5, 64;
ld.shared.u32 %r21, [%r123+256];
ld.shared.u32 %r22, [%r123+384];
barrier.sync 0;

	{add.f16x2 %r124,%r18,%r21;
}

	mov.u32 %r147, 0;

	{add.f16x2 %r127,%r147,%r147;
}

	
	{sub.f16x2 %r130,%r18,%r21;
}

	
	{sub.f16x2 %r133,%r147,%r147;
}

	
	{add.f16x2 %r136,%r19,%r22;
}

	
	{add.f16x2 %r139,%r147,%r147;
}

	
	{sub.f16x2 %r142,%r19,%r22;
}

	
	{sub.f16x2 %r145,%r147,%r147;
}

	
	{xor.b32 %r148,%r142,0x80008000;
}

	
	{add.f16x2 %r150,%r124,%r136;
}

	
	{add.f16x2 %r153,%r127,%r139;
}

	
	{sub.f16x2 %r156,%r124,%r136;
}

	
	{sub.f16x2 %r159,%r127,%r139;
}

	
	{add.f16x2 %r162,%r130,%r145;
}

	
	{add.f16x2 %r165,%r133,%r148;
}

	
	{sub.f16x2 %r168,%r130,%r145;
}

	
	{sub.f16x2 %r171,%r133,%r148;
}

	and.b32 %r25, %r5, 31;
cvt.rn.f32.u32	%f12, %r25;
mul.f32 %f13, %f12, 0f3D490FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r174, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r177, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r179, {high,high};}


	
	{mul.f16x2 %r181,%r165,%r179;
}

	
	{xor.b32 %r184,%r181,0x80008000;
}

	
	{fma.rn.f16x2 %r186,%r162,%r177,%r184;
}

	
	{mul.f16x2 %r190,%r162,%r179;
}

	
	{fma.rn.f16x2 %r193,%r165,%r177,%r190;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r197, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r199, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r201, {low,high};}


	
	{mul.f16x2 %r202,%r199,%r201;
}

	
	{mul.f16x2 %r205,%r174,%r197;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r208, {high,low};}


	
	{fma.rn.f16x2 %r210,%r202,%r208,%r205;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r210;
mov.b32 %r214, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r210;
mov.b32 %r216, {high,high};}


	
	{mul.f16x2 %r218,%r159,%r216;
}

	
	{xor.b32 %r221,%r218,0x80008000;
}

	
	{fma.rn.f16x2 %r223,%r156,%r214,%r221;
}

	
	{mul.f16x2 %r227,%r156,%r216;
}

	
	{fma.rn.f16x2 %r230,%r159,%r214,%r227;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r234, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r236, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r238, {low,high};}


	
	{mul.f16x2 %r239,%r236,%r238;
}

	
	{mul.f16x2 %r242,%r210,%r234;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r210;
mov.b32 %r245, {high,low};}


	
	{fma.rn.f16x2 %r247,%r239,%r245,%r242;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r247;
mov.b32 %r251, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r247;
mov.b32 %r253, {high,high};}


	
	{mul.f16x2 %r255,%r171,%r253;
}

	
	{xor.b32 %r258,%r255,0x80008000;
}

	
	{fma.rn.f16x2 %r260,%r168,%r251,%r258;
}

	
	{mul.f16x2 %r264,%r168,%r253;
}

	
	{fma.rn.f16x2 %r267,%r171,%r251,%r264;
}

	and.b32 %r32, %r121, -128;
barrier.sync 0;
shl.b32 %r289, %r25, 2;
add.s32 %r290, %r289, %r32;
shl.b32 %r291, %r290, 2;
add.s32 %r33, %r119, %r291;
st.shared.u32 [%r33], %r150;
st.shared.u32 [%r33+4], %r186;
st.shared.u32 [%r33+8], %r223;
st.shared.u32 [%r33+12], %r260;
barrier.sync 0;
add.s32 %r293, %r25, %r32;
shl.b32 %r294, %r293, 2;
add.s32 %r34, %r119, %r294;
ld.shared.u32 %r35, [%r34];
ld.shared.u32 %r36, [%r34+128];
ld.shared.u32 %r37, [%r34+256];
ld.shared.u32 %r38, [%r34+384];
barrier.sync 0;
st.shared.u32 [%r33], %r153;
st.shared.u32 [%r33+4], %r193;
st.shared.u32 [%r33+8], %r230;
st.shared.u32 [%r33+12], %r267;
barrier.sync 0;
ld.shared.u32 %r300, [%r34];
ld.shared.u32 %r312, [%r34+128];
ld.shared.u32 %r301, [%r34+256];
ld.shared.u32 %r313, [%r34+384];

	{add.f16x2 %r296,%r35,%r37;
}

	
	{add.f16x2 %r299,%r300,%r301;
}

	
	{sub.f16x2 %r302,%r35,%r37;
}

	
	{sub.f16x2 %r305,%r300,%r301;
}

	
	{add.f16x2 %r308,%r36,%r38;
}

	
	{add.f16x2 %r311,%r312,%r313;
}

	
	{sub.f16x2 %r314,%r36,%r38;
}

	
	{sub.f16x2 %r317,%r312,%r313;
}

	
	{xor.b32 %r320,%r314,0x80008000;
}

	
	{add.f16x2 %r322,%r296,%r308;
}

	
	{add.f16x2 %r325,%r299,%r311;
}

	
	{sub.f16x2 %r328,%r296,%r308;
}

	
	{sub.f16x2 %r331,%r299,%r311;
}

	
	{add.f16x2 %r334,%r302,%r317;
}

	
	{add.f16x2 %r337,%r305,%r320;
}

	
	{sub.f16x2 %r340,%r302,%r317;
}

	
	{sub.f16x2 %r343,%r305,%r320;
}

	shr.u32 %r41, %r25, 2;
cvt.rn.f32.u32	%f25, %r41;
mul.f32 %f26, %f25, 0f3E490FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r346, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r349, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r351, {high,high};}


	
	{mul.f16x2 %r353,%r337,%r351;
}

	
	{xor.b32 %r356,%r353,0x80008000;
}

	
	{fma.rn.f16x2 %r358,%r334,%r349,%r356;
}

	
	{mul.f16x2 %r362,%r334,%r351;
}

	
	{fma.rn.f16x2 %r365,%r337,%r349,%r362;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r369, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r371, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r373, {low,high};}


	
	{mul.f16x2 %r374,%r371,%r373;
}

	
	{mul.f16x2 %r377,%r346,%r369;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r380, {high,low};}


	
	{fma.rn.f16x2 %r382,%r374,%r380,%r377;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r386, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r388, {high,high};}


	
	{mul.f16x2 %r390,%r331,%r388;
}

	
	{xor.b32 %r393,%r390,0x80008000;
}

	
	{fma.rn.f16x2 %r395,%r328,%r386,%r393;
}

	
	{mul.f16x2 %r399,%r328,%r388;
}

	
	{fma.rn.f16x2 %r402,%r331,%r386,%r399;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r406, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r408, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r410, {low,high};}


	
	{mul.f16x2 %r411,%r408,%r410;
}

	
	{mul.f16x2 %r414,%r382,%r406;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r417, {high,low};}


	
	{fma.rn.f16x2 %r419,%r411,%r417,%r414;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r419;
mov.b32 %r423, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r419;
mov.b32 %r425, {high,high};}


	
	{mul.f16x2 %r427,%r343,%r425;
}

	
	{xor.b32 %r430,%r427,0x80008000;
}

	
	{fma.rn.f16x2 %r432,%r340,%r423,%r430;
}

	
	{mul.f16x2 %r436,%r340,%r425;
}

	
	{fma.rn.f16x2 %r439,%r343,%r423,%r436;
}

	and.b32 %r460, %r5, 3;
add.s32 %r48, %r32, %r460;
barrier.sync 0;
shl.b32 %r461, %r41, 4;
add.s32 %r462, %r461, %r48;
shl.b32 %r463, %r462, 2;
add.s32 %r49, %r119, %r463;
st.shared.u32 [%r49], %r322;
st.shared.u32 [%r49+16], %r358;
st.shared.u32 [%r49+32], %r395;
st.shared.u32 [%r49+48], %r432;
barrier.sync 0;
shl.b32 %r465, %r41, 2;
add.s32 %r466, %r465, %r48;
shl.b32 %r467, %r466, 2;
add.s32 %r50, %r119, %r467;
ld.shared.u32 %r51, [%r50];
ld.shared.u32 %r52, [%r50+128];
ld.shared.u32 %r53, [%r50+256];
ld.shared.u32 %r54, [%r50+384];
barrier.sync 0;
st.shared.u32 [%r49], %r325;
st.shared.u32 [%r49+16], %r365;
st.shared.u32 [%r49+32], %r402;
st.shared.u32 [%r49+48], %r439;
barrier.sync 0;
ld.shared.u32 %r473, [%r50];
ld.shared.u32 %r485, [%r50+128];
ld.shared.u32 %r474, [%r50+256];
ld.shared.u32 %r486, [%r50+384];

	{add.f16x2 %r469,%r51,%r53;
}

	
	{add.f16x2 %r472,%r473,%r474;
}

	
	{sub.f16x2 %r475,%r51,%r53;
}

	
	{sub.f16x2 %r478,%r473,%r474;
}

	
	{add.f16x2 %r481,%r52,%r54;
}

	
	{add.f16x2 %r484,%r485,%r486;
}

	
	{sub.f16x2 %r487,%r52,%r54;
}

	
	{sub.f16x2 %r490,%r485,%r486;
}

	
	{xor.b32 %r493,%r487,0x80008000;
}

	
	{add.f16x2 %r495,%r469,%r481;
}

	
	{add.f16x2 %r498,%r472,%r484;
}

	
	{sub.f16x2 %r501,%r469,%r481;
}

	
	{sub.f16x2 %r504,%r472,%r484;
}

	
	{add.f16x2 %r507,%r475,%r490;
}

	
	{add.f16x2 %r510,%r478,%r493;
}

	
	{sub.f16x2 %r513,%r475,%r490;
}

	
	{sub.f16x2 %r516,%r478,%r493;
}

	shr.u32 %r57, %r25, 4;
cvt.rn.f32.u32	%f38, %r57;
mul.f32 %f39, %f38, 0f3F490FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r519, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r522, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r524, {high,high};}


	
	{mul.f16x2 %r526,%r510,%r524;
}

	
	{xor.b32 %r529,%r526,0x80008000;
}

	
	{fma.rn.f16x2 %r531,%r507,%r522,%r529;
}

	
	{mul.f16x2 %r535,%r507,%r524;
}

	
	{fma.rn.f16x2 %r538,%r510,%r522,%r535;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r542, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r544, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r546, {low,high};}


	
	{mul.f16x2 %r547,%r544,%r546;
}

	
	{mul.f16x2 %r550,%r519,%r542;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r553, {high,low};}


	
	{fma.rn.f16x2 %r555,%r547,%r553,%r550;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r559, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r561, {high,high};}


	
	{mul.f16x2 %r563,%r504,%r561;
}

	
	{xor.b32 %r566,%r563,0x80008000;
}

	
	{fma.rn.f16x2 %r568,%r501,%r559,%r566;
}

	
	{mul.f16x2 %r572,%r501,%r561;
}

	
	{fma.rn.f16x2 %r575,%r504,%r559,%r572;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r579, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r581, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r583, {low,high};}


	
	{mul.f16x2 %r584,%r581,%r583;
}

	
	{mul.f16x2 %r587,%r555,%r579;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r590, {high,low};}


	
	{fma.rn.f16x2 %r592,%r584,%r590,%r587;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r592;
mov.b32 %r596, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r592;
mov.b32 %r598, {high,high};}


	
	{mul.f16x2 %r600,%r516,%r598;
}

	
	{xor.b32 %r603,%r600,0x80008000;
}

	
	{fma.rn.f16x2 %r605,%r513,%r596,%r603;
}

	
	{mul.f16x2 %r609,%r513,%r598;
}

	
	{fma.rn.f16x2 %r612,%r516,%r596,%r609;
}

	and.b32 %r633, %r5, 15;
add.s32 %r64, %r32, %r633;
barrier.sync 0;
shl.b32 %r634, %r57, 6;
add.s32 %r635, %r634, %r64;
shl.b32 %r636, %r635, 2;
add.s32 %r65, %r119, %r636;
st.shared.u32 [%r65], %r495;
st.shared.u32 [%r65+64], %r531;
st.shared.u32 [%r65+128], %r568;
st.shared.u32 [%r65+192], %r605;
barrier.sync 0;
shl.b32 %r638, %r57, 4;
add.s32 %r639, %r638, %r64;
shl.b32 %r640, %r639, 2;
add.s32 %r66, %r119, %r640;
ld.shared.u32 %r67, [%r66];
ld.shared.u32 %r68, [%r66+128];
ld.shared.u32 %r69, [%r66+256];
ld.shared.u32 %r70, [%r66+384];
barrier.sync 0;
st.shared.u32 [%r65], %r498;
st.shared.u32 [%r65+64], %r538;
st.shared.u32 [%r65+128], %r575;
st.shared.u32 [%r65+192], %r612;
barrier.sync 0;
ld.shared.u32 %r646, [%r66];
ld.shared.u32 %r658, [%r66+128];
ld.shared.u32 %r647, [%r66+256];
ld.shared.u32 %r659, [%r66+384];

	{add.f16x2 %r642,%r67,%r69;
}

	
	{add.f16x2 %r645,%r646,%r647;
}

	
	{sub.f16x2 %r648,%r67,%r69;
}

	
	{sub.f16x2 %r651,%r646,%r647;
}

	
	{add.f16x2 %r654,%r68,%r70;
}

	
	{add.f16x2 %r657,%r658,%r659;
}

	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r642;
mov.b32 {blow,bhigh}, %r645;
mov.b32 %r666, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r642;
mov.b32 {blow,bhigh}, %r645;
mov.b32 %r669, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r654;
mov.b32 {blow,bhigh}, %r657;
mov.b32 %r672, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r654;
mov.b32 {blow,bhigh}, %r657;
mov.b32 %r675, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r648;
mov.b32 {blow,bhigh}, %r651;
mov.b32 %r678, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r648;
mov.b32 {blow,bhigh}, %r651;
mov.b32 %r681, {ahigh,bhigh};}


	mad.lo.s32 %r690, %r2, 130, %r5;
cvta.to.global.u64 %rd9, %rd5;
mul.wide.u32 %rd10, %r690, 4;
add.s64 %rd3, %rd9, %rd10;
@%p2 bra BB6_11;
bra.uni BB6_5;

BB6_11:
st.global.u32 [%rd3], %r666;
st.global.u32 [%rd3+128], %r672;
setp.gt.u32	%p9, %r20, 64;
@%p9 bra BB6_13;

st.global.u32 [%rd3+256], %r678;

BB6_13:
st.global.u32 [%rd3+260], %r669;
st.global.u32 [%rd3+388], %r675;
@%p9 bra BB6_15;
bra.uni BB6_14;

BB6_5:
shl.b32 %r692, %r3, 1;
add.s32 %r693, %r692, -1;
setp.lt.u32	%p5, %r693, %r1;
st.global.u32 [%rd3], %r666;
st.global.u32 [%rd3+128], %r672;
@%p5 bra BB6_8;
bra.uni BB6_6;

BB6_8:
setp.gt.u32	%p7, %r20, 64;
@%p7 bra BB6_10;

st.global.u32 [%rd3+256], %r678;

BB6_10:
st.global.u32 [%rd3+260], %r669;
st.global.u32 [%rd3+388], %r675;
@%p7 bra BB6_15;

BB6_14:
st.global.u32 [%rd3+516], %r681;
bra.uni BB6_15;

BB6_6:
setp.gt.u32	%p6, %r20, 64;
@%p6 bra BB6_15;

st.global.u32 [%rd3+256], %r678;

BB6_15:
ret;
}


.weak .entry _Z14vector_fft_r2cILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 64, 1, 1
{
.reg .pred %p<11>;
.reg .b16 %rs<9>;
.reg .f32 %f<41>;
.reg .b32 %r<723>;
.reg .f64 %fd<2>;
.reg .b64 %rd<11>;


ld.param.u32 %r1, [_Z14vector_fft_r2cILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z14vector_fft_r2cILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj256ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
setp.eq.s16	%p1, %rs1, 0;
selp.b32	%r98, 128, 129, %p1;
mov.u32 %r2, %ctaid.x;
shl.b32 %r99, %r2, 1;
add.s32 %r100, %r99, 1;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
mad.lo.s32 %r101, %r99, %r98, %r5;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r101, 4;
add.s64 %rd1, %rd6, %rd7;
mad.lo.s32 %r102, %r100, %r98, %r5;
mul.wide.u32 %rd8, %r102, 4;
add.s64 %rd2, %rd6, %rd8;
@%p2 bra BB7_2;
bra.uni BB7_1;

BB7_2:
ld.global.u32 %r722, [%rd1];
ld.global.u32 %r720, [%rd1+256];
bra.uni BB7_3;

BB7_1:
shl.b32 %r104, %r3, 1;
add.s32 %r105, %r104, -1;
ld.global.u32 %r722, [%rd1];
ld.global.u32 %r720, [%rd1+256];
setp.ge.u32	%p3, %r105, %r1;
@%p3 bra BB7_4;

BB7_3:
ld.global.u32 %r721, [%rd2];
ld.global.u32 %r719, [%rd2+256];

BB7_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r722;
mov.b32 {blow,bhigh}, %r721;
mov.b32 %r106, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r722;
mov.b32 {blow,bhigh}, %r721;
mov.b32 %r109, {ahigh,bhigh};}


	shl.b32 %r118, %r5, 3;
mov.u32 %r119, smem_full;
add.s32 %r120, %r119, %r118;
st.shared.u32 [%r120], %r106;
st.shared.u32 [%r120+4], %r109;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r720;
mov.b32 {blow,bhigh}, %r719;
mov.b32 %r112, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r720;
mov.b32 {blow,bhigh}, %r719;
mov.b32 %r115, {ahigh,bhigh};}


	st.shared.u32 [%r120+512], %r112;
st.shared.u32 [%r120+516], %r115;
barrier.sync 0;
shl.b32 %r121, %r5, 2;
add.s32 %r123, %r119, %r121;
ld.shared.u32 %r18, [%r123];
ld.shared.u32 %r19, [%r123+256];
add.s32 %r20, %r5, 128;
ld.shared.u32 %r21, [%r123+512];
ld.shared.u32 %r22, [%r123+768];
barrier.sync 0;

	{add.f16x2 %r124,%r18,%r21;
}

	mov.u32 %r147, 0;

	{add.f16x2 %r127,%r147,%r147;
}

	
	{sub.f16x2 %r130,%r18,%r21;
}

	
	{sub.f16x2 %r133,%r147,%r147;
}

	
	{add.f16x2 %r136,%r19,%r22;
}

	
	{add.f16x2 %r139,%r147,%r147;
}

	
	{sub.f16x2 %r142,%r19,%r22;
}

	
	{sub.f16x2 %r145,%r147,%r147;
}

	
	{xor.b32 %r148,%r142,0x80008000;
}

	
	{add.f16x2 %r150,%r124,%r136;
}

	
	{add.f16x2 %r153,%r127,%r139;
}

	
	{sub.f16x2 %r156,%r124,%r136;
}

	
	{sub.f16x2 %r159,%r127,%r139;
}

	
	{add.f16x2 %r162,%r130,%r145;
}

	
	{add.f16x2 %r165,%r133,%r148;
}

	
	{sub.f16x2 %r168,%r130,%r145;
}

	
	{sub.f16x2 %r171,%r133,%r148;
}

	and.b32 %r25, %r5, 63;
cvt.rn.f32.u32	%f12, %r25;
mul.f32 %f13, %f12, 0f3CC90FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r174, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r177, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r179, {high,high};}


	
	{mul.f16x2 %r181,%r165,%r179;
}

	
	{xor.b32 %r184,%r181,0x80008000;
}

	
	{fma.rn.f16x2 %r186,%r162,%r177,%r184;
}

	
	{mul.f16x2 %r190,%r162,%r179;
}

	
	{fma.rn.f16x2 %r193,%r165,%r177,%r190;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r197, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r199, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r201, {low,high};}


	
	{mul.f16x2 %r202,%r199,%r201;
}

	
	{mul.f16x2 %r205,%r174,%r197;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r208, {high,low};}


	
	{fma.rn.f16x2 %r210,%r202,%r208,%r205;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r210;
mov.b32 %r214, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r210;
mov.b32 %r216, {high,high};}


	
	{mul.f16x2 %r218,%r159,%r216;
}

	
	{xor.b32 %r221,%r218,0x80008000;
}

	
	{fma.rn.f16x2 %r223,%r156,%r214,%r221;
}

	
	{mul.f16x2 %r227,%r156,%r216;
}

	
	{fma.rn.f16x2 %r230,%r159,%r214,%r227;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r234, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r174;
mov.b32 %r236, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r238, {low,high};}


	
	{mul.f16x2 %r239,%r236,%r238;
}

	
	{mul.f16x2 %r242,%r210,%r234;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r210;
mov.b32 %r245, {high,low};}


	
	{fma.rn.f16x2 %r247,%r239,%r245,%r242;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r247;
mov.b32 %r251, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r247;
mov.b32 %r253, {high,high};}


	
	{mul.f16x2 %r255,%r171,%r253;
}

	
	{xor.b32 %r258,%r255,0x80008000;
}

	
	{fma.rn.f16x2 %r260,%r168,%r251,%r258;
}

	
	{mul.f16x2 %r264,%r168,%r253;
}

	
	{fma.rn.f16x2 %r267,%r171,%r251,%r264;
}

	and.b32 %r32, %r121, -256;
barrier.sync 0;
shl.b32 %r289, %r25, 2;
add.s32 %r290, %r289, %r32;
shl.b32 %r291, %r290, 2;
add.s32 %r33, %r119, %r291;
st.shared.u32 [%r33], %r150;
st.shared.u32 [%r33+4], %r186;
st.shared.u32 [%r33+8], %r223;
st.shared.u32 [%r33+12], %r260;
barrier.sync 0;
add.s32 %r293, %r25, %r32;
shl.b32 %r294, %r293, 2;
add.s32 %r34, %r119, %r294;
ld.shared.u32 %r35, [%r34];
ld.shared.u32 %r36, [%r34+256];
ld.shared.u32 %r37, [%r34+512];
ld.shared.u32 %r38, [%r34+768];
barrier.sync 0;
st.shared.u32 [%r33], %r153;
st.shared.u32 [%r33+4], %r193;
st.shared.u32 [%r33+8], %r230;
st.shared.u32 [%r33+12], %r267;
barrier.sync 0;
ld.shared.u32 %r300, [%r34];
ld.shared.u32 %r312, [%r34+256];
ld.shared.u32 %r301, [%r34+512];
ld.shared.u32 %r313, [%r34+768];

	{add.f16x2 %r296,%r35,%r37;
}

	
	{add.f16x2 %r299,%r300,%r301;
}

	
	{sub.f16x2 %r302,%r35,%r37;
}

	
	{sub.f16x2 %r305,%r300,%r301;
}

	
	{add.f16x2 %r308,%r36,%r38;
}

	
	{add.f16x2 %r311,%r312,%r313;
}

	
	{sub.f16x2 %r314,%r36,%r38;
}

	
	{sub.f16x2 %r317,%r312,%r313;
}

	
	{xor.b32 %r320,%r314,0x80008000;
}

	
	{add.f16x2 %r322,%r296,%r308;
}

	
	{add.f16x2 %r325,%r299,%r311;
}

	
	{sub.f16x2 %r328,%r296,%r308;
}

	
	{sub.f16x2 %r331,%r299,%r311;
}

	
	{add.f16x2 %r334,%r302,%r317;
}

	
	{add.f16x2 %r337,%r305,%r320;
}

	
	{sub.f16x2 %r340,%r302,%r317;
}

	
	{sub.f16x2 %r343,%r305,%r320;
}

	and.b32 %r41, %r5, 60;
bfe.u32 %r460, %r5, 2, 4;
cvt.rn.f32.u32	%f25, %r460;
mul.f32 %f26, %f25, 0f3DC90FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r346, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r349, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r351, {high,high};}


	
	{mul.f16x2 %r353,%r337,%r351;
}

	
	{xor.b32 %r356,%r353,0x80008000;
}

	
	{fma.rn.f16x2 %r358,%r334,%r349,%r356;
}

	
	{mul.f16x2 %r362,%r334,%r351;
}

	
	{fma.rn.f16x2 %r365,%r337,%r349,%r362;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r369, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r371, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r373, {low,high};}


	
	{mul.f16x2 %r374,%r371,%r373;
}

	
	{mul.f16x2 %r377,%r346,%r369;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r380, {high,low};}


	
	{fma.rn.f16x2 %r382,%r374,%r380,%r377;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r386, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r388, {high,high};}


	
	{mul.f16x2 %r390,%r331,%r388;
}

	
	{xor.b32 %r393,%r390,0x80008000;
}

	
	{fma.rn.f16x2 %r395,%r328,%r386,%r393;
}

	
	{mul.f16x2 %r399,%r328,%r388;
}

	
	{fma.rn.f16x2 %r402,%r331,%r386,%r399;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r406, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r346;
mov.b32 %r408, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r410, {low,high};}


	
	{mul.f16x2 %r411,%r408,%r410;
}

	
	{mul.f16x2 %r414,%r382,%r406;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r382;
mov.b32 %r417, {high,low};}


	
	{fma.rn.f16x2 %r419,%r411,%r417,%r414;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r419;
mov.b32 %r423, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r419;
mov.b32 %r425, {high,high};}


	
	{mul.f16x2 %r427,%r343,%r425;
}

	
	{xor.b32 %r430,%r427,0x80008000;
}

	
	{fma.rn.f16x2 %r432,%r340,%r423,%r430;
}

	
	{mul.f16x2 %r436,%r340,%r425;
}

	
	{fma.rn.f16x2 %r439,%r343,%r423,%r436;
}

	and.b32 %r461, %r5, 3;
add.s32 %r48, %r32, %r461;
barrier.sync 0;
shl.b32 %r462, %r41, 2;
add.s32 %r463, %r462, %r48;
shl.b32 %r464, %r463, 2;
add.s32 %r49, %r119, %r464;
st.shared.u32 [%r49], %r322;
st.shared.u32 [%r49+16], %r358;
st.shared.u32 [%r49+32], %r395;
st.shared.u32 [%r49+48], %r432;
barrier.sync 0;
add.s32 %r466, %r41, %r48;
shl.b32 %r467, %r466, 2;
add.s32 %r50, %r119, %r467;
ld.shared.u32 %r51, [%r50];
ld.shared.u32 %r52, [%r50+256];
ld.shared.u32 %r53, [%r50+512];
ld.shared.u32 %r54, [%r50+768];
barrier.sync 0;
st.shared.u32 [%r49], %r325;
st.shared.u32 [%r49+16], %r365;
st.shared.u32 [%r49+32], %r402;
st.shared.u32 [%r49+48], %r439;
barrier.sync 0;
ld.shared.u32 %r473, [%r50];
ld.shared.u32 %r485, [%r50+256];
ld.shared.u32 %r474, [%r50+512];
ld.shared.u32 %r486, [%r50+768];

	{add.f16x2 %r469,%r51,%r53;
}

	
	{add.f16x2 %r472,%r473,%r474;
}

	
	{sub.f16x2 %r475,%r51,%r53;
}

	
	{sub.f16x2 %r478,%r473,%r474;
}

	
	{add.f16x2 %r481,%r52,%r54;
}

	
	{add.f16x2 %r484,%r485,%r486;
}

	
	{sub.f16x2 %r487,%r52,%r54;
}

	
	{sub.f16x2 %r490,%r485,%r486;
}

	
	{xor.b32 %r493,%r487,0x80008000;
}

	
	{add.f16x2 %r495,%r469,%r481;
}

	
	{add.f16x2 %r498,%r472,%r484;
}

	
	{sub.f16x2 %r501,%r469,%r481;
}

	
	{sub.f16x2 %r504,%r472,%r484;
}

	
	{add.f16x2 %r507,%r475,%r490;
}

	
	{add.f16x2 %r510,%r478,%r493;
}

	
	{sub.f16x2 %r513,%r475,%r490;
}

	
	{sub.f16x2 %r516,%r478,%r493;
}

	and.b32 %r57, %r5, 48;
bfe.u32 %r633, %r5, 4, 2;
cvt.rn.f32.u32	%f38, %r633;
mul.f32 %f39, %f38, 0f3EC90FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r519, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r522, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r524, {high,high};}


	
	{mul.f16x2 %r526,%r510,%r524;
}

	
	{xor.b32 %r529,%r526,0x80008000;
}

	
	{fma.rn.f16x2 %r531,%r507,%r522,%r529;
}

	
	{mul.f16x2 %r535,%r507,%r524;
}

	
	{fma.rn.f16x2 %r538,%r510,%r522,%r535;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r542, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r544, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r546, {low,high};}


	
	{mul.f16x2 %r547,%r544,%r546;
}

	
	{mul.f16x2 %r550,%r519,%r542;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r553, {high,low};}


	
	{fma.rn.f16x2 %r555,%r547,%r553,%r550;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r559, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r561, {high,high};}


	
	{mul.f16x2 %r563,%r504,%r561;
}

	
	{xor.b32 %r566,%r563,0x80008000;
}

	
	{fma.rn.f16x2 %r568,%r501,%r559,%r566;
}

	
	{mul.f16x2 %r572,%r501,%r561;
}

	
	{fma.rn.f16x2 %r575,%r504,%r559,%r572;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r579, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r519;
mov.b32 %r581, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r583, {low,high};}


	
	{mul.f16x2 %r584,%r581,%r583;
}

	
	{mul.f16x2 %r587,%r555,%r579;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r555;
mov.b32 %r590, {high,low};}


	
	{fma.rn.f16x2 %r592,%r584,%r590,%r587;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r592;
mov.b32 %r596, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r592;
mov.b32 %r598, {high,high};}


	
	{mul.f16x2 %r600,%r516,%r598;
}

	
	{xor.b32 %r603,%r600,0x80008000;
}

	
	{fma.rn.f16x2 %r605,%r513,%r596,%r603;
}

	
	{mul.f16x2 %r609,%r513,%r598;
}

	
	{fma.rn.f16x2 %r612,%r516,%r596,%r609;
}

	and.b32 %r634, %r5, 15;
add.s32 %r64, %r32, %r634;
barrier.sync 0;
shl.b32 %r635, %r57, 2;
add.s32 %r636, %r635, %r64;
shl.b32 %r637, %r636, 2;
add.s32 %r65, %r119, %r637;
st.shared.u32 [%r65], %r495;
st.shared.u32 [%r65+64], %r531;
st.shared.u32 [%r65+128], %r568;
st.shared.u32 [%r65+192], %r605;
barrier.sync 0;
add.s32 %r639, %r57, %r64;
shl.b32 %r640, %r639, 2;
add.s32 %r66, %r119, %r640;
ld.shared.u32 %r67, [%r66];
ld.shared.u32 %r68, [%r66+256];
ld.shared.u32 %r69, [%r66+512];
ld.shared.u32 %r70, [%r66+768];
barrier.sync 0;
st.shared.u32 [%r65], %r498;
st.shared.u32 [%r65+64], %r538;
st.shared.u32 [%r65+128], %r575;
st.shared.u32 [%r65+192], %r612;
barrier.sync 0;
ld.shared.u32 %r646, [%r66];
ld.shared.u32 %r658, [%r66+256];
ld.shared.u32 %r647, [%r66+512];
ld.shared.u32 %r659, [%r66+768];

	{add.f16x2 %r642,%r67,%r69;
}

	
	{add.f16x2 %r645,%r646,%r647;
}

	
	{sub.f16x2 %r648,%r67,%r69;
}

	
	{sub.f16x2 %r651,%r646,%r647;
}

	
	{add.f16x2 %r654,%r68,%r70;
}

	
	{add.f16x2 %r657,%r658,%r659;
}

	
	{sub.f16x2 %r660,%r68,%r70;
}

	
	{sub.f16x2 %r663,%r658,%r659;
}

	
	{xor.b32 %r666,%r660,0x80008000;
}

	
	{add.f16x2 %r668,%r642,%r654;
}

	
	{add.f16x2 %r671,%r645,%r657;
}

	
	{sub.f16x2 %r674,%r642,%r654;
}

	
	{sub.f16x2 %r677,%r645,%r657;
}

	
	{add.f16x2 %r680,%r648,%r663;
}

	
	{add.f16x2 %r683,%r651,%r666;
}

	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r668;
mov.b32 {blow,bhigh}, %r671;
mov.b32 %r692, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r668;
mov.b32 {blow,bhigh}, %r671;
mov.b32 %r695, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r680;
mov.b32 {blow,bhigh}, %r683;
mov.b32 %r698, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r680;
mov.b32 {blow,bhigh}, %r683;
mov.b32 %r701, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r674;
mov.b32 {blow,bhigh}, %r677;
mov.b32 %r704, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r674;
mov.b32 {blow,bhigh}, %r677;
mov.b32 %r707, {ahigh,bhigh};}


	mad.lo.s32 %r716, %r2, 258, %r5;
cvta.to.global.u64 %rd9, %rd5;
mul.wide.u32 %rd10, %r716, 4;
add.s64 %rd3, %rd9, %rd10;
@%p2 bra BB7_11;
bra.uni BB7_5;

BB7_11:
st.global.u32 [%rd3], %r692;
st.global.u32 [%rd3+256], %r698;
setp.gt.u32	%p9, %r20, 128;
@%p9 bra BB7_13;

st.global.u32 [%rd3+512], %r704;

BB7_13:
st.global.u32 [%rd3+516], %r695;
st.global.u32 [%rd3+772], %r701;
@%p9 bra BB7_15;
bra.uni BB7_14;

BB7_5:
shl.b32 %r717, %r3, 1;
add.s32 %r718, %r717, -1;
setp.lt.u32	%p5, %r718, %r1;
st.global.u32 [%rd3], %r692;
st.global.u32 [%rd3+256], %r698;
@%p5 bra BB7_8;
bra.uni BB7_6;

BB7_8:
setp.gt.u32	%p7, %r20, 128;
@%p7 bra BB7_10;

st.global.u32 [%rd3+512], %r704;

BB7_10:
st.global.u32 [%rd3+516], %r695;
st.global.u32 [%rd3+772], %r701;
@%p7 bra BB7_15;

BB7_14:
st.global.u32 [%rd3+1028], %r707;
bra.uni BB7_15;

BB7_6:
setp.gt.u32	%p6, %r20, 128;
@%p6 bra BB7_15;

st.global.u32 [%rd3+512], %r704;

BB7_15:
ret;
}


.weak .entry _Z14vector_fft_r2cILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 128, 1, 1
{
.reg .pred %p<11>;
.reg .b16 %rs<9>;
.reg .f32 %f<54>;
.reg .b32 %r<886>;
.reg .f64 %fd<2>;
.reg .b64 %rd<11>;


ld.param.u32 %r1, [_Z14vector_fft_r2cILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z14vector_fft_r2cILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj512ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
setp.eq.s16	%p1, %rs1, 0;
selp.b32	%r114, 256, 257, %p1;
mov.u32 %r2, %ctaid.x;
shl.b32 %r115, %r2, 1;
add.s32 %r116, %r115, 1;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
mad.lo.s32 %r117, %r115, %r114, %r5;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r117, 4;
add.s64 %rd1, %rd6, %rd7;
mad.lo.s32 %r118, %r116, %r114, %r5;
mul.wide.u32 %rd8, %r118, 4;
add.s64 %rd2, %rd6, %rd8;
@%p2 bra BB8_2;
bra.uni BB8_1;

BB8_2:
ld.global.u32 %r885, [%rd1];
ld.global.u32 %r883, [%rd1+512];
bra.uni BB8_3;

BB8_1:
shl.b32 %r120, %r3, 1;
add.s32 %r121, %r120, -1;
ld.global.u32 %r885, [%rd1];
ld.global.u32 %r883, [%rd1+512];
setp.ge.u32	%p3, %r121, %r1;
@%p3 bra BB8_4;

BB8_3:
ld.global.u32 %r884, [%rd2];
ld.global.u32 %r882, [%rd2+512];

BB8_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r885;
mov.b32 {blow,bhigh}, %r884;
mov.b32 %r122, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r885;
mov.b32 {blow,bhigh}, %r884;
mov.b32 %r125, {ahigh,bhigh};}


	shl.b32 %r134, %r5, 3;
mov.u32 %r135, smem_full;
add.s32 %r136, %r135, %r134;
st.shared.u32 [%r136], %r122;
st.shared.u32 [%r136+4], %r125;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r883;
mov.b32 {blow,bhigh}, %r882;
mov.b32 %r128, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r883;
mov.b32 {blow,bhigh}, %r882;
mov.b32 %r131, {ahigh,bhigh};}


	st.shared.u32 [%r136+1024], %r128;
st.shared.u32 [%r136+1028], %r131;
barrier.sync 0;
shl.b32 %r137, %r5, 2;
add.s32 %r139, %r135, %r137;
ld.shared.u32 %r18, [%r139];
ld.shared.u32 %r19, [%r139+512];
add.s32 %r20, %r5, 256;
ld.shared.u32 %r21, [%r139+1024];
ld.shared.u32 %r22, [%r139+1536];
barrier.sync 0;

	{add.f16x2 %r140,%r18,%r21;
}

	mov.u32 %r163, 0;

	{add.f16x2 %r143,%r163,%r163;
}

	
	{sub.f16x2 %r146,%r18,%r21;
}

	
	{sub.f16x2 %r149,%r163,%r163;
}

	
	{add.f16x2 %r152,%r19,%r22;
}

	
	{add.f16x2 %r155,%r163,%r163;
}

	
	{sub.f16x2 %r158,%r19,%r22;
}

	
	{sub.f16x2 %r161,%r163,%r163;
}

	
	{xor.b32 %r164,%r158,0x80008000;
}

	
	{add.f16x2 %r166,%r140,%r152;
}

	
	{add.f16x2 %r169,%r143,%r155;
}

	
	{sub.f16x2 %r172,%r140,%r152;
}

	
	{sub.f16x2 %r175,%r143,%r155;
}

	
	{add.f16x2 %r178,%r146,%r161;
}

	
	{add.f16x2 %r181,%r149,%r164;
}

	
	{sub.f16x2 %r184,%r146,%r161;
}

	
	{sub.f16x2 %r187,%r149,%r164;
}

	and.b32 %r25, %r5, 127;
cvt.rn.f32.u32	%f12, %r25;
mul.f32 %f13, %f12, 0f3C490FDB;
cos.approx.f32 %f2, %f13;
sin.approx.f32 %f14, %f13;
neg.f32 %f3, %f14;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f2;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r190, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r190;
mov.b32 %r193, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r190;
mov.b32 %r195, {high,high};}


	
	{mul.f16x2 %r197,%r181,%r195;
}

	
	{xor.b32 %r200,%r197,0x80008000;
}

	
	{fma.rn.f16x2 %r202,%r178,%r193,%r200;
}

	
	{mul.f16x2 %r206,%r178,%r195;
}

	
	{fma.rn.f16x2 %r209,%r181,%r193,%r206;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r190;
mov.b32 %r213, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r190;
mov.b32 %r215, {high,high};}


	mov.f32 %f8, 0fBF800000;
mov.f32 %f9, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r217, {low,high};}


	
	{mul.f16x2 %r218,%r215,%r217;
}

	
	{mul.f16x2 %r221,%r190,%r213;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r190;
mov.b32 %r224, {high,low};}


	
	{fma.rn.f16x2 %r226,%r218,%r224,%r221;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r226;
mov.b32 %r230, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r226;
mov.b32 %r232, {high,high};}


	
	{mul.f16x2 %r234,%r175,%r232;
}

	
	{xor.b32 %r237,%r234,0x80008000;
}

	
	{fma.rn.f16x2 %r239,%r172,%r230,%r237;
}

	
	{mul.f16x2 %r243,%r172,%r232;
}

	
	{fma.rn.f16x2 %r246,%r175,%r230,%r243;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r190;
mov.b32 %r250, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r190;
mov.b32 %r252, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r254, {low,high};}


	
	{mul.f16x2 %r255,%r252,%r254;
}

	
	{mul.f16x2 %r258,%r226,%r250;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r226;
mov.b32 %r261, {high,low};}


	
	{fma.rn.f16x2 %r263,%r255,%r261,%r258;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r263;
mov.b32 %r267, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r263;
mov.b32 %r269, {high,high};}


	
	{mul.f16x2 %r271,%r187,%r269;
}

	
	{xor.b32 %r274,%r271,0x80008000;
}

	
	{fma.rn.f16x2 %r276,%r184,%r267,%r274;
}

	
	{mul.f16x2 %r280,%r184,%r269;
}

	
	{fma.rn.f16x2 %r283,%r187,%r267,%r280;
}

	and.b32 %r32, %r137, -512;
barrier.sync 0;
shl.b32 %r305, %r25, 2;
add.s32 %r306, %r305, %r32;
shl.b32 %r307, %r306, 2;
add.s32 %r33, %r135, %r307;
st.shared.u32 [%r33], %r166;
st.shared.u32 [%r33+4], %r202;
st.shared.u32 [%r33+8], %r239;
st.shared.u32 [%r33+12], %r276;
barrier.sync 0;
add.s32 %r309, %r25, %r32;
shl.b32 %r310, %r309, 2;
add.s32 %r34, %r135, %r310;
ld.shared.u32 %r35, [%r34];
ld.shared.u32 %r36, [%r34+512];
ld.shared.u32 %r37, [%r34+1024];
ld.shared.u32 %r38, [%r34+1536];
barrier.sync 0;
st.shared.u32 [%r33], %r169;
st.shared.u32 [%r33+4], %r209;
st.shared.u32 [%r33+8], %r246;
st.shared.u32 [%r33+12], %r283;
barrier.sync 0;
ld.shared.u32 %r316, [%r34];
ld.shared.u32 %r328, [%r34+512];
ld.shared.u32 %r317, [%r34+1024];
ld.shared.u32 %r329, [%r34+1536];

	{add.f16x2 %r312,%r35,%r37;
}

	
	{add.f16x2 %r315,%r316,%r317;
}

	
	{sub.f16x2 %r318,%r35,%r37;
}

	
	{sub.f16x2 %r321,%r316,%r317;
}

	
	{add.f16x2 %r324,%r36,%r38;
}

	
	{add.f16x2 %r327,%r328,%r329;
}

	
	{sub.f16x2 %r330,%r36,%r38;
}

	
	{sub.f16x2 %r333,%r328,%r329;
}

	
	{xor.b32 %r336,%r330,0x80008000;
}

	
	{add.f16x2 %r338,%r312,%r324;
}

	
	{add.f16x2 %r341,%r315,%r327;
}

	
	{sub.f16x2 %r344,%r312,%r324;
}

	
	{sub.f16x2 %r347,%r315,%r327;
}

	
	{add.f16x2 %r350,%r318,%r333;
}

	
	{add.f16x2 %r353,%r321,%r336;
}

	
	{sub.f16x2 %r356,%r318,%r333;
}

	
	{sub.f16x2 %r359,%r321,%r336;
}

	and.b32 %r41, %r5, 124;
bfe.u32 %r476, %r5, 2, 5;
cvt.rn.f32.u32	%f25, %r476;
mul.f32 %f26, %f25, 0f3D490FDB;
cos.approx.f32 %f15, %f26;
sin.approx.f32 %f27, %f26;
neg.f32 %f16, %f27;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f15;
cvt.rn.f16.f32 high, %f16;
mov.b32 %r362, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r365, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r367, {high,high};}


	
	{mul.f16x2 %r369,%r353,%r367;
}

	
	{xor.b32 %r372,%r369,0x80008000;
}

	
	{fma.rn.f16x2 %r374,%r350,%r365,%r372;
}

	
	{mul.f16x2 %r378,%r350,%r367;
}

	
	{fma.rn.f16x2 %r381,%r353,%r365,%r378;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r385, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r387, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r389, {low,high};}


	
	{mul.f16x2 %r390,%r387,%r389;
}

	
	{mul.f16x2 %r393,%r362,%r385;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r396, {high,low};}


	
	{fma.rn.f16x2 %r398,%r390,%r396,%r393;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r402, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r404, {high,high};}


	
	{mul.f16x2 %r406,%r347,%r404;
}

	
	{xor.b32 %r409,%r406,0x80008000;
}

	
	{fma.rn.f16x2 %r411,%r344,%r402,%r409;
}

	
	{mul.f16x2 %r415,%r344,%r404;
}

	
	{fma.rn.f16x2 %r418,%r347,%r402,%r415;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r422, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r424, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r426, {low,high};}


	
	{mul.f16x2 %r427,%r424,%r426;
}

	
	{mul.f16x2 %r430,%r398,%r422;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r433, {high,low};}


	
	{fma.rn.f16x2 %r435,%r427,%r433,%r430;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r435;
mov.b32 %r439, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r435;
mov.b32 %r441, {high,high};}


	
	{mul.f16x2 %r443,%r359,%r441;
}

	
	{xor.b32 %r446,%r443,0x80008000;
}

	
	{fma.rn.f16x2 %r448,%r356,%r439,%r446;
}

	
	{mul.f16x2 %r452,%r356,%r441;
}

	
	{fma.rn.f16x2 %r455,%r359,%r439,%r452;
}

	and.b32 %r477, %r5, 3;
add.s32 %r48, %r32, %r477;
barrier.sync 0;
shl.b32 %r478, %r41, 2;
add.s32 %r479, %r478, %r48;
shl.b32 %r480, %r479, 2;
add.s32 %r49, %r135, %r480;
st.shared.u32 [%r49], %r338;
st.shared.u32 [%r49+16], %r374;
st.shared.u32 [%r49+32], %r411;
st.shared.u32 [%r49+48], %r448;
barrier.sync 0;
add.s32 %r482, %r41, %r48;
shl.b32 %r483, %r482, 2;
add.s32 %r50, %r135, %r483;
ld.shared.u32 %r51, [%r50];
ld.shared.u32 %r52, [%r50+512];
ld.shared.u32 %r53, [%r50+1024];
ld.shared.u32 %r54, [%r50+1536];
barrier.sync 0;
st.shared.u32 [%r49], %r341;
st.shared.u32 [%r49+16], %r381;
st.shared.u32 [%r49+32], %r418;
st.shared.u32 [%r49+48], %r455;
barrier.sync 0;
ld.shared.u32 %r489, [%r50];
ld.shared.u32 %r501, [%r50+512];
ld.shared.u32 %r490, [%r50+1024];
ld.shared.u32 %r502, [%r50+1536];

	{add.f16x2 %r485,%r51,%r53;
}

	
	{add.f16x2 %r488,%r489,%r490;
}

	
	{sub.f16x2 %r491,%r51,%r53;
}

	
	{sub.f16x2 %r494,%r489,%r490;
}

	
	{add.f16x2 %r497,%r52,%r54;
}

	
	{add.f16x2 %r500,%r501,%r502;
}

	
	{sub.f16x2 %r503,%r52,%r54;
}

	
	{sub.f16x2 %r506,%r501,%r502;
}

	
	{xor.b32 %r509,%r503,0x80008000;
}

	
	{add.f16x2 %r511,%r485,%r497;
}

	
	{add.f16x2 %r514,%r488,%r500;
}

	
	{sub.f16x2 %r517,%r485,%r497;
}

	
	{sub.f16x2 %r520,%r488,%r500;
}

	
	{add.f16x2 %r523,%r491,%r506;
}

	
	{add.f16x2 %r526,%r494,%r509;
}

	
	{sub.f16x2 %r529,%r491,%r506;
}

	
	{sub.f16x2 %r532,%r494,%r509;
}

	and.b32 %r57, %r5, 112;
bfe.u32 %r649, %r5, 4, 3;
cvt.rn.f32.u32	%f38, %r649;
mul.f32 %f39, %f38, 0f3E490FDB;
cos.approx.f32 %f28, %f39;
sin.approx.f32 %f40, %f39;
neg.f32 %f29, %f40;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f28;
cvt.rn.f16.f32 high, %f29;
mov.b32 %r535, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r535;
mov.b32 %r538, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r535;
mov.b32 %r540, {high,high};}


	
	{mul.f16x2 %r542,%r526,%r540;
}

	
	{xor.b32 %r545,%r542,0x80008000;
}

	
	{fma.rn.f16x2 %r547,%r523,%r538,%r545;
}

	
	{mul.f16x2 %r551,%r523,%r540;
}

	
	{fma.rn.f16x2 %r554,%r526,%r538,%r551;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r535;
mov.b32 %r558, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r535;
mov.b32 %r560, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r562, {low,high};}


	
	{mul.f16x2 %r563,%r560,%r562;
}

	
	{mul.f16x2 %r566,%r535,%r558;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r535;
mov.b32 %r569, {high,low};}


	
	{fma.rn.f16x2 %r571,%r563,%r569,%r566;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r571;
mov.b32 %r575, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r571;
mov.b32 %r577, {high,high};}


	
	{mul.f16x2 %r579,%r520,%r577;
}

	
	{xor.b32 %r582,%r579,0x80008000;
}

	
	{fma.rn.f16x2 %r584,%r517,%r575,%r582;
}

	
	{mul.f16x2 %r588,%r517,%r577;
}

	
	{fma.rn.f16x2 %r591,%r520,%r575,%r588;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r535;
mov.b32 %r595, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r535;
mov.b32 %r597, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r599, {low,high};}


	
	{mul.f16x2 %r600,%r597,%r599;
}

	
	{mul.f16x2 %r603,%r571,%r595;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r571;
mov.b32 %r606, {high,low};}


	
	{fma.rn.f16x2 %r608,%r600,%r606,%r603;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r608;
mov.b32 %r612, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r608;
mov.b32 %r614, {high,high};}


	
	{mul.f16x2 %r616,%r532,%r614;
}

	
	{xor.b32 %r619,%r616,0x80008000;
}

	
	{fma.rn.f16x2 %r621,%r529,%r612,%r619;
}

	
	{mul.f16x2 %r625,%r529,%r614;
}

	
	{fma.rn.f16x2 %r628,%r532,%r612,%r625;
}

	and.b32 %r650, %r5, 15;
add.s32 %r64, %r32, %r650;
barrier.sync 0;
shl.b32 %r651, %r57, 2;
add.s32 %r652, %r651, %r64;
shl.b32 %r653, %r652, 2;
add.s32 %r65, %r135, %r653;
st.shared.u32 [%r65], %r511;
st.shared.u32 [%r65+64], %r547;
st.shared.u32 [%r65+128], %r584;
st.shared.u32 [%r65+192], %r621;
barrier.sync 0;
add.s32 %r655, %r57, %r64;
shl.b32 %r656, %r655, 2;
add.s32 %r66, %r135, %r656;
ld.shared.u32 %r67, [%r66];
ld.shared.u32 %r68, [%r66+512];
ld.shared.u32 %r69, [%r66+1024];
ld.shared.u32 %r70, [%r66+1536];
barrier.sync 0;
st.shared.u32 [%r65], %r514;
st.shared.u32 [%r65+64], %r554;
st.shared.u32 [%r65+128], %r591;
st.shared.u32 [%r65+192], %r628;
barrier.sync 0;
ld.shared.u32 %r662, [%r66];
ld.shared.u32 %r674, [%r66+512];
ld.shared.u32 %r663, [%r66+1024];
ld.shared.u32 %r675, [%r66+1536];

	{add.f16x2 %r658,%r67,%r69;
}

	
	{add.f16x2 %r661,%r662,%r663;
}

	
	{sub.f16x2 %r664,%r67,%r69;
}

	
	{sub.f16x2 %r667,%r662,%r663;
}

	
	{add.f16x2 %r670,%r68,%r70;
}

	
	{add.f16x2 %r673,%r674,%r675;
}

	
	{sub.f16x2 %r676,%r68,%r70;
}

	
	{sub.f16x2 %r679,%r674,%r675;
}

	
	{xor.b32 %r682,%r676,0x80008000;
}

	
	{add.f16x2 %r684,%r658,%r670;
}

	
	{add.f16x2 %r687,%r661,%r673;
}

	
	{sub.f16x2 %r690,%r658,%r670;
}

	
	{sub.f16x2 %r693,%r661,%r673;
}

	
	{add.f16x2 %r696,%r664,%r679;
}

	
	{add.f16x2 %r699,%r667,%r682;
}

	
	{sub.f16x2 %r702,%r664,%r679;
}

	
	{sub.f16x2 %r705,%r667,%r682;
}

	and.b32 %r73, %r5, 64;
bfe.u32 %r822, %r5, 6, 1;
cvt.rn.f32.u32	%f51, %r822;
mul.f32 %f52, %f51, 0f3F490FDB;
cos.approx.f32 %f41, %f52;
sin.approx.f32 %f53, %f52;
neg.f32 %f42, %f53;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f41;
cvt.rn.f16.f32 high, %f42;
mov.b32 %r708, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r708;
mov.b32 %r711, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r708;
mov.b32 %r713, {high,high};}


	
	{mul.f16x2 %r715,%r699,%r713;
}

	
	{xor.b32 %r718,%r715,0x80008000;
}

	
	{fma.rn.f16x2 %r720,%r696,%r711,%r718;
}

	
	{mul.f16x2 %r724,%r696,%r713;
}

	
	{fma.rn.f16x2 %r727,%r699,%r711,%r724;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r708;
mov.b32 %r731, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r708;
mov.b32 %r733, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r735, {low,high};}


	
	{mul.f16x2 %r736,%r733,%r735;
}

	
	{mul.f16x2 %r739,%r708,%r731;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r708;
mov.b32 %r742, {high,low};}


	
	{fma.rn.f16x2 %r744,%r736,%r742,%r739;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r744;
mov.b32 %r748, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r744;
mov.b32 %r750, {high,high};}


	
	{mul.f16x2 %r752,%r693,%r750;
}

	
	{xor.b32 %r755,%r752,0x80008000;
}

	
	{fma.rn.f16x2 %r757,%r690,%r748,%r755;
}

	
	{mul.f16x2 %r761,%r690,%r750;
}

	
	{fma.rn.f16x2 %r764,%r693,%r748,%r761;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r708;
mov.b32 %r768, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r708;
mov.b32 %r770, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f8;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r772, {low,high};}


	
	{mul.f16x2 %r773,%r770,%r772;
}

	
	{mul.f16x2 %r776,%r744,%r768;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r744;
mov.b32 %r779, {high,low};}


	
	{fma.rn.f16x2 %r781,%r773,%r779,%r776;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r781;
mov.b32 %r785, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r781;
mov.b32 %r787, {high,high};}


	
	{mul.f16x2 %r789,%r705,%r787;
}

	
	{xor.b32 %r792,%r789,0x80008000;
}

	
	{fma.rn.f16x2 %r794,%r702,%r785,%r792;
}

	
	{mul.f16x2 %r798,%r702,%r787;
}

	
	{fma.rn.f16x2 %r801,%r705,%r785,%r798;
}

	and.b32 %r823, %r5, 63;
add.s32 %r80, %r32, %r823;
barrier.sync 0;
shl.b32 %r824, %r73, 2;
add.s32 %r825, %r824, %r80;
shl.b32 %r826, %r825, 2;
add.s32 %r81, %r135, %r826;
st.shared.u32 [%r81], %r684;
st.shared.u32 [%r81+256], %r720;
st.shared.u32 [%r81+512], %r757;
st.shared.u32 [%r81+768], %r794;
barrier.sync 0;
add.s32 %r828, %r73, %r80;
shl.b32 %r829, %r828, 2;
add.s32 %r82, %r135, %r829;
ld.shared.u32 %r83, [%r82];
ld.shared.u32 %r84, [%r82+512];
ld.shared.u32 %r85, [%r82+1024];
ld.shared.u32 %r86, [%r82+1536];
barrier.sync 0;
st.shared.u32 [%r81], %r687;
st.shared.u32 [%r81+256], %r727;
st.shared.u32 [%r81+512], %r764;
st.shared.u32 [%r81+768], %r801;
barrier.sync 0;
ld.shared.u32 %r835, [%r82];
ld.shared.u32 %r847, [%r82+512];
ld.shared.u32 %r836, [%r82+1024];
ld.shared.u32 %r848, [%r82+1536];

	{add.f16x2 %r831,%r83,%r85;
}

	
	{add.f16x2 %r834,%r835,%r836;
}

	
	{sub.f16x2 %r837,%r83,%r85;
}

	
	{sub.f16x2 %r840,%r835,%r836;
}

	
	{add.f16x2 %r843,%r84,%r86;
}

	
	{add.f16x2 %r846,%r847,%r848;
}

	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r831;
mov.b32 {blow,bhigh}, %r834;
mov.b32 %r855, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r831;
mov.b32 {blow,bhigh}, %r834;
mov.b32 %r858, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r843;
mov.b32 {blow,bhigh}, %r846;
mov.b32 %r861, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r843;
mov.b32 {blow,bhigh}, %r846;
mov.b32 %r864, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r837;
mov.b32 {blow,bhigh}, %r840;
mov.b32 %r867, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r837;
mov.b32 {blow,bhigh}, %r840;
mov.b32 %r870, {ahigh,bhigh};}


	mad.lo.s32 %r879, %r2, 514, %r5;
cvta.to.global.u64 %rd9, %rd5;
mul.wide.u32 %rd10, %r879, 4;
add.s64 %rd3, %rd9, %rd10;
@%p2 bra BB8_11;
bra.uni BB8_5;

BB8_11:
st.global.u32 [%rd3], %r855;
st.global.u32 [%rd3+512], %r861;
setp.gt.u32	%p9, %r20, 256;
@%p9 bra BB8_13;

st.global.u32 [%rd3+1024], %r867;

BB8_13:
st.global.u32 [%rd3+1028], %r858;
st.global.u32 [%rd3+1540], %r864;
@%p9 bra BB8_15;
bra.uni BB8_14;

BB8_5:
shl.b32 %r880, %r3, 1;
add.s32 %r881, %r880, -1;
setp.lt.u32	%p5, %r881, %r1;
st.global.u32 [%rd3], %r855;
st.global.u32 [%rd3+512], %r861;
@%p5 bra BB8_8;
bra.uni BB8_6;

BB8_8:
setp.gt.u32	%p7, %r20, 256;
@%p7 bra BB8_10;

st.global.u32 [%rd3+1024], %r867;

BB8_10:
st.global.u32 [%rd3+1028], %r858;
st.global.u32 [%rd3+1540], %r864;
@%p7 bra BB8_15;

BB8_14:
st.global.u32 [%rd3+2052], %r870;
bra.uni BB8_15;

BB8_6:
setp.gt.u32	%p6, %r20, 256;
@%p6 bra BB8_15;

st.global.u32 [%rd3+1024], %r867;

BB8_15:
ret;
}


.weak .entry _Z14vector_fft_r2cILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 256, 1, 1
{
.reg .pred %p<12>;
.reg .b16 %rs<9>;
.reg .f32 %f<58>;
.reg .b32 %r<918>;
.reg .f64 %fd<2>;
.reg .b64 %rd<11>;


ld.param.u32 %r1, [_Z14vector_fft_r2cILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z14vector_fft_r2cILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj1024ELj4ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
setp.eq.s16	%p1, %rs1, 0;
selp.b32	%r120, 512, 513, %p1;
mov.u32 %r2, %ctaid.x;
shl.b32 %r121, %r2, 1;
add.s32 %r122, %r121, 1;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
mad.lo.s32 %r123, %r121, %r120, %r5;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r123, 4;
add.s64 %rd1, %rd6, %rd7;
mad.lo.s32 %r124, %r122, %r120, %r5;
mul.wide.u32 %rd8, %r124, 4;
add.s64 %rd2, %rd6, %rd8;
@%p2 bra BB9_2;
bra.uni BB9_1;

BB9_2:
ld.global.u32 %r917, [%rd1];
ld.global.u32 %r915, [%rd1+1024];
bra.uni BB9_3;

BB9_1:
shl.b32 %r126, %r3, 1;
add.s32 %r127, %r126, -1;
ld.global.u32 %r917, [%rd1];
ld.global.u32 %r915, [%rd1+1024];
setp.ge.u32	%p3, %r127, %r1;
@%p3 bra BB9_4;

BB9_3:
ld.global.u32 %r916, [%rd2];
ld.global.u32 %r914, [%rd2+1024];

BB9_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r917;
mov.b32 {blow,bhigh}, %r916;
mov.b32 %r128, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r917;
mov.b32 {blow,bhigh}, %r916;
mov.b32 %r131, {ahigh,bhigh};}


	shl.b32 %r140, %r5, 3;
mov.u32 %r141, smem_full;
add.s32 %r142, %r141, %r140;
st.shared.u32 [%r142], %r128;
st.shared.u32 [%r142+4], %r131;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r915;
mov.b32 {blow,bhigh}, %r914;
mov.b32 %r134, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r915;
mov.b32 {blow,bhigh}, %r914;
mov.b32 %r137, {ahigh,bhigh};}


	st.shared.u32 [%r142+2048], %r134;
st.shared.u32 [%r142+2052], %r137;
barrier.sync 0;
shl.b32 %r143, %r5, 2;
add.s32 %r145, %r141, %r143;
ld.shared.u32 %r18, [%r145];
ld.shared.u32 %r19, [%r145+1024];
add.s32 %r20, %r5, 512;
ld.shared.u32 %r21, [%r145+2048];
ld.shared.u32 %r22, [%r145+3072];
barrier.sync 0;

	{add.f16x2 %r146,%r18,%r21;
}

	mov.u32 %r169, 0;

	{add.f16x2 %r149,%r169,%r169;
}

	
	{sub.f16x2 %r152,%r18,%r21;
}

	
	{sub.f16x2 %r155,%r169,%r169;
}

	
	{add.f16x2 %r158,%r19,%r22;
}

	
	{add.f16x2 %r161,%r169,%r169;
}

	
	{sub.f16x2 %r164,%r19,%r22;
}

	
	{sub.f16x2 %r167,%r169,%r169;
}

	
	{xor.b32 %r170,%r164,0x80008000;
}

	
	{add.f16x2 %r172,%r146,%r158;
}

	
	{add.f16x2 %r175,%r149,%r161;
}

	
	{sub.f16x2 %r178,%r146,%r158;
}

	
	{sub.f16x2 %r181,%r149,%r161;
}

	
	{add.f16x2 %r184,%r152,%r167;
}

	
	{add.f16x2 %r187,%r155,%r170;
}

	
	{sub.f16x2 %r190,%r152,%r167;
}

	
	{sub.f16x2 %r193,%r155,%r170;
}

	and.b32 %r31, %r5, 255;
cvt.rn.f32.u32	%f6, %r31;
mul.f32 %f1, %f6, 0f3BC90FDB;
setp.eq.s32	%p4, %r31, 255;
mov.f32 %f57, 0f3BC90F88;
@%p4 bra BB9_6;

cos.approx.f32 %f57, %f1;

BB9_6:
and.b32 %r32, %r143, -1024;
sin.approx.f32 %f17, %f1;
neg.f32 %f8, %f17;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f57;
cvt.rn.f16.f32 high, %f8;
mov.b32 %r196, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r196;
mov.b32 %r199, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r196;
mov.b32 %r201, {high,high};}


	
	{mul.f16x2 %r203,%r187,%r201;
}

	
	{xor.b32 %r206,%r203,0x80008000;
}

	
	{fma.rn.f16x2 %r208,%r184,%r199,%r206;
}

	
	{mul.f16x2 %r212,%r184,%r201;
}

	
	{fma.rn.f16x2 %r215,%r187,%r199,%r212;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r196;
mov.b32 %r219, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r196;
mov.b32 %r221, {high,high};}


	mov.f32 %f13, 0fBF800000;
mov.f32 %f14, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r223, {low,high};}


	
	{mul.f16x2 %r224,%r221,%r223;
}

	
	{mul.f16x2 %r227,%r196,%r219;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r196;
mov.b32 %r230, {high,low};}


	
	{fma.rn.f16x2 %r232,%r224,%r230,%r227;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r232;
mov.b32 %r236, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r232;
mov.b32 %r238, {high,high};}


	
	{mul.f16x2 %r240,%r181,%r238;
}

	
	{xor.b32 %r243,%r240,0x80008000;
}

	
	{fma.rn.f16x2 %r245,%r178,%r236,%r243;
}

	
	{mul.f16x2 %r249,%r178,%r238;
}

	
	{fma.rn.f16x2 %r252,%r181,%r236,%r249;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r196;
mov.b32 %r256, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r196;
mov.b32 %r258, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r260, {low,high};}


	
	{mul.f16x2 %r261,%r258,%r260;
}

	
	{mul.f16x2 %r264,%r232,%r256;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r232;
mov.b32 %r267, {high,low};}


	
	{fma.rn.f16x2 %r269,%r261,%r267,%r264;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r269;
mov.b32 %r273, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r269;
mov.b32 %r275, {high,high};}


	
	{mul.f16x2 %r277,%r193,%r275;
}

	
	{xor.b32 %r280,%r277,0x80008000;
}

	
	{fma.rn.f16x2 %r282,%r190,%r273,%r280;
}

	
	{mul.f16x2 %r286,%r190,%r275;
}

	
	{fma.rn.f16x2 %r289,%r193,%r273,%r286;
}

	barrier.sync 0;
shl.b32 %r311, %r31, 2;
add.s32 %r312, %r311, %r32;
shl.b32 %r313, %r312, 2;
add.s32 %r39, %r141, %r313;
st.shared.u32 [%r39], %r172;
st.shared.u32 [%r39+4], %r208;
st.shared.u32 [%r39+8], %r245;
st.shared.u32 [%r39+12], %r282;
barrier.sync 0;
add.s32 %r315, %r31, %r32;
shl.b32 %r316, %r315, 2;
add.s32 %r40, %r141, %r316;
ld.shared.u32 %r41, [%r40];
ld.shared.u32 %r42, [%r40+1024];
ld.shared.u32 %r43, [%r40+2048];
ld.shared.u32 %r44, [%r40+3072];
barrier.sync 0;
st.shared.u32 [%r39], %r175;
st.shared.u32 [%r39+4], %r215;
st.shared.u32 [%r39+8], %r252;
st.shared.u32 [%r39+12], %r289;
barrier.sync 0;
ld.shared.u32 %r322, [%r40];
ld.shared.u32 %r334, [%r40+1024];
ld.shared.u32 %r323, [%r40+2048];
ld.shared.u32 %r335, [%r40+3072];

	{add.f16x2 %r318,%r41,%r43;
}

	
	{add.f16x2 %r321,%r322,%r323;
}

	
	{sub.f16x2 %r324,%r41,%r43;
}

	
	{sub.f16x2 %r327,%r322,%r323;
}

	
	{add.f16x2 %r330,%r42,%r44;
}

	
	{add.f16x2 %r333,%r334,%r335;
}

	
	{sub.f16x2 %r336,%r42,%r44;
}

	
	{sub.f16x2 %r339,%r334,%r335;
}

	
	{xor.b32 %r342,%r336,0x80008000;
}

	
	{add.f16x2 %r344,%r318,%r330;
}

	
	{add.f16x2 %r347,%r321,%r333;
}

	
	{sub.f16x2 %r350,%r318,%r330;
}

	
	{sub.f16x2 %r353,%r321,%r333;
}

	
	{add.f16x2 %r356,%r324,%r339;
}

	
	{add.f16x2 %r359,%r327,%r342;
}

	
	{sub.f16x2 %r362,%r324,%r339;
}

	
	{sub.f16x2 %r365,%r327,%r342;
}

	and.b32 %r47, %r5, 252;
bfe.u32 %r482, %r5, 2, 6;
cvt.rn.f32.u32	%f28, %r482;
mul.f32 %f29, %f28, 0f3CC90FDB;
cos.approx.f32 %f18, %f29;
sin.approx.f32 %f30, %f29;
neg.f32 %f19, %f30;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f18;
cvt.rn.f16.f32 high, %f19;
mov.b32 %r368, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r368;
mov.b32 %r371, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r368;
mov.b32 %r373, {high,high};}


	
	{mul.f16x2 %r375,%r359,%r373;
}

	
	{xor.b32 %r378,%r375,0x80008000;
}

	
	{fma.rn.f16x2 %r380,%r356,%r371,%r378;
}

	
	{mul.f16x2 %r384,%r356,%r373;
}

	
	{fma.rn.f16x2 %r387,%r359,%r371,%r384;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r368;
mov.b32 %r391, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r368;
mov.b32 %r393, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r395, {low,high};}


	
	{mul.f16x2 %r396,%r393,%r395;
}

	
	{mul.f16x2 %r399,%r368,%r391;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r368;
mov.b32 %r402, {high,low};}


	
	{fma.rn.f16x2 %r404,%r396,%r402,%r399;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r404;
mov.b32 %r408, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r404;
mov.b32 %r410, {high,high};}


	
	{mul.f16x2 %r412,%r353,%r410;
}

	
	{xor.b32 %r415,%r412,0x80008000;
}

	
	{fma.rn.f16x2 %r417,%r350,%r408,%r415;
}

	
	{mul.f16x2 %r421,%r350,%r410;
}

	
	{fma.rn.f16x2 %r424,%r353,%r408,%r421;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r368;
mov.b32 %r428, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r368;
mov.b32 %r430, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r432, {low,high};}


	
	{mul.f16x2 %r433,%r430,%r432;
}

	
	{mul.f16x2 %r436,%r404,%r428;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r404;
mov.b32 %r439, {high,low};}


	
	{fma.rn.f16x2 %r441,%r433,%r439,%r436;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r441;
mov.b32 %r445, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r441;
mov.b32 %r447, {high,high};}


	
	{mul.f16x2 %r449,%r365,%r447;
}

	
	{xor.b32 %r452,%r449,0x80008000;
}

	
	{fma.rn.f16x2 %r454,%r362,%r445,%r452;
}

	
	{mul.f16x2 %r458,%r362,%r447;
}

	
	{fma.rn.f16x2 %r461,%r365,%r445,%r458;
}

	and.b32 %r483, %r5, 3;
add.s32 %r54, %r32, %r483;
barrier.sync 0;
shl.b32 %r484, %r47, 2;
add.s32 %r485, %r484, %r54;
shl.b32 %r486, %r485, 2;
add.s32 %r55, %r141, %r486;
st.shared.u32 [%r55], %r344;
st.shared.u32 [%r55+16], %r380;
st.shared.u32 [%r55+32], %r417;
st.shared.u32 [%r55+48], %r454;
barrier.sync 0;
add.s32 %r488, %r47, %r54;
shl.b32 %r489, %r488, 2;
add.s32 %r56, %r141, %r489;
ld.shared.u32 %r57, [%r56];
ld.shared.u32 %r58, [%r56+1024];
ld.shared.u32 %r59, [%r56+2048];
ld.shared.u32 %r60, [%r56+3072];
barrier.sync 0;
st.shared.u32 [%r55], %r347;
st.shared.u32 [%r55+16], %r387;
st.shared.u32 [%r55+32], %r424;
st.shared.u32 [%r55+48], %r461;
barrier.sync 0;
ld.shared.u32 %r495, [%r56];
ld.shared.u32 %r507, [%r56+1024];
ld.shared.u32 %r496, [%r56+2048];
ld.shared.u32 %r508, [%r56+3072];

	{add.f16x2 %r491,%r57,%r59;
}

	
	{add.f16x2 %r494,%r495,%r496;
}

	
	{sub.f16x2 %r497,%r57,%r59;
}

	
	{sub.f16x2 %r500,%r495,%r496;
}

	
	{add.f16x2 %r503,%r58,%r60;
}

	
	{add.f16x2 %r506,%r507,%r508;
}

	
	{sub.f16x2 %r509,%r58,%r60;
}

	
	{sub.f16x2 %r512,%r507,%r508;
}

	
	{xor.b32 %r515,%r509,0x80008000;
}

	
	{add.f16x2 %r517,%r491,%r503;
}

	
	{add.f16x2 %r520,%r494,%r506;
}

	
	{sub.f16x2 %r523,%r491,%r503;
}

	
	{sub.f16x2 %r526,%r494,%r506;
}

	
	{add.f16x2 %r529,%r497,%r512;
}

	
	{add.f16x2 %r532,%r500,%r515;
}

	
	{sub.f16x2 %r535,%r497,%r512;
}

	
	{sub.f16x2 %r538,%r500,%r515;
}

	and.b32 %r63, %r5, 240;
bfe.u32 %r655, %r5, 4, 4;
cvt.rn.f32.u32	%f41, %r655;
mul.f32 %f42, %f41, 0f3DC90FDB;
cos.approx.f32 %f31, %f42;
sin.approx.f32 %f43, %f42;
neg.f32 %f32, %f43;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f31;
cvt.rn.f16.f32 high, %f32;
mov.b32 %r541, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r541;
mov.b32 %r544, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r541;
mov.b32 %r546, {high,high};}


	
	{mul.f16x2 %r548,%r532,%r546;
}

	
	{xor.b32 %r551,%r548,0x80008000;
}

	
	{fma.rn.f16x2 %r553,%r529,%r544,%r551;
}

	
	{mul.f16x2 %r557,%r529,%r546;
}

	
	{fma.rn.f16x2 %r560,%r532,%r544,%r557;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r541;
mov.b32 %r564, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r541;
mov.b32 %r566, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r568, {low,high};}


	
	{mul.f16x2 %r569,%r566,%r568;
}

	
	{mul.f16x2 %r572,%r541,%r564;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r541;
mov.b32 %r575, {high,low};}


	
	{fma.rn.f16x2 %r577,%r569,%r575,%r572;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r577;
mov.b32 %r581, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r577;
mov.b32 %r583, {high,high};}


	
	{mul.f16x2 %r585,%r526,%r583;
}

	
	{xor.b32 %r588,%r585,0x80008000;
}

	
	{fma.rn.f16x2 %r590,%r523,%r581,%r588;
}

	
	{mul.f16x2 %r594,%r523,%r583;
}

	
	{fma.rn.f16x2 %r597,%r526,%r581,%r594;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r541;
mov.b32 %r601, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r541;
mov.b32 %r603, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r605, {low,high};}


	
	{mul.f16x2 %r606,%r603,%r605;
}

	
	{mul.f16x2 %r609,%r577,%r601;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r577;
mov.b32 %r612, {high,low};}


	
	{fma.rn.f16x2 %r614,%r606,%r612,%r609;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r614;
mov.b32 %r618, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r614;
mov.b32 %r620, {high,high};}


	
	{mul.f16x2 %r622,%r538,%r620;
}

	
	{xor.b32 %r625,%r622,0x80008000;
}

	
	{fma.rn.f16x2 %r627,%r535,%r618,%r625;
}

	
	{mul.f16x2 %r631,%r535,%r620;
}

	
	{fma.rn.f16x2 %r634,%r538,%r618,%r631;
}

	and.b32 %r656, %r5, 15;
add.s32 %r70, %r32, %r656;
barrier.sync 0;
shl.b32 %r657, %r63, 2;
add.s32 %r658, %r657, %r70;
shl.b32 %r659, %r658, 2;
add.s32 %r71, %r141, %r659;
st.shared.u32 [%r71], %r517;
st.shared.u32 [%r71+64], %r553;
st.shared.u32 [%r71+128], %r590;
st.shared.u32 [%r71+192], %r627;
barrier.sync 0;
add.s32 %r661, %r63, %r70;
shl.b32 %r662, %r661, 2;
add.s32 %r72, %r141, %r662;
ld.shared.u32 %r73, [%r72];
ld.shared.u32 %r74, [%r72+1024];
ld.shared.u32 %r75, [%r72+2048];
ld.shared.u32 %r76, [%r72+3072];
barrier.sync 0;
st.shared.u32 [%r71], %r520;
st.shared.u32 [%r71+64], %r560;
st.shared.u32 [%r71+128], %r597;
st.shared.u32 [%r71+192], %r634;
barrier.sync 0;
ld.shared.u32 %r668, [%r72];
ld.shared.u32 %r680, [%r72+1024];
ld.shared.u32 %r669, [%r72+2048];
ld.shared.u32 %r681, [%r72+3072];

	{add.f16x2 %r664,%r73,%r75;
}

	
	{add.f16x2 %r667,%r668,%r669;
}

	
	{sub.f16x2 %r670,%r73,%r75;
}

	
	{sub.f16x2 %r673,%r668,%r669;
}

	
	{add.f16x2 %r676,%r74,%r76;
}

	
	{add.f16x2 %r679,%r680,%r681;
}

	
	{sub.f16x2 %r682,%r74,%r76;
}

	
	{sub.f16x2 %r685,%r680,%r681;
}

	
	{xor.b32 %r688,%r682,0x80008000;
}

	
	{add.f16x2 %r690,%r664,%r676;
}

	
	{add.f16x2 %r693,%r667,%r679;
}

	
	{sub.f16x2 %r696,%r664,%r676;
}

	
	{sub.f16x2 %r699,%r667,%r679;
}

	
	{add.f16x2 %r702,%r670,%r685;
}

	
	{add.f16x2 %r705,%r673,%r688;
}

	
	{sub.f16x2 %r708,%r670,%r685;
}

	
	{sub.f16x2 %r711,%r673,%r688;
}

	and.b32 %r79, %r5, 192;
bfe.u32 %r828, %r5, 6, 2;
cvt.rn.f32.u32	%f54, %r828;
mul.f32 %f55, %f54, 0f3EC90FDB;
cos.approx.f32 %f44, %f55;
sin.approx.f32 %f56, %f55;
neg.f32 %f45, %f56;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r714, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r714;
mov.b32 %r717, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r714;
mov.b32 %r719, {high,high};}


	
	{mul.f16x2 %r721,%r705,%r719;
}

	
	{xor.b32 %r724,%r721,0x80008000;
}

	
	{fma.rn.f16x2 %r726,%r702,%r717,%r724;
}

	
	{mul.f16x2 %r730,%r702,%r719;
}

	
	{fma.rn.f16x2 %r733,%r705,%r717,%r730;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r714;
mov.b32 %r737, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r714;
mov.b32 %r739, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r741, {low,high};}


	
	{mul.f16x2 %r742,%r739,%r741;
}

	
	{mul.f16x2 %r745,%r714,%r737;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r714;
mov.b32 %r748, {high,low};}


	
	{fma.rn.f16x2 %r750,%r742,%r748,%r745;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r750;
mov.b32 %r754, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r750;
mov.b32 %r756, {high,high};}


	
	{mul.f16x2 %r758,%r699,%r756;
}

	
	{xor.b32 %r761,%r758,0x80008000;
}

	
	{fma.rn.f16x2 %r763,%r696,%r754,%r761;
}

	
	{mul.f16x2 %r767,%r696,%r756;
}

	
	{fma.rn.f16x2 %r770,%r699,%r754,%r767;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r714;
mov.b32 %r774, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r714;
mov.b32 %r776, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f14;
mov.b32 %r778, {low,high};}


	
	{mul.f16x2 %r779,%r776,%r778;
}

	
	{mul.f16x2 %r782,%r750,%r774;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r750;
mov.b32 %r785, {high,low};}


	
	{fma.rn.f16x2 %r787,%r779,%r785,%r782;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r787;
mov.b32 %r791, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r787;
mov.b32 %r793, {high,high};}


	
	{mul.f16x2 %r795,%r711,%r793;
}

	
	{xor.b32 %r798,%r795,0x80008000;
}

	
	{fma.rn.f16x2 %r800,%r708,%r791,%r798;
}

	
	{mul.f16x2 %r804,%r708,%r793;
}

	
	{fma.rn.f16x2 %r807,%r711,%r791,%r804;
}

	and.b32 %r829, %r5, 63;
add.s32 %r86, %r32, %r829;
barrier.sync 0;
shl.b32 %r830, %r79, 2;
add.s32 %r831, %r830, %r86;
shl.b32 %r832, %r831, 2;
add.s32 %r87, %r141, %r832;
st.shared.u32 [%r87], %r690;
st.shared.u32 [%r87+256], %r726;
st.shared.u32 [%r87+512], %r763;
st.shared.u32 [%r87+768], %r800;
barrier.sync 0;
add.s32 %r834, %r79, %r86;
shl.b32 %r835, %r834, 2;
add.s32 %r88, %r141, %r835;
ld.shared.u32 %r89, [%r88];
ld.shared.u32 %r90, [%r88+1024];
ld.shared.u32 %r91, [%r88+2048];
ld.shared.u32 %r92, [%r88+3072];
barrier.sync 0;
st.shared.u32 [%r87], %r693;
st.shared.u32 [%r87+256], %r733;
st.shared.u32 [%r87+512], %r770;
st.shared.u32 [%r87+768], %r807;
barrier.sync 0;
ld.shared.u32 %r841, [%r88];
ld.shared.u32 %r853, [%r88+1024];
ld.shared.u32 %r842, [%r88+2048];
ld.shared.u32 %r854, [%r88+3072];

	{add.f16x2 %r837,%r89,%r91;
}

	
	{add.f16x2 %r840,%r841,%r842;
}

	
	{sub.f16x2 %r843,%r89,%r91;
}

	
	{sub.f16x2 %r846,%r841,%r842;
}

	
	{add.f16x2 %r849,%r90,%r92;
}

	
	{add.f16x2 %r852,%r853,%r854;
}

	
	{sub.f16x2 %r855,%r90,%r92;
}

	
	{sub.f16x2 %r858,%r853,%r854;
}

	
	{xor.b32 %r861,%r855,0x80008000;
}

	
	{add.f16x2 %r863,%r837,%r849;
}

	
	{add.f16x2 %r866,%r840,%r852;
}

	
	{sub.f16x2 %r869,%r837,%r849;
}

	
	{sub.f16x2 %r872,%r840,%r852;
}

	
	{add.f16x2 %r875,%r843,%r858;
}

	
	{add.f16x2 %r878,%r846,%r861;
}

	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r863;
mov.b32 {blow,bhigh}, %r866;
mov.b32 %r887, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r863;
mov.b32 {blow,bhigh}, %r866;
mov.b32 %r890, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r875;
mov.b32 {blow,bhigh}, %r878;
mov.b32 %r893, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r875;
mov.b32 {blow,bhigh}, %r878;
mov.b32 %r896, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r869;
mov.b32 {blow,bhigh}, %r872;
mov.b32 %r899, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r869;
mov.b32 {blow,bhigh}, %r872;
mov.b32 %r902, {ahigh,bhigh};}


	mad.lo.s32 %r911, %r2, 1026, %r5;
cvta.to.global.u64 %rd9, %rd5;
mul.wide.u32 %rd10, %r911, 4;
add.s64 %rd3, %rd9, %rd10;
@%p2 bra BB9_13;
bra.uni BB9_7;

BB9_13:
st.global.u32 [%rd3], %r887;
st.global.u32 [%rd3+1024], %r893;
setp.gt.u32	%p10, %r20, 512;
@%p10 bra BB9_15;

st.global.u32 [%rd3+2048], %r899;

BB9_15:
st.global.u32 [%rd3+2052], %r890;
st.global.u32 [%rd3+3076], %r896;
@%p10 bra BB9_17;
bra.uni BB9_16;

BB9_7:
shl.b32 %r912, %r3, 1;
add.s32 %r913, %r912, -1;
setp.lt.u32	%p6, %r913, %r1;
st.global.u32 [%rd3], %r887;
st.global.u32 [%rd3+1024], %r893;
@%p6 bra BB9_10;
bra.uni BB9_8;

BB9_10:
setp.gt.u32	%p8, %r20, 512;
@%p8 bra BB9_12;

st.global.u32 [%rd3+2048], %r899;

BB9_12:
st.global.u32 [%rd3+2052], %r890;
st.global.u32 [%rd3+3076], %r896;
@%p8 bra BB9_17;

BB9_16:
st.global.u32 [%rd3+4100], %r902;
bra.uni BB9_17;

BB9_8:
setp.gt.u32	%p7, %r20, 512;
@%p7 bra BB9_17;

st.global.u32 [%rd3+2048], %r899;

BB9_17:
ret;
}


.weak .entry _Z14vector_fft_r2cILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 512, 1, 1
{
.reg .pred %p<13>;
.reg .b16 %rs<9>;
.reg .f32 %f<72>;
.reg .b32 %r<1058>;
.reg .f64 %fd<2>;
.reg .b64 %rd<11>;


ld.param.u32 %r1, [_Z14vector_fft_r2cILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z14vector_fft_r2cILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj2048ELj4ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
setp.eq.s16	%p2, %rs1, 0;
selp.b32	%r112, 1024, 1025, %p2;
mov.u32 %r113, %ctaid.x;
shl.b32 %r114, %r113, 1;
add.s32 %r115, %r114, 1;
mov.u32 %r2, %nctaid.x;
add.s32 %r3, %r2, -1;
setp.lt.u32	%p3, %r113, %r3;
mov.u32 %r4, %tid.x;
mad.lo.s32 %r116, %r114, %r112, %r4;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r116, 4;
add.s64 %rd1, %rd6, %rd7;
mad.lo.s32 %r117, %r115, %r112, %r4;
mul.wide.u32 %rd8, %r117, 4;
add.s64 %rd2, %rd6, %rd8;
@%p3 bra BB10_2;
bra.uni BB10_1;

BB10_2:
ld.global.u32 %r1057, [%rd1];
ld.global.u32 %r1055, [%rd1+2048];
bra.uni BB10_3;

BB10_1:
shl.b32 %r119, %r2, 1;
add.s32 %r120, %r119, -1;
ld.global.u32 %r1057, [%rd1];
ld.global.u32 %r1055, [%rd1+2048];
setp.ge.u32	%p4, %r120, %r1;
@%p4 bra BB10_4;

BB10_3:
ld.global.u32 %r1056, [%rd2];
ld.global.u32 %r1054, [%rd2+2048];

BB10_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1057;
mov.b32 {blow,bhigh}, %r1056;
mov.b32 %r121, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1057;
mov.b32 {blow,bhigh}, %r1056;
mov.b32 %r124, {ahigh,bhigh};}


	shl.b32 %r133, %r4, 3;
mov.u32 %r134, smem_full;
add.s32 %r135, %r134, %r133;
st.shared.u32 [%r135], %r121;
st.shared.u32 [%r135+4], %r124;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1055;
mov.b32 {blow,bhigh}, %r1054;
mov.b32 %r127, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1055;
mov.b32 {blow,bhigh}, %r1054;
mov.b32 %r130, {ahigh,bhigh};}


	st.shared.u32 [%r135+4096], %r127;
st.shared.u32 [%r135+4100], %r130;
barrier.sync 0;
shl.b32 %r136, %r4, 2;
add.s32 %r138, %r134, %r136;
ld.shared.u32 %r17, [%r138];
ld.shared.u32 %r18, [%r138+2048];
add.s32 %r19, %r4, 1024;
ld.shared.u32 %r20, [%r138+4096];
ld.shared.u32 %r21, [%r138+6144];
barrier.sync 0;

	{add.f16x2 %r139,%r17,%r20;
}

	mov.u32 %r162, 0;

	{add.f16x2 %r142,%r162,%r162;
}

	
	{sub.f16x2 %r145,%r17,%r20;
}

	
	{sub.f16x2 %r148,%r162,%r162;
}

	
	{add.f16x2 %r151,%r18,%r21;
}

	
	{add.f16x2 %r154,%r162,%r162;
}

	
	{sub.f16x2 %r157,%r18,%r21;
}

	
	{sub.f16x2 %r160,%r162,%r162;
}

	
	{xor.b32 %r163,%r157,0x80008000;
}

	
	{add.f16x2 %r165,%r139,%r151;
}

	
	{add.f16x2 %r168,%r142,%r154;
}

	
	{sub.f16x2 %r171,%r139,%r151;
}

	
	{sub.f16x2 %r174,%r142,%r154;
}

	
	{add.f16x2 %r177,%r145,%r160;
}

	
	{add.f16x2 %r180,%r148,%r163;
}

	
	{sub.f16x2 %r183,%r145,%r160;
}

	
	{sub.f16x2 %r186,%r148,%r163;
}

	and.b32 %r30, %r4, 511;
cvt.rn.f32.u32	%f6, %r30;
mul.f32 %f1, %f6, 0f3B490FDB;
setp.eq.s32	%p5, %r30, 510;
mov.f32 %f71, 0f3BC90F88;
@%p5 bra BB10_7;

setp.eq.s32	%p6, %r30, 511;
mov.f32 %f71, 0f3B490FC6;
@%p6 bra BB10_7;

cos.approx.f32 %f71, %f1;

BB10_7:
and.b32 %r31, %r136, -2048;
sin.approx.f32 %f18, %f1;
neg.f32 %f9, %f18;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f71;
cvt.rn.f16.f32 high, %f9;
mov.b32 %r189, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r192, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r194, {high,high};}


	
	{mul.f16x2 %r196,%r180,%r194;
}

	
	{xor.b32 %r199,%r196,0x80008000;
}

	
	{fma.rn.f16x2 %r201,%r177,%r192,%r199;
}

	
	{mul.f16x2 %r205,%r177,%r194;
}

	
	{fma.rn.f16x2 %r208,%r180,%r192,%r205;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r212, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r214, {high,high};}


	mov.f32 %f14, 0fBF800000;
mov.f32 %f15, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r216, {low,high};}


	
	{mul.f16x2 %r217,%r214,%r216;
}

	
	{mul.f16x2 %r220,%r189,%r212;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r223, {high,low};}


	
	{fma.rn.f16x2 %r225,%r217,%r223,%r220;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r225;
mov.b32 %r229, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r225;
mov.b32 %r231, {high,high};}


	
	{mul.f16x2 %r233,%r174,%r231;
}

	
	{xor.b32 %r236,%r233,0x80008000;
}

	
	{fma.rn.f16x2 %r238,%r171,%r229,%r236;
}

	
	{mul.f16x2 %r242,%r171,%r231;
}

	
	{fma.rn.f16x2 %r245,%r174,%r229,%r242;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r249, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r189;
mov.b32 %r251, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r253, {low,high};}


	
	{mul.f16x2 %r254,%r251,%r253;
}

	
	{mul.f16x2 %r257,%r225,%r249;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r225;
mov.b32 %r260, {high,low};}


	
	{fma.rn.f16x2 %r262,%r254,%r260,%r257;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r262;
mov.b32 %r266, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r262;
mov.b32 %r268, {high,high};}


	
	{mul.f16x2 %r270,%r186,%r268;
}

	
	{xor.b32 %r273,%r270,0x80008000;
}

	
	{fma.rn.f16x2 %r275,%r183,%r266,%r273;
}

	
	{mul.f16x2 %r279,%r183,%r268;
}

	
	{fma.rn.f16x2 %r282,%r186,%r266,%r279;
}

	barrier.sync 0;
shl.b32 %r304, %r31, 3;
add.s32 %r39, %r134, %r304;
shl.b32 %r306, %r30, 5;
add.s32 %r307, %r39, %r306;
st.shared.u32 [%r307], %r165;
st.shared.u32 [%r307+4], %r168;
st.shared.u32 [%r307+8], %r201;
st.shared.u32 [%r307+12], %r208;
st.shared.u32 [%r307+16], %r238;
st.shared.u32 [%r307+20], %r245;
st.shared.u32 [%r307+24], %r275;
st.shared.u32 [%r307+28], %r282;
barrier.sync 0;
shl.b32 %r472, %r30, 3;
add.s32 %r473, %r39, %r472;
ld.shared.u32 %r309, [%r473];
ld.shared.u32 %r312, [%r473+4];
ld.shared.u32 %r321, [%r473+4096];
ld.shared.u32 %r324, [%r473+4100];
ld.shared.u32 %r310, [%r473+8192];
ld.shared.u32 %r313, [%r473+8196];
ld.shared.u32 %r322, [%r473+12288];
ld.shared.u32 %r325, [%r473+12292];

	{add.f16x2 %r308,%r309,%r310;
}

	
	{add.f16x2 %r311,%r312,%r313;
}

	
	{sub.f16x2 %r314,%r309,%r310;
}

	
	{sub.f16x2 %r317,%r312,%r313;
}

	
	{add.f16x2 %r320,%r321,%r322;
}

	
	{add.f16x2 %r323,%r324,%r325;
}

	
	{sub.f16x2 %r326,%r321,%r322;
}

	
	{sub.f16x2 %r329,%r324,%r325;
}

	
	{xor.b32 %r332,%r326,0x80008000;
}

	
	{add.f16x2 %r334,%r308,%r320;
}

	
	{add.f16x2 %r337,%r311,%r323;
}

	
	{sub.f16x2 %r340,%r308,%r320;
}

	
	{sub.f16x2 %r343,%r311,%r323;
}

	
	{add.f16x2 %r346,%r314,%r329;
}

	
	{add.f16x2 %r349,%r317,%r332;
}

	
	{sub.f16x2 %r352,%r314,%r329;
}

	
	{sub.f16x2 %r355,%r317,%r332;
}

	and.b32 %r43, %r4, 508;
and.b32 %r474, %r4, 3;
bfe.u32 %r475, %r4, 2, 7;
add.s32 %r476, %r31, %r474;
cvt.rn.f32.u32	%f29, %r475;
mul.f32 %f30, %f29, 0f3C490FDB;
cos.approx.f32 %f19, %f30;
sin.approx.f32 %f31, %f30;
neg.f32 %f20, %f31;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f19;
cvt.rn.f16.f32 high, %f20;
mov.b32 %r358, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r361, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r363, {high,high};}


	
	{mul.f16x2 %r365,%r349,%r363;
}

	
	{xor.b32 %r368,%r365,0x80008000;
}

	
	{fma.rn.f16x2 %r370,%r346,%r361,%r368;
}

	
	{mul.f16x2 %r374,%r346,%r363;
}

	
	{fma.rn.f16x2 %r377,%r349,%r361,%r374;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r381, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r383, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r385, {low,high};}


	
	{mul.f16x2 %r386,%r383,%r385;
}

	
	{mul.f16x2 %r389,%r358,%r381;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r392, {high,low};}


	
	{fma.rn.f16x2 %r394,%r386,%r392,%r389;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r394;
mov.b32 %r398, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r394;
mov.b32 %r400, {high,high};}


	
	{mul.f16x2 %r402,%r343,%r400;
}

	
	{xor.b32 %r405,%r402,0x80008000;
}

	
	{fma.rn.f16x2 %r407,%r340,%r398,%r405;
}

	
	{mul.f16x2 %r411,%r340,%r400;
}

	
	{fma.rn.f16x2 %r414,%r343,%r398,%r411;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r418, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r358;
mov.b32 %r420, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r422, {low,high};}


	
	{mul.f16x2 %r423,%r420,%r422;
}

	
	{mul.f16x2 %r426,%r394,%r418;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r394;
mov.b32 %r429, {high,low};}


	
	{fma.rn.f16x2 %r431,%r423,%r429,%r426;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r431;
mov.b32 %r435, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r431;
mov.b32 %r437, {high,high};}


	
	{mul.f16x2 %r439,%r355,%r437;
}

	
	{xor.b32 %r442,%r439,0x80008000;
}

	
	{fma.rn.f16x2 %r444,%r352,%r435,%r442;
}

	
	{mul.f16x2 %r448,%r352,%r437;
}

	
	{fma.rn.f16x2 %r451,%r355,%r435,%r448;
}

	shl.b32 %r477, %r476, 3;
add.s32 %r50, %r134, %r477;
barrier.sync 0;
shl.b32 %r479, %r43, 5;
add.s32 %r480, %r50, %r479;
st.shared.u32 [%r480], %r334;
st.shared.u32 [%r480+4], %r337;
st.shared.u32 [%r480+32], %r370;
st.shared.u32 [%r480+36], %r377;
st.shared.u32 [%r480+64], %r407;
st.shared.u32 [%r480+68], %r414;
st.shared.u32 [%r480+96], %r444;
st.shared.u32 [%r480+100], %r451;
barrier.sync 0;
shl.b32 %r645, %r43, 3;
add.s32 %r646, %r50, %r645;
ld.shared.u32 %r482, [%r646];
ld.shared.u32 %r485, [%r646+4];
ld.shared.u32 %r494, [%r646+4096];
ld.shared.u32 %r497, [%r646+4100];
ld.shared.u32 %r483, [%r646+8192];
ld.shared.u32 %r486, [%r646+8196];
ld.shared.u32 %r495, [%r646+12288];
ld.shared.u32 %r498, [%r646+12292];

	{add.f16x2 %r481,%r482,%r483;
}

	
	{add.f16x2 %r484,%r485,%r486;
}

	
	{sub.f16x2 %r487,%r482,%r483;
}

	
	{sub.f16x2 %r490,%r485,%r486;
}

	
	{add.f16x2 %r493,%r494,%r495;
}

	
	{add.f16x2 %r496,%r497,%r498;
}

	
	{sub.f16x2 %r499,%r494,%r495;
}

	
	{sub.f16x2 %r502,%r497,%r498;
}

	
	{xor.b32 %r505,%r499,0x80008000;
}

	
	{add.f16x2 %r507,%r481,%r493;
}

	
	{add.f16x2 %r510,%r484,%r496;
}

	
	{sub.f16x2 %r513,%r481,%r493;
}

	
	{sub.f16x2 %r516,%r484,%r496;
}

	
	{add.f16x2 %r519,%r487,%r502;
}

	
	{add.f16x2 %r522,%r490,%r505;
}

	
	{sub.f16x2 %r525,%r487,%r502;
}

	
	{sub.f16x2 %r528,%r490,%r505;
}

	and.b32 %r54, %r4, 496;
and.b32 %r647, %r4, 15;
bfe.u32 %r648, %r4, 4, 5;
add.s32 %r649, %r31, %r647;
cvt.rn.f32.u32	%f42, %r648;
mul.f32 %f43, %f42, 0f3D490FDB;
cos.approx.f32 %f32, %f43;
sin.approx.f32 %f44, %f43;
neg.f32 %f33, %f44;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f32;
cvt.rn.f16.f32 high, %f33;
mov.b32 %r531, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r534, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r536, {high,high};}


	
	{mul.f16x2 %r538,%r522,%r536;
}

	
	{xor.b32 %r541,%r538,0x80008000;
}

	
	{fma.rn.f16x2 %r543,%r519,%r534,%r541;
}

	
	{mul.f16x2 %r547,%r519,%r536;
}

	
	{fma.rn.f16x2 %r550,%r522,%r534,%r547;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r554, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r556, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r558, {low,high};}


	
	{mul.f16x2 %r559,%r556,%r558;
}

	
	{mul.f16x2 %r562,%r531,%r554;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r565, {high,low};}


	
	{fma.rn.f16x2 %r567,%r559,%r565,%r562;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r567;
mov.b32 %r571, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r567;
mov.b32 %r573, {high,high};}


	
	{mul.f16x2 %r575,%r516,%r573;
}

	
	{xor.b32 %r578,%r575,0x80008000;
}

	
	{fma.rn.f16x2 %r580,%r513,%r571,%r578;
}

	
	{mul.f16x2 %r584,%r513,%r573;
}

	
	{fma.rn.f16x2 %r587,%r516,%r571,%r584;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r591, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r531;
mov.b32 %r593, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r595, {low,high};}


	
	{mul.f16x2 %r596,%r593,%r595;
}

	
	{mul.f16x2 %r599,%r567,%r591;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r567;
mov.b32 %r602, {high,low};}


	
	{fma.rn.f16x2 %r604,%r596,%r602,%r599;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r604;
mov.b32 %r608, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r604;
mov.b32 %r610, {high,high};}


	
	{mul.f16x2 %r612,%r528,%r610;
}

	
	{xor.b32 %r615,%r612,0x80008000;
}

	
	{fma.rn.f16x2 %r617,%r525,%r608,%r615;
}

	
	{mul.f16x2 %r621,%r525,%r610;
}

	
	{fma.rn.f16x2 %r624,%r528,%r608,%r621;
}

	shl.b32 %r650, %r649, 3;
add.s32 %r61, %r134, %r650;
barrier.sync 0;
shl.b32 %r652, %r54, 5;
add.s32 %r653, %r61, %r652;
st.shared.u32 [%r653], %r507;
st.shared.u32 [%r653+4], %r510;
st.shared.u32 [%r653+128], %r543;
st.shared.u32 [%r653+132], %r550;
st.shared.u32 [%r653+256], %r580;
st.shared.u32 [%r653+260], %r587;
st.shared.u32 [%r653+384], %r617;
st.shared.u32 [%r653+388], %r624;
barrier.sync 0;
shl.b32 %r818, %r54, 3;
add.s32 %r819, %r61, %r818;
ld.shared.u32 %r655, [%r819];
ld.shared.u32 %r658, [%r819+4];
ld.shared.u32 %r667, [%r819+4096];
ld.shared.u32 %r670, [%r819+4100];
ld.shared.u32 %r656, [%r819+8192];
ld.shared.u32 %r659, [%r819+8196];
ld.shared.u32 %r668, [%r819+12288];
ld.shared.u32 %r671, [%r819+12292];

	{add.f16x2 %r654,%r655,%r656;
}

	
	{add.f16x2 %r657,%r658,%r659;
}

	
	{sub.f16x2 %r660,%r655,%r656;
}

	
	{sub.f16x2 %r663,%r658,%r659;
}

	
	{add.f16x2 %r666,%r667,%r668;
}

	
	{add.f16x2 %r669,%r670,%r671;
}

	
	{sub.f16x2 %r672,%r667,%r668;
}

	
	{sub.f16x2 %r675,%r670,%r671;
}

	
	{xor.b32 %r678,%r672,0x80008000;
}

	
	{add.f16x2 %r680,%r654,%r666;
}

	
	{add.f16x2 %r683,%r657,%r669;
}

	
	{sub.f16x2 %r686,%r654,%r666;
}

	
	{sub.f16x2 %r689,%r657,%r669;
}

	
	{add.f16x2 %r692,%r660,%r675;
}

	
	{add.f16x2 %r695,%r663,%r678;
}

	
	{sub.f16x2 %r698,%r660,%r675;
}

	
	{sub.f16x2 %r701,%r663,%r678;
}

	and.b32 %r65, %r4, 448;
and.b32 %r820, %r4, 63;
bfe.u32 %r821, %r4, 6, 3;
add.s32 %r822, %r31, %r820;
cvt.rn.f32.u32	%f55, %r821;
mul.f32 %f56, %f55, 0f3E490FDB;
cos.approx.f32 %f45, %f56;
sin.approx.f32 %f57, %f56;
neg.f32 %f46, %f57;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f45;
cvt.rn.f16.f32 high, %f46;
mov.b32 %r704, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r707, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r709, {high,high};}


	
	{mul.f16x2 %r711,%r695,%r709;
}

	
	{xor.b32 %r714,%r711,0x80008000;
}

	
	{fma.rn.f16x2 %r716,%r692,%r707,%r714;
}

	
	{mul.f16x2 %r720,%r692,%r709;
}

	
	{fma.rn.f16x2 %r723,%r695,%r707,%r720;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r727, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r729, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r731, {low,high};}


	
	{mul.f16x2 %r732,%r729,%r731;
}

	
	{mul.f16x2 %r735,%r704,%r727;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r738, {high,low};}


	
	{fma.rn.f16x2 %r740,%r732,%r738,%r735;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r740;
mov.b32 %r744, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r740;
mov.b32 %r746, {high,high};}


	
	{mul.f16x2 %r748,%r689,%r746;
}

	
	{xor.b32 %r751,%r748,0x80008000;
}

	
	{fma.rn.f16x2 %r753,%r686,%r744,%r751;
}

	
	{mul.f16x2 %r757,%r686,%r746;
}

	
	{fma.rn.f16x2 %r760,%r689,%r744,%r757;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r764, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r704;
mov.b32 %r766, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r768, {low,high};}


	
	{mul.f16x2 %r769,%r766,%r768;
}

	
	{mul.f16x2 %r772,%r740,%r764;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r740;
mov.b32 %r775, {high,low};}


	
	{fma.rn.f16x2 %r777,%r769,%r775,%r772;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r777;
mov.b32 %r781, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r777;
mov.b32 %r783, {high,high};}


	
	{mul.f16x2 %r785,%r701,%r783;
}

	
	{xor.b32 %r788,%r785,0x80008000;
}

	
	{fma.rn.f16x2 %r790,%r698,%r781,%r788;
}

	
	{mul.f16x2 %r794,%r698,%r783;
}

	
	{fma.rn.f16x2 %r797,%r701,%r781,%r794;
}

	shl.b32 %r823, %r822, 3;
add.s32 %r72, %r134, %r823;
barrier.sync 0;
shl.b32 %r825, %r65, 5;
add.s32 %r826, %r72, %r825;
st.shared.u32 [%r826], %r680;
st.shared.u32 [%r826+4], %r683;
st.shared.u32 [%r826+512], %r716;
st.shared.u32 [%r826+516], %r723;
st.shared.u32 [%r826+1024], %r753;
st.shared.u32 [%r826+1028], %r760;
st.shared.u32 [%r826+1536], %r790;
st.shared.u32 [%r826+1540], %r797;
barrier.sync 0;
shl.b32 %r991, %r65, 3;
add.s32 %r992, %r72, %r991;
ld.shared.u32 %r828, [%r992];
ld.shared.u32 %r831, [%r992+4];
ld.shared.u32 %r840, [%r992+4096];
ld.shared.u32 %r843, [%r992+4100];
ld.shared.u32 %r829, [%r992+8192];
ld.shared.u32 %r832, [%r992+8196];
ld.shared.u32 %r841, [%r992+12288];
ld.shared.u32 %r844, [%r992+12292];

	{add.f16x2 %r827,%r828,%r829;
}

	
	{add.f16x2 %r830,%r831,%r832;
}

	
	{sub.f16x2 %r833,%r828,%r829;
}

	
	{sub.f16x2 %r836,%r831,%r832;
}

	
	{add.f16x2 %r839,%r840,%r841;
}

	
	{add.f16x2 %r842,%r843,%r844;
}

	
	{sub.f16x2 %r845,%r840,%r841;
}

	
	{sub.f16x2 %r848,%r843,%r844;
}

	
	{xor.b32 %r851,%r845,0x80008000;
}

	
	{add.f16x2 %r853,%r827,%r839;
}

	
	{add.f16x2 %r856,%r830,%r842;
}

	
	{sub.f16x2 %r859,%r827,%r839;
}

	
	{sub.f16x2 %r862,%r830,%r842;
}

	
	{add.f16x2 %r865,%r833,%r848;
}

	
	{add.f16x2 %r868,%r836,%r851;
}

	
	{sub.f16x2 %r871,%r833,%r848;
}

	
	{sub.f16x2 %r874,%r836,%r851;
}

	and.b32 %r76, %r4, 256;
and.b32 %r993, %r4, 255;
bfe.u32 %r994, %r4, 8, 1;
add.s32 %r995, %r31, %r993;
cvt.rn.f32.u32	%f68, %r994;
mul.f32 %f69, %f68, 0f3F490FDB;
cos.approx.f32 %f58, %f69;
sin.approx.f32 %f70, %f69;
neg.f32 %f59, %f70;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f58;
cvt.rn.f16.f32 high, %f59;
mov.b32 %r877, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r877;
mov.b32 %r880, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r877;
mov.b32 %r882, {high,high};}


	
	{mul.f16x2 %r884,%r868,%r882;
}

	
	{xor.b32 %r887,%r884,0x80008000;
}

	
	{fma.rn.f16x2 %r889,%r865,%r880,%r887;
}

	
	{mul.f16x2 %r893,%r865,%r882;
}

	
	{fma.rn.f16x2 %r896,%r868,%r880,%r893;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r877;
mov.b32 %r900, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r877;
mov.b32 %r902, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r904, {low,high};}


	
	{mul.f16x2 %r905,%r902,%r904;
}

	
	{mul.f16x2 %r908,%r877,%r900;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r877;
mov.b32 %r911, {high,low};}


	
	{fma.rn.f16x2 %r913,%r905,%r911,%r908;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r913;
mov.b32 %r917, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r913;
mov.b32 %r919, {high,high};}


	
	{mul.f16x2 %r921,%r862,%r919;
}

	
	{xor.b32 %r924,%r921,0x80008000;
}

	
	{fma.rn.f16x2 %r926,%r859,%r917,%r924;
}

	
	{mul.f16x2 %r930,%r859,%r919;
}

	
	{fma.rn.f16x2 %r933,%r862,%r917,%r930;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r877;
mov.b32 %r937, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r877;
mov.b32 %r939, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f14;
cvt.rn.f16.f32 high, %f15;
mov.b32 %r941, {low,high};}


	
	{mul.f16x2 %r942,%r939,%r941;
}

	
	{mul.f16x2 %r945,%r913,%r937;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r913;
mov.b32 %r948, {high,low};}


	
	{fma.rn.f16x2 %r950,%r942,%r948,%r945;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r950;
mov.b32 %r954, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r950;
mov.b32 %r956, {high,high};}


	
	{mul.f16x2 %r958,%r874,%r956;
}

	
	{xor.b32 %r961,%r958,0x80008000;
}

	
	{fma.rn.f16x2 %r963,%r871,%r954,%r961;
}

	
	{mul.f16x2 %r967,%r871,%r956;
}

	
	{fma.rn.f16x2 %r970,%r874,%r954,%r967;
}

	shl.b32 %r996, %r995, 3;
add.s32 %r83, %r134, %r996;
barrier.sync 0;
shl.b32 %r998, %r76, 5;
add.s32 %r999, %r83, %r998;
st.shared.u32 [%r999], %r853;
st.shared.u32 [%r999+4], %r856;
st.shared.u32 [%r999+2048], %r889;
st.shared.u32 [%r999+2052], %r896;
st.shared.u32 [%r999+4096], %r926;
st.shared.u32 [%r999+4100], %r933;
st.shared.u32 [%r999+6144], %r963;
st.shared.u32 [%r999+6148], %r970;
barrier.sync 0;
shl.b32 %r1048, %r76, 3;
add.s32 %r1049, %r83, %r1048;
ld.shared.u32 %r1001, [%r1049];
ld.shared.u32 %r1004, [%r1049+4];
ld.shared.u32 %r1013, [%r1049+4096];
ld.shared.u32 %r1016, [%r1049+4100];
ld.shared.u32 %r1002, [%r1049+8192];
ld.shared.u32 %r1005, [%r1049+8196];
ld.shared.u32 %r1014, [%r1049+12288];
ld.shared.u32 %r1017, [%r1049+12292];

	{add.f16x2 %r1000,%r1001,%r1002;
}

	
	{add.f16x2 %r1003,%r1004,%r1005;
}

	
	{sub.f16x2 %r1006,%r1001,%r1002;
}

	
	{sub.f16x2 %r1009,%r1004,%r1005;
}

	
	{add.f16x2 %r1012,%r1013,%r1014;
}

	
	{add.f16x2 %r1015,%r1016,%r1017;
}

	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1000;
mov.b32 {blow,bhigh}, %r1003;
mov.b32 %r1024, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1000;
mov.b32 {blow,bhigh}, %r1003;
mov.b32 %r1027, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1012;
mov.b32 {blow,bhigh}, %r1015;
mov.b32 %r1030, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1012;
mov.b32 {blow,bhigh}, %r1015;
mov.b32 %r1033, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1006;
mov.b32 {blow,bhigh}, %r1009;
mov.b32 %r1036, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1006;
mov.b32 {blow,bhigh}, %r1009;
mov.b32 %r1039, {ahigh,bhigh};}


	mad.lo.s32 %r1050, %r113, 2050, %r4;
cvta.to.global.u64 %rd9, %rd5;
mul.wide.u32 %rd10, %r1050, 4;
add.s64 %rd3, %rd9, %rd10;
@%p3 bra BB10_14;
bra.uni BB10_8;

BB10_14:
st.global.u32 [%rd3], %r1024;
st.global.u32 [%rd3+2048], %r1030;
setp.gt.u32	%p11, %r19, 1024;
@%p11 bra BB10_16;

st.global.u32 [%rd3+4096], %r1036;

BB10_16:
st.global.u32 [%rd3+4100], %r1027;
st.global.u32 [%rd3+6148], %r1033;
@%p11 bra BB10_18;
bra.uni BB10_17;

BB10_8:
shl.b32 %r1052, %r2, 1;
add.s32 %r1053, %r1052, -1;
setp.lt.u32	%p7, %r1053, %r1;
st.global.u32 [%rd3], %r1024;
st.global.u32 [%rd3+2048], %r1030;
@%p7 bra BB10_11;
bra.uni BB10_9;

BB10_11:
setp.gt.u32	%p9, %r19, 1024;
@%p9 bra BB10_13;

st.global.u32 [%rd3+4096], %r1036;

BB10_13:
st.global.u32 [%rd3+4100], %r1027;
st.global.u32 [%rd3+6148], %r1033;
@%p9 bra BB10_18;

BB10_17:
st.global.u32 [%rd3+8196], %r1039;
bra.uni BB10_18;

BB10_9:
setp.gt.u32	%p8, %r19, 1024;
@%p8 bra BB10_18;

st.global.u32 [%rd3+4096], %r1036;

BB10_18:
ret;
}


.weak .entry _Z14vector_fft_r2cILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 512, 1, 1
{
.reg .pred %p<11>;
.reg .b16 %rs<9>;
.reg .f32 %f<177>;
.reg .b32 %r<1820>;
.reg .f64 %fd<2>;
.reg .b64 %rd<11>;


ld.param.u32 %r1, [_Z14vector_fft_r2cILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z14vector_fft_r2cILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj4096ELj8ELj2EL9padding_t0EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
setp.eq.s16	%p1, %rs1, 0;
selp.b32	%r128, 2048, 2049, %p1;
mov.u32 %r2, %ctaid.x;
shl.b32 %r129, %r2, 1;
add.s32 %r130, %r129, 1;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
mad.lo.s32 %r131, %r129, %r128, %r5;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r131, 4;
add.s64 %rd1, %rd6, %rd7;
mad.lo.s32 %r132, %r130, %r128, %r5;
mul.wide.u32 %rd8, %r132, 4;
add.s64 %rd2, %rd6, %rd8;
@%p2 bra BB11_2;
bra.uni BB11_1;

BB11_2:
ld.global.u32 %r1819, [%rd1];
ld.global.u32 %r1817, [%rd1+2048];
ld.global.u32 %r1815, [%rd1+4096];
ld.global.u32 %r1813, [%rd1+6144];
bra.uni BB11_3;

BB11_1:
shl.b32 %r134, %r3, 1;
add.s32 %r135, %r134, -1;
ld.global.u32 %r1819, [%rd1];
ld.global.u32 %r1817, [%rd1+2048];
ld.global.u32 %r1815, [%rd1+4096];
ld.global.u32 %r1813, [%rd1+6144];
setp.ge.u32	%p3, %r135, %r1;
@%p3 bra BB11_4;

BB11_3:
ld.global.u32 %r1818, [%rd2];
ld.global.u32 %r1816, [%rd2+2048];
ld.global.u32 %r1814, [%rd2+4096];
ld.global.u32 %r1812, [%rd2+6144];

BB11_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1819;
mov.b32 {blow,bhigh}, %r1818;
mov.b32 %r136, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1819;
mov.b32 {blow,bhigh}, %r1818;
mov.b32 %r139, {ahigh,bhigh};}


	shl.b32 %r160, %r5, 3;
mov.u32 %r161, smem_full;
add.s32 %r162, %r161, %r160;
st.shared.u32 [%r162], %r136;
st.shared.u32 [%r162+4], %r139;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1817;
mov.b32 {blow,bhigh}, %r1816;
mov.b32 %r142, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1817;
mov.b32 {blow,bhigh}, %r1816;
mov.b32 %r145, {ahigh,bhigh};}


	st.shared.u32 [%r162+4096], %r142;
st.shared.u32 [%r162+4100], %r145;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1815;
mov.b32 {blow,bhigh}, %r1814;
mov.b32 %r148, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1815;
mov.b32 {blow,bhigh}, %r1814;
mov.b32 %r151, {ahigh,bhigh};}


	st.shared.u32 [%r162+8192], %r148;
st.shared.u32 [%r162+8196], %r151;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1813;
mov.b32 {blow,bhigh}, %r1812;
mov.b32 %r154, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1813;
mov.b32 {blow,bhigh}, %r1812;
mov.b32 %r157, {ahigh,bhigh};}


	st.shared.u32 [%r162+12288], %r154;
st.shared.u32 [%r162+12292], %r157;
barrier.sync 0;
shl.b32 %r163, %r5, 2;
add.s32 %r165, %r161, %r163;
ld.shared.u32 %r30, [%r165];
ld.shared.u32 %r31, [%r165+2048];
ld.shared.u32 %r32, [%r165+4096];
ld.shared.u32 %r33, [%r165+6144];
add.s32 %r34, %r5, 2048;
ld.shared.u32 %r35, [%r165+8192];
ld.shared.u32 %r36, [%r165+10240];
ld.shared.u32 %r37, [%r165+12288];
ld.shared.u32 %r38, [%r165+14336];
barrier.sync 0;

	{add.f16x2 %r166,%r30,%r35;
}

	mov.u32 %r239, 0;

	{add.f16x2 %r169,%r239,%r239;
}

	
	{sub.f16x2 %r172,%r30,%r35;
}

	
	{sub.f16x2 %r175,%r239,%r239;
}

	
	{add.f16x2 %r178,%r32,%r37;
}

	
	{add.f16x2 %r181,%r239,%r239;
}

	
	{sub.f16x2 %r184,%r32,%r37;
}

	
	{sub.f16x2 %r187,%r239,%r239;
}

	
	{xor.b32 %r190,%r184,0x80008000;
}

	
	{add.f16x2 %r192,%r166,%r178;
}

	
	{add.f16x2 %r195,%r169,%r181;
}

	
	{sub.f16x2 %r198,%r166,%r178;
}

	
	{sub.f16x2 %r201,%r169,%r181;
}

	
	{add.f16x2 %r204,%r172,%r187;
}

	
	{add.f16x2 %r207,%r175,%r190;
}

	
	{sub.f16x2 %r210,%r172,%r187;
}

	
	{sub.f16x2 %r213,%r175,%r190;
}

	
	{add.f16x2 %r216,%r31,%r36;
}

	
	{add.f16x2 %r219,%r239,%r239;
}

	
	{sub.f16x2 %r222,%r31,%r36;
}

	
	{sub.f16x2 %r225,%r239,%r239;
}

	
	{add.f16x2 %r228,%r33,%r38;
}

	
	{add.f16x2 %r231,%r239,%r239;
}

	
	{sub.f16x2 %r234,%r33,%r38;
}

	
	{sub.f16x2 %r237,%r239,%r239;
}

	
	{xor.b32 %r240,%r234,0x80008000;
}

	
	{add.f16x2 %r242,%r216,%r228;
}

	
	{add.f16x2 %r245,%r219,%r231;
}

	
	{sub.f16x2 %r248,%r216,%r228;
}

	
	{sub.f16x2 %r251,%r219,%r231;
}

	
	{add.f16x2 %r254,%r222,%r237;
}

	
	{add.f16x2 %r257,%r225,%r240;
}

	
	{sub.f16x2 %r260,%r222,%r237;
}

	
	{sub.f16x2 %r263,%r225,%r240;
}

	mov.f32 %f3, 0f3F3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r266, {low,high};}


	mov.f32 %f13, 0fBF3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r267, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r270, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r271, {low,high};}


	
	{mul.f16x2 %r280,%r254,%r266;
}

	
	{mul.f16x2 %r283,%r257,%r267;
}

	
	{sub.f16x2 %r286,%r280,%r283;
}

	
	{mul.f16x2 %r289,%r254,%r267;
}

	
	{fma.rn.f16x2 %r292,%r257,%r266,%r289;
}

	
	{xor.b32 %r296,%r248,0x80008000;
}

	
	{mul.f16x2 %r298,%r260,%r270;
}

	
	{mul.f16x2 %r301,%r263,%r271;
}

	
	{sub.f16x2 %r304,%r298,%r301;
}

	
	{mul.f16x2 %r307,%r260,%r271;
}

	
	{fma.rn.f16x2 %r310,%r263,%r270,%r307;
}

	
	{add.f16x2 %r314,%r192,%r242;
}

	
	{add.f16x2 %r317,%r195,%r245;
}

	
	{sub.f16x2 %r320,%r192,%r242;
}

	
	{sub.f16x2 %r323,%r195,%r245;
}

	
	{add.f16x2 %r326,%r204,%r286;
}

	
	{add.f16x2 %r329,%r207,%r292;
}

	
	{sub.f16x2 %r332,%r204,%r286;
}

	
	{sub.f16x2 %r335,%r207,%r292;
}

	
	{add.f16x2 %r338,%r198,%r251;
}

	
	{add.f16x2 %r341,%r201,%r296;
}

	
	{sub.f16x2 %r344,%r198,%r251;
}

	
	{sub.f16x2 %r347,%r201,%r296;
}

	
	{add.f16x2 %r350,%r210,%r304;
}

	
	{add.f16x2 %r353,%r213,%r310;
}

	
	{sub.f16x2 %r356,%r210,%r304;
}

	
	{sub.f16x2 %r359,%r213,%r310;
}

	shr.u32 %r624, %r5, 9;
shl.b32 %r41, %r624, 12;
and.b32 %r42, %r5, 511;
cvt.rn.f32.u32	%f48, %r42;
mul.f32 %f49, %f48, 0f3AC90FDB;
cos.approx.f32 %f30, %f49;
sin.approx.f32 %f50, %f49;
neg.f32 %f31, %f50;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f30;
cvt.rn.f16.f32 high, %f31;
mov.b32 %r362, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r365, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r367, {high,high};}


	
	{mul.f16x2 %r369,%r329,%r367;
}

	
	{xor.b32 %r372,%r369,0x80008000;
}

	
	{fma.rn.f16x2 %r374,%r326,%r365,%r372;
}

	
	{mul.f16x2 %r378,%r326,%r367;
}

	
	{fma.rn.f16x2 %r381,%r329,%r365,%r378;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r385, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r387, {high,high};}


	mov.f32 %f44, 0fBF800000;
mov.f32 %f45, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r389, {low,high};}


	
	{mul.f16x2 %r390,%r387,%r389;
}

	
	{mul.f16x2 %r393,%r362,%r385;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r396, {high,low};}


	
	{fma.rn.f16x2 %r398,%r390,%r396,%r393;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r402, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r404, {high,high};}


	
	{mul.f16x2 %r406,%r341,%r404;
}

	
	{xor.b32 %r409,%r406,0x80008000;
}

	
	{fma.rn.f16x2 %r411,%r338,%r402,%r409;
}

	
	{mul.f16x2 %r415,%r338,%r404;
}

	
	{fma.rn.f16x2 %r418,%r341,%r402,%r415;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r422, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r424, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r426, {low,high};}


	
	{mul.f16x2 %r427,%r424,%r426;
}

	
	{mul.f16x2 %r430,%r398,%r422;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r398;
mov.b32 %r433, {high,low};}


	
	{fma.rn.f16x2 %r435,%r427,%r433,%r430;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r435;
mov.b32 %r439, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r435;
mov.b32 %r441, {high,high};}


	
	{mul.f16x2 %r443,%r353,%r441;
}

	
	{xor.b32 %r446,%r443,0x80008000;
}

	
	{fma.rn.f16x2 %r448,%r350,%r439,%r446;
}

	
	{mul.f16x2 %r452,%r350,%r441;
}

	
	{fma.rn.f16x2 %r455,%r353,%r439,%r452;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r459, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r461, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r463, {low,high};}


	
	{mul.f16x2 %r464,%r461,%r463;
}

	
	{mul.f16x2 %r467,%r435,%r459;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r435;
mov.b32 %r470, {high,low};}


	
	{fma.rn.f16x2 %r472,%r464,%r470,%r467;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r472;
mov.b32 %r476, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r472;
mov.b32 %r478, {high,high};}


	
	{mul.f16x2 %r480,%r323,%r478;
}

	
	{xor.b32 %r483,%r480,0x80008000;
}

	
	{fma.rn.f16x2 %r485,%r320,%r476,%r483;
}

	
	{mul.f16x2 %r489,%r320,%r478;
}

	
	{fma.rn.f16x2 %r492,%r323,%r476,%r489;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r496, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r498, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r500, {low,high};}


	
	{mul.f16x2 %r501,%r498,%r500;
}

	
	{mul.f16x2 %r504,%r472,%r496;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r472;
mov.b32 %r507, {high,low};}


	
	{fma.rn.f16x2 %r509,%r501,%r507,%r504;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r509;
mov.b32 %r513, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r509;
mov.b32 %r515, {high,high};}


	
	{mul.f16x2 %r517,%r335,%r515;
}

	
	{xor.b32 %r520,%r517,0x80008000;
}

	
	{fma.rn.f16x2 %r522,%r332,%r513,%r520;
}

	
	{mul.f16x2 %r526,%r332,%r515;
}

	
	{fma.rn.f16x2 %r529,%r335,%r513,%r526;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r533, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r535, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r537, {low,high};}


	
	{mul.f16x2 %r538,%r535,%r537;
}

	
	{mul.f16x2 %r541,%r509,%r533;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r509;
mov.b32 %r544, {high,low};}


	
	{fma.rn.f16x2 %r546,%r538,%r544,%r541;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r546;
mov.b32 %r550, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r546;
mov.b32 %r552, {high,high};}


	
	{mul.f16x2 %r554,%r347,%r552;
}

	
	{xor.b32 %r557,%r554,0x80008000;
}

	
	{fma.rn.f16x2 %r559,%r344,%r550,%r557;
}

	
	{mul.f16x2 %r563,%r344,%r552;
}

	
	{fma.rn.f16x2 %r566,%r347,%r550,%r563;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r570, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r362;
mov.b32 %r572, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r574, {low,high};}


	
	{mul.f16x2 %r575,%r572,%r574;
}

	
	{mul.f16x2 %r578,%r546,%r570;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r546;
mov.b32 %r581, {high,low};}


	
	{fma.rn.f16x2 %r583,%r575,%r581,%r578;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r583;
mov.b32 %r587, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r583;
mov.b32 %r589, {high,high};}


	
	{mul.f16x2 %r591,%r359,%r589;
}

	
	{xor.b32 %r594,%r591,0x80008000;
}

	
	{fma.rn.f16x2 %r596,%r356,%r587,%r594;
}

	
	{mul.f16x2 %r600,%r356,%r589;
}

	
	{fma.rn.f16x2 %r603,%r359,%r587,%r600;
}

	shl.b32 %r625, %r624, 15;
add.s32 %r57, %r161, %r625;
barrier.sync 0;
shl.b32 %r627, %r42, 6;
add.s32 %r628, %r57, %r627;
st.shared.u32 [%r628], %r314;
st.shared.u32 [%r628+4], %r317;
st.shared.u32 [%r628+8], %r374;
st.shared.u32 [%r628+12], %r381;
st.shared.u32 [%r628+16], %r411;
st.shared.u32 [%r628+20], %r418;
st.shared.u32 [%r628+24], %r448;
st.shared.u32 [%r628+28], %r455;
st.shared.u32 [%r628+32], %r485;
st.shared.u32 [%r628+36], %r492;
st.shared.u32 [%r628+40], %r522;
st.shared.u32 [%r628+44], %r529;
st.shared.u32 [%r628+48], %r559;
st.shared.u32 [%r628+52], %r566;
st.shared.u32 [%r628+56], %r596;
st.shared.u32 [%r628+60], %r603;
barrier.sync 0;
shl.b32 %r1087, %r42, 3;
add.s32 %r1088, %r57, %r1087;
ld.shared.u32 %r630, [%r1088];
ld.shared.u32 %r633, [%r1088+4];
ld.shared.u32 %r680, [%r1088+4096];
ld.shared.u32 %r683, [%r1088+4100];
ld.shared.u32 %r642, [%r1088+8192];
ld.shared.u32 %r645, [%r1088+8196];
ld.shared.u32 %r692, [%r1088+12288];
ld.shared.u32 %r695, [%r1088+12292];
ld.shared.u32 %r631, [%r1088+16384];
ld.shared.u32 %r634, [%r1088+16388];
ld.shared.u32 %r681, [%r1088+20480];
ld.shared.u32 %r684, [%r1088+20484];
ld.shared.u32 %r643, [%r1088+24576];
ld.shared.u32 %r646, [%r1088+24580];
ld.shared.u32 %r693, [%r1088+28672];
ld.shared.u32 %r696, [%r1088+28676];

	{add.f16x2 %r629,%r630,%r631;
}

	
	{add.f16x2 %r632,%r633,%r634;
}

	
	{sub.f16x2 %r635,%r630,%r631;
}

	
	{sub.f16x2 %r638,%r633,%r634;
}

	
	{add.f16x2 %r641,%r642,%r643;
}

	
	{add.f16x2 %r644,%r645,%r646;
}

	
	{sub.f16x2 %r647,%r642,%r643;
}

	
	{sub.f16x2 %r650,%r645,%r646;
}

	
	{xor.b32 %r653,%r647,0x80008000;
}

	
	{add.f16x2 %r655,%r629,%r641;
}

	
	{add.f16x2 %r658,%r632,%r644;
}

	
	{sub.f16x2 %r661,%r629,%r641;
}

	
	{sub.f16x2 %r664,%r632,%r644;
}

	
	{add.f16x2 %r667,%r635,%r650;
}

	
	{add.f16x2 %r670,%r638,%r653;
}

	
	{sub.f16x2 %r673,%r635,%r650;
}

	
	{sub.f16x2 %r676,%r638,%r653;
}

	
	{add.f16x2 %r679,%r680,%r681;
}

	
	{add.f16x2 %r682,%r683,%r684;
}

	
	{sub.f16x2 %r685,%r680,%r681;
}

	
	{sub.f16x2 %r688,%r683,%r684;
}

	
	{add.f16x2 %r691,%r692,%r693;
}

	
	{add.f16x2 %r694,%r695,%r696;
}

	
	{sub.f16x2 %r697,%r692,%r693;
}

	
	{sub.f16x2 %r700,%r695,%r696;
}

	
	{xor.b32 %r703,%r697,0x80008000;
}

	
	{add.f16x2 %r705,%r679,%r691;
}

	
	{add.f16x2 %r708,%r682,%r694;
}

	
	{sub.f16x2 %r711,%r679,%r691;
}

	
	{sub.f16x2 %r714,%r682,%r694;
}

	
	{add.f16x2 %r717,%r685,%r700;
}

	
	{add.f16x2 %r720,%r688,%r703;
}

	
	{sub.f16x2 %r723,%r685,%r700;
}

	
	{sub.f16x2 %r726,%r688,%r703;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r729, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r730, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r733, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r734, {low,high};}


	
	{mul.f16x2 %r743,%r717,%r729;
}

	
	{mul.f16x2 %r746,%r720,%r730;
}

	
	{sub.f16x2 %r749,%r743,%r746;
}

	
	{mul.f16x2 %r752,%r717,%r730;
}

	
	{fma.rn.f16x2 %r755,%r720,%r729,%r752;
}

	
	{xor.b32 %r759,%r711,0x80008000;
}

	
	{mul.f16x2 %r761,%r723,%r733;
}

	
	{mul.f16x2 %r764,%r726,%r734;
}

	
	{sub.f16x2 %r767,%r761,%r764;
}

	
	{mul.f16x2 %r770,%r723,%r734;
}

	
	{fma.rn.f16x2 %r773,%r726,%r733,%r770;
}

	
	{add.f16x2 %r777,%r655,%r705;
}

	
	{add.f16x2 %r780,%r658,%r708;
}

	
	{sub.f16x2 %r783,%r655,%r705;
}

	
	{sub.f16x2 %r786,%r658,%r708;
}

	
	{add.f16x2 %r789,%r667,%r749;
}

	
	{add.f16x2 %r792,%r670,%r755;
}

	
	{sub.f16x2 %r795,%r667,%r749;
}

	
	{sub.f16x2 %r798,%r670,%r755;
}

	
	{add.f16x2 %r801,%r661,%r714;
}

	
	{add.f16x2 %r804,%r664,%r759;
}

	
	{sub.f16x2 %r807,%r661,%r714;
}

	
	{sub.f16x2 %r810,%r664,%r759;
}

	
	{add.f16x2 %r813,%r673,%r767;
}

	
	{add.f16x2 %r816,%r676,%r773;
}

	
	{sub.f16x2 %r819,%r673,%r767;
}

	
	{sub.f16x2 %r822,%r676,%r773;
}

	and.b32 %r61, %r5, 504;
bfe.u32 %r1089, %r5, 3, 6;
and.b32 %r1090, %r5, 7;
add.s32 %r1091, %r41, %r1090;
cvt.rn.f32.u32	%f97, %r1089;
mul.f32 %f98, %f97, 0f3C490FDB;
cos.approx.f32 %f79, %f98;
sin.approx.f32 %f99, %f98;
neg.f32 %f80, %f99;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f79;
cvt.rn.f16.f32 high, %f80;
mov.b32 %r825, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r828, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r830, {high,high};}


	
	{mul.f16x2 %r832,%r792,%r830;
}

	
	{xor.b32 %r835,%r832,0x80008000;
}

	
	{fma.rn.f16x2 %r837,%r789,%r828,%r835;
}

	
	{mul.f16x2 %r841,%r789,%r830;
}

	
	{fma.rn.f16x2 %r844,%r792,%r828,%r841;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r848, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r850, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r852, {low,high};}


	
	{mul.f16x2 %r853,%r850,%r852;
}

	
	{mul.f16x2 %r856,%r825,%r848;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r859, {high,low};}


	
	{fma.rn.f16x2 %r861,%r853,%r859,%r856;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r865, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r867, {high,high};}


	
	{mul.f16x2 %r869,%r804,%r867;
}

	
	{xor.b32 %r872,%r869,0x80008000;
}

	
	{fma.rn.f16x2 %r874,%r801,%r865,%r872;
}

	
	{mul.f16x2 %r878,%r801,%r867;
}

	
	{fma.rn.f16x2 %r881,%r804,%r865,%r878;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r885, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r887, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r889, {low,high};}


	
	{mul.f16x2 %r890,%r887,%r889;
}

	
	{mul.f16x2 %r893,%r861,%r885;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r861;
mov.b32 %r896, {high,low};}


	
	{fma.rn.f16x2 %r898,%r890,%r896,%r893;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r898;
mov.b32 %r902, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r898;
mov.b32 %r904, {high,high};}


	
	{mul.f16x2 %r906,%r816,%r904;
}

	
	{xor.b32 %r909,%r906,0x80008000;
}

	
	{fma.rn.f16x2 %r911,%r813,%r902,%r909;
}

	
	{mul.f16x2 %r915,%r813,%r904;
}

	
	{fma.rn.f16x2 %r918,%r816,%r902,%r915;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r922, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r924, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r926, {low,high};}


	
	{mul.f16x2 %r927,%r924,%r926;
}

	
	{mul.f16x2 %r930,%r898,%r922;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r898;
mov.b32 %r933, {high,low};}


	
	{fma.rn.f16x2 %r935,%r927,%r933,%r930;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r935;
mov.b32 %r939, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r935;
mov.b32 %r941, {high,high};}


	
	{mul.f16x2 %r943,%r786,%r941;
}

	
	{xor.b32 %r946,%r943,0x80008000;
}

	
	{fma.rn.f16x2 %r948,%r783,%r939,%r946;
}

	
	{mul.f16x2 %r952,%r783,%r941;
}

	
	{fma.rn.f16x2 %r955,%r786,%r939,%r952;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r959, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r961, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r963, {low,high};}


	
	{mul.f16x2 %r964,%r961,%r963;
}

	
	{mul.f16x2 %r967,%r935,%r959;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r935;
mov.b32 %r970, {high,low};}


	
	{fma.rn.f16x2 %r972,%r964,%r970,%r967;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r972;
mov.b32 %r976, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r972;
mov.b32 %r978, {high,high};}


	
	{mul.f16x2 %r980,%r798,%r978;
}

	
	{xor.b32 %r983,%r980,0x80008000;
}

	
	{fma.rn.f16x2 %r985,%r795,%r976,%r983;
}

	
	{mul.f16x2 %r989,%r795,%r978;
}

	
	{fma.rn.f16x2 %r992,%r798,%r976,%r989;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r996, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r998, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1000, {low,high};}


	
	{mul.f16x2 %r1001,%r998,%r1000;
}

	
	{mul.f16x2 %r1004,%r972,%r996;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r972;
mov.b32 %r1007, {high,low};}


	
	{fma.rn.f16x2 %r1009,%r1001,%r1007,%r1004;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1009;
mov.b32 %r1013, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1009;
mov.b32 %r1015, {high,high};}


	
	{mul.f16x2 %r1017,%r810,%r1015;
}

	
	{xor.b32 %r1020,%r1017,0x80008000;
}

	
	{fma.rn.f16x2 %r1022,%r807,%r1013,%r1020;
}

	
	{mul.f16x2 %r1026,%r807,%r1015;
}

	
	{fma.rn.f16x2 %r1029,%r810,%r1013,%r1026;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r1033, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r825;
mov.b32 %r1035, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1037, {low,high};}


	
	{mul.f16x2 %r1038,%r1035,%r1037;
}

	
	{mul.f16x2 %r1041,%r1009,%r1033;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1009;
mov.b32 %r1044, {high,low};}


	
	{fma.rn.f16x2 %r1046,%r1038,%r1044,%r1041;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1046;
mov.b32 %r1050, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1046;
mov.b32 %r1052, {high,high};}


	
	{mul.f16x2 %r1054,%r822,%r1052;
}

	
	{xor.b32 %r1057,%r1054,0x80008000;
}

	
	{fma.rn.f16x2 %r1059,%r819,%r1050,%r1057;
}

	
	{mul.f16x2 %r1063,%r819,%r1052;
}

	
	{fma.rn.f16x2 %r1066,%r822,%r1050,%r1063;
}

	shl.b32 %r1092, %r1091, 3;
add.s32 %r76, %r161, %r1092;
barrier.sync 0;
shl.b32 %r1094, %r61, 6;
add.s32 %r1095, %r76, %r1094;
st.shared.u32 [%r1095], %r777;
st.shared.u32 [%r1095+4], %r780;
st.shared.u32 [%r1095+64], %r837;
st.shared.u32 [%r1095+68], %r844;
st.shared.u32 [%r1095+128], %r874;
st.shared.u32 [%r1095+132], %r881;
st.shared.u32 [%r1095+192], %r911;
st.shared.u32 [%r1095+196], %r918;
st.shared.u32 [%r1095+256], %r948;
st.shared.u32 [%r1095+260], %r955;
st.shared.u32 [%r1095+320], %r985;
st.shared.u32 [%r1095+324], %r992;
st.shared.u32 [%r1095+384], %r1022;
st.shared.u32 [%r1095+388], %r1029;
st.shared.u32 [%r1095+448], %r1059;
st.shared.u32 [%r1095+452], %r1066;
barrier.sync 0;
shl.b32 %r1554, %r61, 3;
add.s32 %r1555, %r76, %r1554;
ld.shared.u32 %r1097, [%r1555];
ld.shared.u32 %r1100, [%r1555+4];
ld.shared.u32 %r1147, [%r1555+4096];
ld.shared.u32 %r1150, [%r1555+4100];
ld.shared.u32 %r1109, [%r1555+8192];
ld.shared.u32 %r1112, [%r1555+8196];
ld.shared.u32 %r1159, [%r1555+12288];
ld.shared.u32 %r1162, [%r1555+12292];
ld.shared.u32 %r1098, [%r1555+16384];
ld.shared.u32 %r1101, [%r1555+16388];
ld.shared.u32 %r1148, [%r1555+20480];
ld.shared.u32 %r1151, [%r1555+20484];
ld.shared.u32 %r1110, [%r1555+24576];
ld.shared.u32 %r1113, [%r1555+24580];
ld.shared.u32 %r1160, [%r1555+28672];
ld.shared.u32 %r1163, [%r1555+28676];

	{add.f16x2 %r1096,%r1097,%r1098;
}

	
	{add.f16x2 %r1099,%r1100,%r1101;
}

	
	{sub.f16x2 %r1102,%r1097,%r1098;
}

	
	{sub.f16x2 %r1105,%r1100,%r1101;
}

	
	{add.f16x2 %r1108,%r1109,%r1110;
}

	
	{add.f16x2 %r1111,%r1112,%r1113;
}

	
	{sub.f16x2 %r1114,%r1109,%r1110;
}

	
	{sub.f16x2 %r1117,%r1112,%r1113;
}

	
	{xor.b32 %r1120,%r1114,0x80008000;
}

	
	{add.f16x2 %r1122,%r1096,%r1108;
}

	
	{add.f16x2 %r1125,%r1099,%r1111;
}

	
	{sub.f16x2 %r1128,%r1096,%r1108;
}

	
	{sub.f16x2 %r1131,%r1099,%r1111;
}

	
	{add.f16x2 %r1134,%r1102,%r1117;
}

	
	{add.f16x2 %r1137,%r1105,%r1120;
}

	
	{sub.f16x2 %r1140,%r1102,%r1117;
}

	
	{sub.f16x2 %r1143,%r1105,%r1120;
}

	
	{add.f16x2 %r1146,%r1147,%r1148;
}

	
	{add.f16x2 %r1149,%r1150,%r1151;
}

	
	{sub.f16x2 %r1152,%r1147,%r1148;
}

	
	{sub.f16x2 %r1155,%r1150,%r1151;
}

	
	{add.f16x2 %r1158,%r1159,%r1160;
}

	
	{add.f16x2 %r1161,%r1162,%r1163;
}

	
	{sub.f16x2 %r1164,%r1159,%r1160;
}

	
	{sub.f16x2 %r1167,%r1162,%r1163;
}

	
	{xor.b32 %r1170,%r1164,0x80008000;
}

	
	{add.f16x2 %r1172,%r1146,%r1158;
}

	
	{add.f16x2 %r1175,%r1149,%r1161;
}

	
	{sub.f16x2 %r1178,%r1146,%r1158;
}

	
	{sub.f16x2 %r1181,%r1149,%r1161;
}

	
	{add.f16x2 %r1184,%r1152,%r1167;
}

	
	{add.f16x2 %r1187,%r1155,%r1170;
}

	
	{sub.f16x2 %r1190,%r1152,%r1167;
}

	
	{sub.f16x2 %r1193,%r1155,%r1170;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r1196, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1197, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1200, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1201, {low,high};}


	
	{mul.f16x2 %r1210,%r1184,%r1196;
}

	
	{mul.f16x2 %r1213,%r1187,%r1197;
}

	
	{sub.f16x2 %r1216,%r1210,%r1213;
}

	
	{mul.f16x2 %r1219,%r1184,%r1197;
}

	
	{fma.rn.f16x2 %r1222,%r1187,%r1196,%r1219;
}

	
	{xor.b32 %r1226,%r1178,0x80008000;
}

	
	{mul.f16x2 %r1228,%r1190,%r1200;
}

	
	{mul.f16x2 %r1231,%r1193,%r1201;
}

	
	{sub.f16x2 %r1234,%r1228,%r1231;
}

	
	{mul.f16x2 %r1237,%r1190,%r1201;
}

	
	{fma.rn.f16x2 %r1240,%r1193,%r1200,%r1237;
}

	
	{add.f16x2 %r1244,%r1122,%r1172;
}

	
	{add.f16x2 %r1247,%r1125,%r1175;
}

	
	{sub.f16x2 %r1250,%r1122,%r1172;
}

	
	{sub.f16x2 %r1253,%r1125,%r1175;
}

	
	{add.f16x2 %r1256,%r1134,%r1216;
}

	
	{add.f16x2 %r1259,%r1137,%r1222;
}

	
	{sub.f16x2 %r1262,%r1134,%r1216;
}

	
	{sub.f16x2 %r1265,%r1137,%r1222;
}

	
	{add.f16x2 %r1268,%r1128,%r1181;
}

	
	{add.f16x2 %r1271,%r1131,%r1226;
}

	
	{sub.f16x2 %r1274,%r1128,%r1181;
}

	
	{sub.f16x2 %r1277,%r1131,%r1226;
}

	
	{add.f16x2 %r1280,%r1140,%r1234;
}

	
	{add.f16x2 %r1283,%r1143,%r1240;
}

	
	{sub.f16x2 %r1286,%r1140,%r1234;
}

	
	{sub.f16x2 %r1289,%r1143,%r1240;
}

	and.b32 %r80, %r5, 448;
bfe.u32 %r1556, %r5, 6, 3;
and.b32 %r1557, %r5, 63;
add.s32 %r1558, %r41, %r1557;
cvt.rn.f32.u32	%f146, %r1556;
mul.f32 %f147, %f146, 0f3DC90FDB;
cos.approx.f32 %f128, %f147;
sin.approx.f32 %f148, %f147;
neg.f32 %f129, %f148;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f128;
cvt.rn.f16.f32 high, %f129;
mov.b32 %r1292, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1295, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1297, {high,high};}


	
	{mul.f16x2 %r1299,%r1259,%r1297;
}

	
	{xor.b32 %r1302,%r1299,0x80008000;
}

	
	{fma.rn.f16x2 %r1304,%r1256,%r1295,%r1302;
}

	
	{mul.f16x2 %r1308,%r1256,%r1297;
}

	
	{fma.rn.f16x2 %r1311,%r1259,%r1295,%r1308;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1315, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1317, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1319, {low,high};}


	
	{mul.f16x2 %r1320,%r1317,%r1319;
}

	
	{mul.f16x2 %r1323,%r1292,%r1315;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1326, {high,low};}


	
	{fma.rn.f16x2 %r1328,%r1320,%r1326,%r1323;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1332, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1334, {high,high};}


	
	{mul.f16x2 %r1336,%r1271,%r1334;
}

	
	{xor.b32 %r1339,%r1336,0x80008000;
}

	
	{fma.rn.f16x2 %r1341,%r1268,%r1332,%r1339;
}

	
	{mul.f16x2 %r1345,%r1268,%r1334;
}

	
	{fma.rn.f16x2 %r1348,%r1271,%r1332,%r1345;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1352, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1354, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1356, {low,high};}


	
	{mul.f16x2 %r1357,%r1354,%r1356;
}

	
	{mul.f16x2 %r1360,%r1328,%r1352;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1328;
mov.b32 %r1363, {high,low};}


	
	{fma.rn.f16x2 %r1365,%r1357,%r1363,%r1360;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1365;
mov.b32 %r1369, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1365;
mov.b32 %r1371, {high,high};}


	
	{mul.f16x2 %r1373,%r1283,%r1371;
}

	
	{xor.b32 %r1376,%r1373,0x80008000;
}

	
	{fma.rn.f16x2 %r1378,%r1280,%r1369,%r1376;
}

	
	{mul.f16x2 %r1382,%r1280,%r1371;
}

	
	{fma.rn.f16x2 %r1385,%r1283,%r1369,%r1382;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1389, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1391, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1393, {low,high};}


	
	{mul.f16x2 %r1394,%r1391,%r1393;
}

	
	{mul.f16x2 %r1397,%r1365,%r1389;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1365;
mov.b32 %r1400, {high,low};}


	
	{fma.rn.f16x2 %r1402,%r1394,%r1400,%r1397;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1402;
mov.b32 %r1406, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1402;
mov.b32 %r1408, {high,high};}


	
	{mul.f16x2 %r1410,%r1253,%r1408;
}

	
	{xor.b32 %r1413,%r1410,0x80008000;
}

	
	{fma.rn.f16x2 %r1415,%r1250,%r1406,%r1413;
}

	
	{mul.f16x2 %r1419,%r1250,%r1408;
}

	
	{fma.rn.f16x2 %r1422,%r1253,%r1406,%r1419;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1426, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1428, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1430, {low,high};}


	
	{mul.f16x2 %r1431,%r1428,%r1430;
}

	
	{mul.f16x2 %r1434,%r1402,%r1426;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1402;
mov.b32 %r1437, {high,low};}


	
	{fma.rn.f16x2 %r1439,%r1431,%r1437,%r1434;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1439;
mov.b32 %r1443, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1439;
mov.b32 %r1445, {high,high};}


	
	{mul.f16x2 %r1447,%r1265,%r1445;
}

	
	{xor.b32 %r1450,%r1447,0x80008000;
}

	
	{fma.rn.f16x2 %r1452,%r1262,%r1443,%r1450;
}

	
	{mul.f16x2 %r1456,%r1262,%r1445;
}

	
	{fma.rn.f16x2 %r1459,%r1265,%r1443,%r1456;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1463, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1465, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1467, {low,high};}


	
	{mul.f16x2 %r1468,%r1465,%r1467;
}

	
	{mul.f16x2 %r1471,%r1439,%r1463;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1439;
mov.b32 %r1474, {high,low};}


	
	{fma.rn.f16x2 %r1476,%r1468,%r1474,%r1471;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1476;
mov.b32 %r1480, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1476;
mov.b32 %r1482, {high,high};}


	
	{mul.f16x2 %r1484,%r1277,%r1482;
}

	
	{xor.b32 %r1487,%r1484,0x80008000;
}

	
	{fma.rn.f16x2 %r1489,%r1274,%r1480,%r1487;
}

	
	{mul.f16x2 %r1493,%r1274,%r1482;
}

	
	{fma.rn.f16x2 %r1496,%r1277,%r1480,%r1493;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1500, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1292;
mov.b32 %r1502, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1504, {low,high};}


	
	{mul.f16x2 %r1505,%r1502,%r1504;
}

	
	{mul.f16x2 %r1508,%r1476,%r1500;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1476;
mov.b32 %r1511, {high,low};}


	
	{fma.rn.f16x2 %r1513,%r1505,%r1511,%r1508;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1513;
mov.b32 %r1517, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1513;
mov.b32 %r1519, {high,high};}


	
	{mul.f16x2 %r1521,%r1289,%r1519;
}

	
	{xor.b32 %r1524,%r1521,0x80008000;
}

	
	{fma.rn.f16x2 %r1526,%r1286,%r1517,%r1524;
}

	
	{mul.f16x2 %r1530,%r1286,%r1519;
}

	
	{fma.rn.f16x2 %r1533,%r1289,%r1517,%r1530;
}

	shl.b32 %r1559, %r1558, 3;
add.s32 %r95, %r161, %r1559;
barrier.sync 0;
shl.b32 %r1561, %r80, 6;
add.s32 %r1562, %r95, %r1561;
st.shared.u32 [%r1562], %r1244;
st.shared.u32 [%r1562+4], %r1247;
st.shared.u32 [%r1562+512], %r1304;
st.shared.u32 [%r1562+516], %r1311;
st.shared.u32 [%r1562+1024], %r1341;
st.shared.u32 [%r1562+1028], %r1348;
st.shared.u32 [%r1562+1536], %r1378;
st.shared.u32 [%r1562+1540], %r1385;
st.shared.u32 [%r1562+2048], %r1415;
st.shared.u32 [%r1562+2052], %r1422;
st.shared.u32 [%r1562+2560], %r1452;
st.shared.u32 [%r1562+2564], %r1459;
st.shared.u32 [%r1562+3072], %r1489;
st.shared.u32 [%r1562+3076], %r1496;
st.shared.u32 [%r1562+3584], %r1526;
st.shared.u32 [%r1562+3588], %r1533;
barrier.sync 0;
shl.b32 %r1807, %r80, 3;
add.s32 %r1808, %r95, %r1807;
ld.shared.u32 %r1564, [%r1808];
ld.shared.u32 %r1567, [%r1808+4];
ld.shared.u32 %r1614, [%r1808+4096];
ld.shared.u32 %r1617, [%r1808+4100];
ld.shared.u32 %r1576, [%r1808+8192];
ld.shared.u32 %r1579, [%r1808+8196];
ld.shared.u32 %r1626, [%r1808+12288];
ld.shared.u32 %r1629, [%r1808+12292];
ld.shared.u32 %r1565, [%r1808+16384];
ld.shared.u32 %r1568, [%r1808+16388];
ld.shared.u32 %r1615, [%r1808+20480];
ld.shared.u32 %r1618, [%r1808+20484];
ld.shared.u32 %r1577, [%r1808+24576];
ld.shared.u32 %r1580, [%r1808+24580];
ld.shared.u32 %r1627, [%r1808+28672];
ld.shared.u32 %r1630, [%r1808+28676];

	{add.f16x2 %r1563,%r1564,%r1565;
}

	
	{add.f16x2 %r1566,%r1567,%r1568;
}

	
	{sub.f16x2 %r1569,%r1564,%r1565;
}

	
	{sub.f16x2 %r1572,%r1567,%r1568;
}

	
	{add.f16x2 %r1575,%r1576,%r1577;
}

	
	{add.f16x2 %r1578,%r1579,%r1580;
}

	
	{sub.f16x2 %r1581,%r1576,%r1577;
}

	
	{sub.f16x2 %r1584,%r1579,%r1580;
}

	
	{xor.b32 %r1587,%r1581,0x80008000;
}

	
	{add.f16x2 %r1589,%r1563,%r1575;
}

	
	{add.f16x2 %r1592,%r1566,%r1578;
}

	
	{sub.f16x2 %r1595,%r1563,%r1575;
}

	
	{sub.f16x2 %r1598,%r1566,%r1578;
}

	
	{add.f16x2 %r1601,%r1569,%r1584;
}

	
	{add.f16x2 %r1604,%r1572,%r1587;
}

	
	{sub.f16x2 %r1607,%r1569,%r1584;
}

	
	{sub.f16x2 %r1610,%r1572,%r1587;
}

	
	{add.f16x2 %r1613,%r1614,%r1615;
}

	
	{add.f16x2 %r1616,%r1617,%r1618;
}

	
	{sub.f16x2 %r1619,%r1614,%r1615;
}

	
	{sub.f16x2 %r1622,%r1617,%r1618;
}

	
	{add.f16x2 %r1625,%r1626,%r1627;
}

	
	{add.f16x2 %r1628,%r1629,%r1630;
}

	
	{sub.f16x2 %r1631,%r1626,%r1627;
}

	
	{sub.f16x2 %r1634,%r1629,%r1630;
}

	
	{xor.b32 %r1637,%r1631,0x80008000;
}

	
	{add.f16x2 %r1639,%r1613,%r1625;
}

	
	{add.f16x2 %r1642,%r1616,%r1628;
}

	
	{sub.f16x2 %r1645,%r1613,%r1625;
}

	
	{sub.f16x2 %r1648,%r1616,%r1628;
}

	
	{add.f16x2 %r1651,%r1619,%r1634;
}

	
	{add.f16x2 %r1654,%r1622,%r1637;
}

	
	{sub.f16x2 %r1657,%r1619,%r1634;
}

	
	{sub.f16x2 %r1660,%r1622,%r1637;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r1663, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1664, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1667, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1668, {low,high};}


	
	{mul.f16x2 %r1677,%r1651,%r1663;
}

	
	{mul.f16x2 %r1680,%r1654,%r1664;
}

	
	{sub.f16x2 %r1683,%r1677,%r1680;
}

	
	{mul.f16x2 %r1686,%r1651,%r1664;
}

	
	{fma.rn.f16x2 %r1689,%r1654,%r1663,%r1686;
}

	
	{xor.b32 %r1693,%r1645,0x80008000;
}

	
	{mul.f16x2 %r1695,%r1657,%r1667;
}

	
	{mul.f16x2 %r1698,%r1660,%r1668;
}

	
	{sub.f16x2 %r1701,%r1695,%r1698;
}

	
	{mul.f16x2 %r1704,%r1657,%r1668;
}

	
	{fma.rn.f16x2 %r1707,%r1660,%r1667,%r1704;
}

	
	{add.f16x2 %r1711,%r1589,%r1639;
}

	
	{add.f16x2 %r1714,%r1592,%r1642;
}

	
	{sub.f16x2 %r1717,%r1589,%r1639;
}

	
	{sub.f16x2 %r1720,%r1592,%r1642;
}

	
	{add.f16x2 %r1723,%r1601,%r1683;
}

	
	{add.f16x2 %r1726,%r1604,%r1689;
}

	
	{add.f16x2 %r1735,%r1595,%r1648;
}

	
	{add.f16x2 %r1738,%r1598,%r1693;
}

	
	{add.f16x2 %r1747,%r1607,%r1701;
}

	
	{add.f16x2 %r1750,%r1610,%r1707;
}

	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1711;
mov.b32 {blow,bhigh}, %r1714;
mov.b32 %r1759, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1711;
mov.b32 {blow,bhigh}, %r1714;
mov.b32 %r1762, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1723;
mov.b32 {blow,bhigh}, %r1726;
mov.b32 %r1765, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1723;
mov.b32 {blow,bhigh}, %r1726;
mov.b32 %r1768, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1735;
mov.b32 {blow,bhigh}, %r1738;
mov.b32 %r1771, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1735;
mov.b32 {blow,bhigh}, %r1738;
mov.b32 %r1774, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1747;
mov.b32 {blow,bhigh}, %r1750;
mov.b32 %r1777, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1747;
mov.b32 {blow,bhigh}, %r1750;
mov.b32 %r1780, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1717;
mov.b32 {blow,bhigh}, %r1720;
mov.b32 %r1783, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r1717;
mov.b32 {blow,bhigh}, %r1720;
mov.b32 %r1786, {ahigh,bhigh};}


	mad.lo.s32 %r1809, %r2, 4098, %r5;
cvta.to.global.u64 %rd9, %rd5;
mul.wide.u32 %rd10, %r1809, 4;
add.s64 %rd3, %rd9, %rd10;
@%p2 bra BB11_11;
bra.uni BB11_5;

BB11_11:
st.global.u32 [%rd3], %r1759;
st.global.u32 [%rd3+2048], %r1765;
st.global.u32 [%rd3+4096], %r1771;
st.global.u32 [%rd3+6144], %r1777;
setp.gt.u32	%p9, %r34, 2048;
@%p9 bra BB11_13;

st.global.u32 [%rd3+8192], %r1783;

BB11_13:
st.global.u32 [%rd3+8196], %r1762;
st.global.u32 [%rd3+10244], %r1768;
st.global.u32 [%rd3+12292], %r1774;
st.global.u32 [%rd3+14340], %r1780;
@%p9 bra BB11_15;
bra.uni BB11_14;

BB11_5:
shl.b32 %r1810, %r3, 1;
add.s32 %r1811, %r1810, -1;
setp.lt.u32	%p5, %r1811, %r1;
st.global.u32 [%rd3], %r1759;
st.global.u32 [%rd3+2048], %r1765;
st.global.u32 [%rd3+4096], %r1771;
st.global.u32 [%rd3+6144], %r1777;
@%p5 bra BB11_8;
bra.uni BB11_6;

BB11_8:
setp.gt.u32	%p7, %r34, 2048;
@%p7 bra BB11_10;

st.global.u32 [%rd3+8192], %r1783;

BB11_10:
st.global.u32 [%rd3+8196], %r1762;
st.global.u32 [%rd3+10244], %r1768;
st.global.u32 [%rd3+12292], %r1774;
st.global.u32 [%rd3+14340], %r1780;
@%p7 bra BB11_15;

BB11_14:
st.global.u32 [%rd3+16388], %r1786;
bra.uni BB11_15;

BB11_6:
setp.gt.u32	%p6, %r34, 2048;
@%p6 bra BB11_15;

st.global.u32 [%rd3+8192], %r1783;

BB11_15:
ret;
}


.weak .entry _Z14vector_fft_r2cILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E(
.param .align 8 .b8 _Z14vector_fft_r2cILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0[120]
)
.maxntid 1024, 1, 1
{
.reg .pred %p<11>;
.reg .b16 %rs<9>;
.reg .f32 %f<198>;
.reg .b32 %r<2201>;
.reg .f64 %fd<2>;
.reg .b64 %rd<11>;


ld.param.u32 %r1, [_Z14vector_fft_r2cILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+40];
ld.param.u64 %rd5, [_Z14vector_fft_r2cILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+8];
ld.param.u64 %rd4, [_Z14vector_fft_r2cILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0];
ld.param.u8 %rs1, [_Z14vector_fft_r2cILj8192ELj8ELj2EL9padding_t1EL9twiddle_t5EL20loadstore_modifier_t2EL8layout_t0Ej6__halfEv18kernel_arguments_tIT6_E_param_0+112];
setp.eq.s16	%p1, %rs1, 0;
selp.b32	%r182, 4096, 4097, %p1;
mov.u32 %r2, %ctaid.x;
shl.b32 %r183, %r2, 1;
add.s32 %r184, %r183, 1;
mov.u32 %r3, %nctaid.x;
add.s32 %r4, %r3, -1;
setp.lt.u32	%p2, %r2, %r4;
mov.u32 %r5, %tid.x;
mad.lo.s32 %r185, %r183, %r182, %r5;
cvta.to.global.u64 %rd6, %rd4;
mul.wide.u32 %rd7, %r185, 4;
add.s64 %rd1, %rd6, %rd7;
mad.lo.s32 %r186, %r184, %r182, %r5;
mul.wide.u32 %rd8, %r186, 4;
add.s64 %rd2, %rd6, %rd8;
@%p2 bra BB12_2;
bra.uni BB12_1;

BB12_2:
ld.global.u32 %r2200, [%rd1];
ld.global.u32 %r2198, [%rd1+4096];
ld.global.u32 %r2196, [%rd1+8192];
ld.global.u32 %r2194, [%rd1+12288];
bra.uni BB12_3;

BB12_1:
shl.b32 %r188, %r3, 1;
add.s32 %r189, %r188, -1;
ld.global.u32 %r2200, [%rd1];
ld.global.u32 %r2198, [%rd1+4096];
ld.global.u32 %r2196, [%rd1+8192];
ld.global.u32 %r2194, [%rd1+12288];
setp.ge.u32	%p3, %r189, %r1;
@%p3 bra BB12_4;

BB12_3:
ld.global.u32 %r2199, [%rd2];
ld.global.u32 %r2197, [%rd2+4096];
ld.global.u32 %r2195, [%rd2+8192];
ld.global.u32 %r2193, [%rd2+12288];

BB12_4:

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2200;
mov.b32 {blow,bhigh}, %r2199;
mov.b32 %r190, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2200;
mov.b32 {blow,bhigh}, %r2199;
mov.b32 %r193, {ahigh,bhigh};}


	shl.b32 %r214, %r5, 3;
mov.u32 %r215, smem_full;
add.s32 %r216, %r215, %r214;
st.shared.u32 [%r216], %r190;
st.shared.u32 [%r216+4], %r193;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2198;
mov.b32 {blow,bhigh}, %r2197;
mov.b32 %r196, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2198;
mov.b32 {blow,bhigh}, %r2197;
mov.b32 %r199, {ahigh,bhigh};}


	st.shared.u32 [%r216+8192], %r196;
st.shared.u32 [%r216+8196], %r199;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2196;
mov.b32 {blow,bhigh}, %r2195;
mov.b32 %r202, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2196;
mov.b32 {blow,bhigh}, %r2195;
mov.b32 %r205, {ahigh,bhigh};}


	st.shared.u32 [%r216+16384], %r202;
st.shared.u32 [%r216+16388], %r205;

	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2194;
mov.b32 {blow,bhigh}, %r2193;
mov.b32 %r208, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2194;
mov.b32 {blow,bhigh}, %r2193;
mov.b32 %r211, {ahigh,bhigh};}


	st.shared.u32 [%r216+24576], %r208;
st.shared.u32 [%r216+24580], %r211;
barrier.sync 0;
shl.b32 %r217, %r5, 2;
add.s32 %r219, %r215, %r217;
ld.shared.u32 %r30, [%r219];
ld.shared.u32 %r31, [%r219+4096];
ld.shared.u32 %r32, [%r219+8192];
ld.shared.u32 %r33, [%r219+12288];
add.s32 %r34, %r5, 4096;
ld.shared.u32 %r35, [%r219+16384];
ld.shared.u32 %r36, [%r219+20480];
ld.shared.u32 %r37, [%r219+24576];
ld.shared.u32 %r38, [%r219+28672];
barrier.sync 0;

	{add.f16x2 %r220,%r30,%r35;
}

	mov.u32 %r293, 0;

	{add.f16x2 %r223,%r293,%r293;
}

	
	{sub.f16x2 %r226,%r30,%r35;
}

	
	{sub.f16x2 %r229,%r293,%r293;
}

	
	{add.f16x2 %r232,%r32,%r37;
}

	
	{add.f16x2 %r235,%r293,%r293;
}

	
	{sub.f16x2 %r238,%r32,%r37;
}

	
	{sub.f16x2 %r241,%r293,%r293;
}

	
	{xor.b32 %r244,%r238,0x80008000;
}

	
	{add.f16x2 %r246,%r220,%r232;
}

	
	{add.f16x2 %r249,%r223,%r235;
}

	
	{sub.f16x2 %r252,%r220,%r232;
}

	
	{sub.f16x2 %r255,%r223,%r235;
}

	
	{add.f16x2 %r258,%r226,%r241;
}

	
	{add.f16x2 %r261,%r229,%r244;
}

	
	{sub.f16x2 %r264,%r226,%r241;
}

	
	{sub.f16x2 %r267,%r229,%r244;
}

	
	{add.f16x2 %r270,%r31,%r36;
}

	
	{add.f16x2 %r273,%r293,%r293;
}

	
	{sub.f16x2 %r276,%r31,%r36;
}

	
	{sub.f16x2 %r279,%r293,%r293;
}

	
	{add.f16x2 %r282,%r33,%r38;
}

	
	{add.f16x2 %r285,%r293,%r293;
}

	
	{sub.f16x2 %r288,%r33,%r38;
}

	
	{sub.f16x2 %r291,%r293,%r293;
}

	
	{xor.b32 %r294,%r288,0x80008000;
}

	
	{add.f16x2 %r296,%r270,%r282;
}

	
	{add.f16x2 %r299,%r273,%r285;
}

	
	{sub.f16x2 %r302,%r270,%r282;
}

	
	{sub.f16x2 %r305,%r273,%r285;
}

	
	{add.f16x2 %r308,%r276,%r291;
}

	
	{add.f16x2 %r311,%r279,%r294;
}

	
	{sub.f16x2 %r314,%r276,%r291;
}

	
	{sub.f16x2 %r317,%r279,%r294;
}

	mov.f32 %f3, 0f3F3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r320, {low,high};}


	mov.f32 %f13, 0fBF3504F3;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r321, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r324, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r325, {low,high};}


	
	{mul.f16x2 %r334,%r308,%r320;
}

	
	{mul.f16x2 %r337,%r311,%r321;
}

	
	{sub.f16x2 %r340,%r334,%r337;
}

	
	{mul.f16x2 %r343,%r308,%r321;
}

	
	{fma.rn.f16x2 %r346,%r311,%r320,%r343;
}

	
	{xor.b32 %r350,%r302,0x80008000;
}

	
	{mul.f16x2 %r352,%r314,%r324;
}

	
	{mul.f16x2 %r355,%r317,%r325;
}

	
	{sub.f16x2 %r358,%r352,%r355;
}

	
	{mul.f16x2 %r361,%r314,%r325;
}

	
	{fma.rn.f16x2 %r364,%r317,%r324,%r361;
}

	
	{add.f16x2 %r368,%r246,%r296;
}

	
	{add.f16x2 %r371,%r249,%r299;
}

	
	{sub.f16x2 %r374,%r246,%r296;
}

	
	{sub.f16x2 %r377,%r249,%r299;
}

	
	{add.f16x2 %r380,%r258,%r340;
}

	
	{add.f16x2 %r383,%r261,%r346;
}

	
	{sub.f16x2 %r386,%r258,%r340;
}

	
	{sub.f16x2 %r389,%r261,%r346;
}

	
	{add.f16x2 %r392,%r252,%r305;
}

	
	{add.f16x2 %r395,%r255,%r350;
}

	
	{sub.f16x2 %r398,%r252,%r305;
}

	
	{sub.f16x2 %r401,%r255,%r350;
}

	
	{add.f16x2 %r404,%r264,%r358;
}

	
	{add.f16x2 %r407,%r267,%r364;
}

	
	{sub.f16x2 %r410,%r264,%r358;
}

	
	{sub.f16x2 %r413,%r267,%r364;
}

	and.b32 %r41, %r5, 1023;
cvt.rn.f32.u32	%f48, %r41;
mul.f32 %f49, %f48, 0f3A490FDB;
cos.approx.f32 %f30, %f49;
sin.approx.f32 %f50, %f49;
neg.f32 %f31, %f50;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f30;
cvt.rn.f16.f32 high, %f31;
mov.b32 %r416, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r419, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r421, {high,high};}


	
	{mul.f16x2 %r423,%r383,%r421;
}

	
	{xor.b32 %r426,%r423,0x80008000;
}

	
	{fma.rn.f16x2 %r428,%r380,%r419,%r426;
}

	
	{mul.f16x2 %r432,%r380,%r421;
}

	
	{fma.rn.f16x2 %r435,%r383,%r419,%r432;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r439, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r441, {high,high};}


	mov.f32 %f44, 0fBF800000;
mov.f32 %f45, 0f3F800000;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r443, {low,high};}


	
	{mul.f16x2 %r444,%r441,%r443;
}

	
	{mul.f16x2 %r447,%r416,%r439;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r450, {high,low};}


	
	{fma.rn.f16x2 %r452,%r444,%r450,%r447;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r456, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r458, {high,high};}


	
	{mul.f16x2 %r460,%r395,%r458;
}

	
	{xor.b32 %r463,%r460,0x80008000;
}

	
	{fma.rn.f16x2 %r465,%r392,%r456,%r463;
}

	
	{mul.f16x2 %r469,%r392,%r458;
}

	
	{fma.rn.f16x2 %r472,%r395,%r456,%r469;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r476, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r478, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r480, {low,high};}


	
	{mul.f16x2 %r481,%r478,%r480;
}

	
	{mul.f16x2 %r484,%r452,%r476;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r452;
mov.b32 %r487, {high,low};}


	
	{fma.rn.f16x2 %r489,%r481,%r487,%r484;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r489;
mov.b32 %r493, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r489;
mov.b32 %r495, {high,high};}


	
	{mul.f16x2 %r497,%r407,%r495;
}

	
	{xor.b32 %r500,%r497,0x80008000;
}

	
	{fma.rn.f16x2 %r502,%r404,%r493,%r500;
}

	
	{mul.f16x2 %r506,%r404,%r495;
}

	
	{fma.rn.f16x2 %r509,%r407,%r493,%r506;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r513, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r515, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r517, {low,high};}


	
	{mul.f16x2 %r518,%r515,%r517;
}

	
	{mul.f16x2 %r521,%r489,%r513;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r489;
mov.b32 %r524, {high,low};}


	
	{fma.rn.f16x2 %r526,%r518,%r524,%r521;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r526;
mov.b32 %r530, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r526;
mov.b32 %r532, {high,high};}


	
	{mul.f16x2 %r534,%r377,%r532;
}

	
	{xor.b32 %r537,%r534,0x80008000;
}

	
	{fma.rn.f16x2 %r539,%r374,%r530,%r537;
}

	
	{mul.f16x2 %r543,%r374,%r532;
}

	
	{fma.rn.f16x2 %r546,%r377,%r530,%r543;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r550, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r552, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r554, {low,high};}


	
	{mul.f16x2 %r555,%r552,%r554;
}

	
	{mul.f16x2 %r558,%r526,%r550;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r526;
mov.b32 %r561, {high,low};}


	
	{fma.rn.f16x2 %r563,%r555,%r561,%r558;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r563;
mov.b32 %r567, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r563;
mov.b32 %r569, {high,high};}


	
	{mul.f16x2 %r571,%r389,%r569;
}

	
	{xor.b32 %r574,%r571,0x80008000;
}

	
	{fma.rn.f16x2 %r576,%r386,%r567,%r574;
}

	
	{mul.f16x2 %r580,%r386,%r569;
}

	
	{fma.rn.f16x2 %r583,%r389,%r567,%r580;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r587, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r589, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r591, {low,high};}


	
	{mul.f16x2 %r592,%r589,%r591;
}

	
	{mul.f16x2 %r595,%r563,%r587;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r563;
mov.b32 %r598, {high,low};}


	
	{fma.rn.f16x2 %r600,%r592,%r598,%r595;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r600;
mov.b32 %r604, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r600;
mov.b32 %r606, {high,high};}


	
	{mul.f16x2 %r608,%r401,%r606;
}

	
	{xor.b32 %r611,%r608,0x80008000;
}

	
	{fma.rn.f16x2 %r613,%r398,%r604,%r611;
}

	
	{mul.f16x2 %r617,%r398,%r606;
}

	
	{fma.rn.f16x2 %r620,%r401,%r604,%r617;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r624, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r416;
mov.b32 %r626, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r628, {low,high};}


	
	{mul.f16x2 %r629,%r626,%r628;
}

	
	{mul.f16x2 %r632,%r600,%r624;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r600;
mov.b32 %r635, {high,low};}


	
	{fma.rn.f16x2 %r637,%r629,%r635,%r632;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r637;
mov.b32 %r641, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r637;
mov.b32 %r643, {high,high};}


	
	{mul.f16x2 %r645,%r413,%r643;
}

	
	{xor.b32 %r648,%r645,0x80008000;
}

	
	{fma.rn.f16x2 %r650,%r410,%r641,%r648;
}

	
	{mul.f16x2 %r654,%r410,%r643;
}

	
	{fma.rn.f16x2 %r657,%r413,%r641,%r654;
}

	and.b32 %r56, %r214, -8192;
barrier.sync 0;
shl.b32 %r679, %r41, 3;
add.s32 %r680, %r679, %r56;
shl.b32 %r681, %r680, 2;
add.s32 %r57, %r215, %r681;
st.shared.u32 [%r57], %r368;
st.shared.u32 [%r57+4], %r428;
st.shared.u32 [%r57+8], %r465;
st.shared.u32 [%r57+12], %r502;
st.shared.u32 [%r57+16], %r539;
st.shared.u32 [%r57+20], %r576;
st.shared.u32 [%r57+24], %r613;
st.shared.u32 [%r57+28], %r650;
barrier.sync 0;
add.s32 %r683, %r41, %r56;
shl.b32 %r684, %r683, 2;
add.s32 %r58, %r215, %r684;
ld.shared.u32 %r59, [%r58];
ld.shared.u32 %r60, [%r58+4096];
ld.shared.u32 %r61, [%r58+8192];
ld.shared.u32 %r62, [%r58+12288];
ld.shared.u32 %r63, [%r58+16384];
ld.shared.u32 %r64, [%r58+20480];
ld.shared.u32 %r65, [%r58+24576];
ld.shared.u32 %r66, [%r58+28672];
barrier.sync 0;
st.shared.u32 [%r57], %r371;
st.shared.u32 [%r57+4], %r435;
st.shared.u32 [%r57+8], %r472;
st.shared.u32 [%r57+12], %r509;
st.shared.u32 [%r57+16], %r546;
st.shared.u32 [%r57+20], %r583;
st.shared.u32 [%r57+24], %r620;
st.shared.u32 [%r57+28], %r657;
barrier.sync 0;
ld.shared.u32 %r690, [%r58];
ld.shared.u32 %r740, [%r58+4096];
ld.shared.u32 %r702, [%r58+8192];
ld.shared.u32 %r752, [%r58+12288];
ld.shared.u32 %r691, [%r58+16384];
ld.shared.u32 %r741, [%r58+20480];
ld.shared.u32 %r703, [%r58+24576];
ld.shared.u32 %r753, [%r58+28672];

	{add.f16x2 %r686,%r59,%r63;
}

	
	{add.f16x2 %r689,%r690,%r691;
}

	
	{sub.f16x2 %r692,%r59,%r63;
}

	
	{sub.f16x2 %r695,%r690,%r691;
}

	
	{add.f16x2 %r698,%r61,%r65;
}

	
	{add.f16x2 %r701,%r702,%r703;
}

	
	{sub.f16x2 %r704,%r61,%r65;
}

	
	{sub.f16x2 %r707,%r702,%r703;
}

	
	{xor.b32 %r710,%r704,0x80008000;
}

	
	{add.f16x2 %r712,%r686,%r698;
}

	
	{add.f16x2 %r715,%r689,%r701;
}

	
	{sub.f16x2 %r718,%r686,%r698;
}

	
	{sub.f16x2 %r721,%r689,%r701;
}

	
	{add.f16x2 %r724,%r692,%r707;
}

	
	{add.f16x2 %r727,%r695,%r710;
}

	
	{sub.f16x2 %r730,%r692,%r707;
}

	
	{sub.f16x2 %r733,%r695,%r710;
}

	
	{add.f16x2 %r736,%r60,%r64;
}

	
	{add.f16x2 %r739,%r740,%r741;
}

	
	{sub.f16x2 %r742,%r60,%r64;
}

	
	{sub.f16x2 %r745,%r740,%r741;
}

	
	{add.f16x2 %r748,%r62,%r66;
}

	
	{add.f16x2 %r751,%r752,%r753;
}

	
	{sub.f16x2 %r754,%r62,%r66;
}

	
	{sub.f16x2 %r757,%r752,%r753;
}

	
	{xor.b32 %r760,%r754,0x80008000;
}

	
	{add.f16x2 %r762,%r736,%r748;
}

	
	{add.f16x2 %r765,%r739,%r751;
}

	
	{sub.f16x2 %r768,%r736,%r748;
}

	
	{sub.f16x2 %r771,%r739,%r751;
}

	
	{add.f16x2 %r774,%r742,%r757;
}

	
	{add.f16x2 %r777,%r745,%r760;
}

	
	{sub.f16x2 %r780,%r742,%r757;
}

	
	{sub.f16x2 %r783,%r745,%r760;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r786, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r787, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r790, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r791, {low,high};}


	
	{mul.f16x2 %r800,%r774,%r786;
}

	
	{mul.f16x2 %r803,%r777,%r787;
}

	
	{sub.f16x2 %r806,%r800,%r803;
}

	
	{mul.f16x2 %r809,%r774,%r787;
}

	
	{fma.rn.f16x2 %r812,%r777,%r786,%r809;
}

	
	{xor.b32 %r816,%r768,0x80008000;
}

	
	{mul.f16x2 %r818,%r780,%r790;
}

	
	{mul.f16x2 %r821,%r783,%r791;
}

	
	{sub.f16x2 %r824,%r818,%r821;
}

	
	{mul.f16x2 %r827,%r780,%r791;
}

	
	{fma.rn.f16x2 %r830,%r783,%r790,%r827;
}

	
	{add.f16x2 %r834,%r712,%r762;
}

	
	{add.f16x2 %r837,%r715,%r765;
}

	
	{sub.f16x2 %r840,%r712,%r762;
}

	
	{sub.f16x2 %r843,%r715,%r765;
}

	
	{add.f16x2 %r846,%r724,%r806;
}

	
	{add.f16x2 %r849,%r727,%r812;
}

	
	{sub.f16x2 %r852,%r724,%r806;
}

	
	{sub.f16x2 %r855,%r727,%r812;
}

	
	{add.f16x2 %r858,%r718,%r771;
}

	
	{add.f16x2 %r861,%r721,%r816;
}

	
	{sub.f16x2 %r864,%r718,%r771;
}

	
	{sub.f16x2 %r867,%r721,%r816;
}

	
	{add.f16x2 %r870,%r730,%r824;
}

	
	{add.f16x2 %r873,%r733,%r830;
}

	
	{sub.f16x2 %r876,%r730,%r824;
}

	
	{sub.f16x2 %r879,%r733,%r830;
}

	and.b32 %r69, %r5, 1016;
bfe.u32 %r1144, %r5, 3, 7;
cvt.rn.f32.u32	%f97, %r1144;
mul.f32 %f98, %f97, 0f3BC90FDB;
cos.approx.f32 %f79, %f98;
sin.approx.f32 %f99, %f98;
neg.f32 %f80, %f99;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f79;
cvt.rn.f16.f32 high, %f80;
mov.b32 %r882, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r885, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r887, {high,high};}


	
	{mul.f16x2 %r889,%r849,%r887;
}

	
	{xor.b32 %r892,%r889,0x80008000;
}

	
	{fma.rn.f16x2 %r894,%r846,%r885,%r892;
}

	
	{mul.f16x2 %r898,%r846,%r887;
}

	
	{fma.rn.f16x2 %r901,%r849,%r885,%r898;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r905, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r907, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r909, {low,high};}


	
	{mul.f16x2 %r910,%r907,%r909;
}

	
	{mul.f16x2 %r913,%r882,%r905;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r916, {high,low};}


	
	{fma.rn.f16x2 %r918,%r910,%r916,%r913;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r922, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r924, {high,high};}


	
	{mul.f16x2 %r926,%r861,%r924;
}

	
	{xor.b32 %r929,%r926,0x80008000;
}

	
	{fma.rn.f16x2 %r931,%r858,%r922,%r929;
}

	
	{mul.f16x2 %r935,%r858,%r924;
}

	
	{fma.rn.f16x2 %r938,%r861,%r922,%r935;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r942, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r944, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r946, {low,high};}


	
	{mul.f16x2 %r947,%r944,%r946;
}

	
	{mul.f16x2 %r950,%r918,%r942;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r918;
mov.b32 %r953, {high,low};}


	
	{fma.rn.f16x2 %r955,%r947,%r953,%r950;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r955;
mov.b32 %r959, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r955;
mov.b32 %r961, {high,high};}


	
	{mul.f16x2 %r963,%r873,%r961;
}

	
	{xor.b32 %r966,%r963,0x80008000;
}

	
	{fma.rn.f16x2 %r968,%r870,%r959,%r966;
}

	
	{mul.f16x2 %r972,%r870,%r961;
}

	
	{fma.rn.f16x2 %r975,%r873,%r959,%r972;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r979, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r981, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r983, {low,high};}


	
	{mul.f16x2 %r984,%r981,%r983;
}

	
	{mul.f16x2 %r987,%r955,%r979;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r955;
mov.b32 %r990, {high,low};}


	
	{fma.rn.f16x2 %r992,%r984,%r990,%r987;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r992;
mov.b32 %r996, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r992;
mov.b32 %r998, {high,high};}


	
	{mul.f16x2 %r1000,%r843,%r998;
}

	
	{xor.b32 %r1003,%r1000,0x80008000;
}

	
	{fma.rn.f16x2 %r1005,%r840,%r996,%r1003;
}

	
	{mul.f16x2 %r1009,%r840,%r998;
}

	
	{fma.rn.f16x2 %r1012,%r843,%r996,%r1009;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r1016, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r1018, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1020, {low,high};}


	
	{mul.f16x2 %r1021,%r1018,%r1020;
}

	
	{mul.f16x2 %r1024,%r992,%r1016;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r992;
mov.b32 %r1027, {high,low};}


	
	{fma.rn.f16x2 %r1029,%r1021,%r1027,%r1024;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1029;
mov.b32 %r1033, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1029;
mov.b32 %r1035, {high,high};}


	
	{mul.f16x2 %r1037,%r855,%r1035;
}

	
	{xor.b32 %r1040,%r1037,0x80008000;
}

	
	{fma.rn.f16x2 %r1042,%r852,%r1033,%r1040;
}

	
	{mul.f16x2 %r1046,%r852,%r1035;
}

	
	{fma.rn.f16x2 %r1049,%r855,%r1033,%r1046;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r1053, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r1055, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1057, {low,high};}


	
	{mul.f16x2 %r1058,%r1055,%r1057;
}

	
	{mul.f16x2 %r1061,%r1029,%r1053;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1029;
mov.b32 %r1064, {high,low};}


	
	{fma.rn.f16x2 %r1066,%r1058,%r1064,%r1061;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1066;
mov.b32 %r1070, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1066;
mov.b32 %r1072, {high,high};}


	
	{mul.f16x2 %r1074,%r867,%r1072;
}

	
	{xor.b32 %r1077,%r1074,0x80008000;
}

	
	{fma.rn.f16x2 %r1079,%r864,%r1070,%r1077;
}

	
	{mul.f16x2 %r1083,%r864,%r1072;
}

	
	{fma.rn.f16x2 %r1086,%r867,%r1070,%r1083;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r1090, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r882;
mov.b32 %r1092, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1094, {low,high};}


	
	{mul.f16x2 %r1095,%r1092,%r1094;
}

	
	{mul.f16x2 %r1098,%r1066,%r1090;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1066;
mov.b32 %r1101, {high,low};}


	
	{fma.rn.f16x2 %r1103,%r1095,%r1101,%r1098;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1103;
mov.b32 %r1107, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1103;
mov.b32 %r1109, {high,high};}


	
	{mul.f16x2 %r1111,%r879,%r1109;
}

	
	{xor.b32 %r1114,%r1111,0x80008000;
}

	
	{fma.rn.f16x2 %r1116,%r876,%r1107,%r1114;
}

	
	{mul.f16x2 %r1120,%r876,%r1109;
}

	
	{fma.rn.f16x2 %r1123,%r879,%r1107,%r1120;
}

	and.b32 %r1145, %r5, 7;
add.s32 %r84, %r56, %r1145;
barrier.sync 0;
shl.b32 %r1146, %r69, 3;
add.s32 %r1147, %r1146, %r84;
shl.b32 %r1148, %r1147, 2;
add.s32 %r85, %r215, %r1148;
st.shared.u32 [%r85], %r834;
st.shared.u32 [%r85+32], %r894;
st.shared.u32 [%r85+64], %r931;
st.shared.u32 [%r85+96], %r968;
st.shared.u32 [%r85+128], %r1005;
st.shared.u32 [%r85+160], %r1042;
st.shared.u32 [%r85+192], %r1079;
st.shared.u32 [%r85+224], %r1116;
barrier.sync 0;
add.s32 %r1150, %r69, %r84;
shl.b32 %r1151, %r1150, 2;
add.s32 %r86, %r215, %r1151;
ld.shared.u32 %r87, [%r86];
ld.shared.u32 %r88, [%r86+4096];
ld.shared.u32 %r89, [%r86+8192];
ld.shared.u32 %r90, [%r86+12288];
ld.shared.u32 %r91, [%r86+16384];
ld.shared.u32 %r92, [%r86+20480];
ld.shared.u32 %r93, [%r86+24576];
ld.shared.u32 %r94, [%r86+28672];
barrier.sync 0;
st.shared.u32 [%r85], %r837;
st.shared.u32 [%r85+32], %r901;
st.shared.u32 [%r85+64], %r938;
st.shared.u32 [%r85+96], %r975;
st.shared.u32 [%r85+128], %r1012;
st.shared.u32 [%r85+160], %r1049;
st.shared.u32 [%r85+192], %r1086;
st.shared.u32 [%r85+224], %r1123;
barrier.sync 0;
ld.shared.u32 %r1157, [%r86];
ld.shared.u32 %r1207, [%r86+4096];
ld.shared.u32 %r1169, [%r86+8192];
ld.shared.u32 %r1219, [%r86+12288];
ld.shared.u32 %r1158, [%r86+16384];
ld.shared.u32 %r1208, [%r86+20480];
ld.shared.u32 %r1170, [%r86+24576];
ld.shared.u32 %r1220, [%r86+28672];

	{add.f16x2 %r1153,%r87,%r91;
}

	
	{add.f16x2 %r1156,%r1157,%r1158;
}

	
	{sub.f16x2 %r1159,%r87,%r91;
}

	
	{sub.f16x2 %r1162,%r1157,%r1158;
}

	
	{add.f16x2 %r1165,%r89,%r93;
}

	
	{add.f16x2 %r1168,%r1169,%r1170;
}

	
	{sub.f16x2 %r1171,%r89,%r93;
}

	
	{sub.f16x2 %r1174,%r1169,%r1170;
}

	
	{xor.b32 %r1177,%r1171,0x80008000;
}

	
	{add.f16x2 %r1179,%r1153,%r1165;
}

	
	{add.f16x2 %r1182,%r1156,%r1168;
}

	
	{sub.f16x2 %r1185,%r1153,%r1165;
}

	
	{sub.f16x2 %r1188,%r1156,%r1168;
}

	
	{add.f16x2 %r1191,%r1159,%r1174;
}

	
	{add.f16x2 %r1194,%r1162,%r1177;
}

	
	{sub.f16x2 %r1197,%r1159,%r1174;
}

	
	{sub.f16x2 %r1200,%r1162,%r1177;
}

	
	{add.f16x2 %r1203,%r88,%r92;
}

	
	{add.f16x2 %r1206,%r1207,%r1208;
}

	
	{sub.f16x2 %r1209,%r88,%r92;
}

	
	{sub.f16x2 %r1212,%r1207,%r1208;
}

	
	{add.f16x2 %r1215,%r90,%r94;
}

	
	{add.f16x2 %r1218,%r1219,%r1220;
}

	
	{sub.f16x2 %r1221,%r90,%r94;
}

	
	{sub.f16x2 %r1224,%r1219,%r1220;
}

	
	{xor.b32 %r1227,%r1221,0x80008000;
}

	
	{add.f16x2 %r1229,%r1203,%r1215;
}

	
	{add.f16x2 %r1232,%r1206,%r1218;
}

	
	{sub.f16x2 %r1235,%r1203,%r1215;
}

	
	{sub.f16x2 %r1238,%r1206,%r1218;
}

	
	{add.f16x2 %r1241,%r1209,%r1224;
}

	
	{add.f16x2 %r1244,%r1212,%r1227;
}

	
	{sub.f16x2 %r1247,%r1209,%r1224;
}

	
	{sub.f16x2 %r1250,%r1212,%r1227;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r1253, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1254, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1257, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1258, {low,high};}


	
	{mul.f16x2 %r1267,%r1241,%r1253;
}

	
	{mul.f16x2 %r1270,%r1244,%r1254;
}

	
	{sub.f16x2 %r1273,%r1267,%r1270;
}

	
	{mul.f16x2 %r1276,%r1241,%r1254;
}

	
	{fma.rn.f16x2 %r1279,%r1244,%r1253,%r1276;
}

	
	{xor.b32 %r1283,%r1235,0x80008000;
}

	
	{mul.f16x2 %r1285,%r1247,%r1257;
}

	
	{mul.f16x2 %r1288,%r1250,%r1258;
}

	
	{sub.f16x2 %r1291,%r1285,%r1288;
}

	
	{mul.f16x2 %r1294,%r1247,%r1258;
}

	
	{fma.rn.f16x2 %r1297,%r1250,%r1257,%r1294;
}

	
	{add.f16x2 %r1301,%r1179,%r1229;
}

	
	{add.f16x2 %r1304,%r1182,%r1232;
}

	
	{sub.f16x2 %r1307,%r1179,%r1229;
}

	
	{sub.f16x2 %r1310,%r1182,%r1232;
}

	
	{add.f16x2 %r1313,%r1191,%r1273;
}

	
	{add.f16x2 %r1316,%r1194,%r1279;
}

	
	{sub.f16x2 %r1319,%r1191,%r1273;
}

	
	{sub.f16x2 %r1322,%r1194,%r1279;
}

	
	{add.f16x2 %r1325,%r1185,%r1238;
}

	
	{add.f16x2 %r1328,%r1188,%r1283;
}

	
	{sub.f16x2 %r1331,%r1185,%r1238;
}

	
	{sub.f16x2 %r1334,%r1188,%r1283;
}

	
	{add.f16x2 %r1337,%r1197,%r1291;
}

	
	{add.f16x2 %r1340,%r1200,%r1297;
}

	
	{sub.f16x2 %r1343,%r1197,%r1291;
}

	
	{sub.f16x2 %r1346,%r1200,%r1297;
}

	and.b32 %r97, %r5, 960;
bfe.u32 %r1611, %r5, 6, 4;
cvt.rn.f32.u32	%f146, %r1611;
mul.f32 %f147, %f146, 0f3D490FDB;
cos.approx.f32 %f128, %f147;
sin.approx.f32 %f148, %f147;
neg.f32 %f129, %f148;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f128;
cvt.rn.f16.f32 high, %f129;
mov.b32 %r1349, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1352, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1354, {high,high};}


	
	{mul.f16x2 %r1356,%r1316,%r1354;
}

	
	{xor.b32 %r1359,%r1356,0x80008000;
}

	
	{fma.rn.f16x2 %r1361,%r1313,%r1352,%r1359;
}

	
	{mul.f16x2 %r1365,%r1313,%r1354;
}

	
	{fma.rn.f16x2 %r1368,%r1316,%r1352,%r1365;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1372, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1374, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1376, {low,high};}


	
	{mul.f16x2 %r1377,%r1374,%r1376;
}

	
	{mul.f16x2 %r1380,%r1349,%r1372;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1383, {high,low};}


	
	{fma.rn.f16x2 %r1385,%r1377,%r1383,%r1380;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1389, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1391, {high,high};}


	
	{mul.f16x2 %r1393,%r1328,%r1391;
}

	
	{xor.b32 %r1396,%r1393,0x80008000;
}

	
	{fma.rn.f16x2 %r1398,%r1325,%r1389,%r1396;
}

	
	{mul.f16x2 %r1402,%r1325,%r1391;
}

	
	{fma.rn.f16x2 %r1405,%r1328,%r1389,%r1402;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1409, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1411, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1413, {low,high};}


	
	{mul.f16x2 %r1414,%r1411,%r1413;
}

	
	{mul.f16x2 %r1417,%r1385,%r1409;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1385;
mov.b32 %r1420, {high,low};}


	
	{fma.rn.f16x2 %r1422,%r1414,%r1420,%r1417;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1422;
mov.b32 %r1426, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1422;
mov.b32 %r1428, {high,high};}


	
	{mul.f16x2 %r1430,%r1340,%r1428;
}

	
	{xor.b32 %r1433,%r1430,0x80008000;
}

	
	{fma.rn.f16x2 %r1435,%r1337,%r1426,%r1433;
}

	
	{mul.f16x2 %r1439,%r1337,%r1428;
}

	
	{fma.rn.f16x2 %r1442,%r1340,%r1426,%r1439;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1446, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1448, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1450, {low,high};}


	
	{mul.f16x2 %r1451,%r1448,%r1450;
}

	
	{mul.f16x2 %r1454,%r1422,%r1446;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1422;
mov.b32 %r1457, {high,low};}


	
	{fma.rn.f16x2 %r1459,%r1451,%r1457,%r1454;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1459;
mov.b32 %r1463, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1459;
mov.b32 %r1465, {high,high};}


	
	{mul.f16x2 %r1467,%r1310,%r1465;
}

	
	{xor.b32 %r1470,%r1467,0x80008000;
}

	
	{fma.rn.f16x2 %r1472,%r1307,%r1463,%r1470;
}

	
	{mul.f16x2 %r1476,%r1307,%r1465;
}

	
	{fma.rn.f16x2 %r1479,%r1310,%r1463,%r1476;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1483, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1485, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1487, {low,high};}


	
	{mul.f16x2 %r1488,%r1485,%r1487;
}

	
	{mul.f16x2 %r1491,%r1459,%r1483;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1459;
mov.b32 %r1494, {high,low};}


	
	{fma.rn.f16x2 %r1496,%r1488,%r1494,%r1491;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1496;
mov.b32 %r1500, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1496;
mov.b32 %r1502, {high,high};}


	
	{mul.f16x2 %r1504,%r1322,%r1502;
}

	
	{xor.b32 %r1507,%r1504,0x80008000;
}

	
	{fma.rn.f16x2 %r1509,%r1319,%r1500,%r1507;
}

	
	{mul.f16x2 %r1513,%r1319,%r1502;
}

	
	{fma.rn.f16x2 %r1516,%r1322,%r1500,%r1513;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1520, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1522, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1524, {low,high};}


	
	{mul.f16x2 %r1525,%r1522,%r1524;
}

	
	{mul.f16x2 %r1528,%r1496,%r1520;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1496;
mov.b32 %r1531, {high,low};}


	
	{fma.rn.f16x2 %r1533,%r1525,%r1531,%r1528;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1533;
mov.b32 %r1537, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1533;
mov.b32 %r1539, {high,high};}


	
	{mul.f16x2 %r1541,%r1334,%r1539;
}

	
	{xor.b32 %r1544,%r1541,0x80008000;
}

	
	{fma.rn.f16x2 %r1546,%r1331,%r1537,%r1544;
}

	
	{mul.f16x2 %r1550,%r1331,%r1539;
}

	
	{fma.rn.f16x2 %r1553,%r1334,%r1537,%r1550;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1557, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1349;
mov.b32 %r1559, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1561, {low,high};}


	
	{mul.f16x2 %r1562,%r1559,%r1561;
}

	
	{mul.f16x2 %r1565,%r1533,%r1557;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1533;
mov.b32 %r1568, {high,low};}


	
	{fma.rn.f16x2 %r1570,%r1562,%r1568,%r1565;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1570;
mov.b32 %r1574, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1570;
mov.b32 %r1576, {high,high};}


	
	{mul.f16x2 %r1578,%r1346,%r1576;
}

	
	{xor.b32 %r1581,%r1578,0x80008000;
}

	
	{fma.rn.f16x2 %r1583,%r1343,%r1574,%r1581;
}

	
	{mul.f16x2 %r1587,%r1343,%r1576;
}

	
	{fma.rn.f16x2 %r1590,%r1346,%r1574,%r1587;
}

	and.b32 %r1612, %r5, 63;
add.s32 %r112, %r56, %r1612;
barrier.sync 0;
shl.b32 %r1613, %r97, 3;
add.s32 %r1614, %r1613, %r112;
shl.b32 %r1615, %r1614, 2;
add.s32 %r113, %r215, %r1615;
st.shared.u32 [%r113], %r1301;
st.shared.u32 [%r113+256], %r1361;
st.shared.u32 [%r113+512], %r1398;
st.shared.u32 [%r113+768], %r1435;
st.shared.u32 [%r113+1024], %r1472;
st.shared.u32 [%r113+1280], %r1509;
st.shared.u32 [%r113+1536], %r1546;
st.shared.u32 [%r113+1792], %r1583;
barrier.sync 0;
add.s32 %r1617, %r97, %r112;
shl.b32 %r1618, %r1617, 2;
add.s32 %r114, %r215, %r1618;
ld.shared.u32 %r115, [%r114];
ld.shared.u32 %r116, [%r114+4096];
ld.shared.u32 %r117, [%r114+8192];
ld.shared.u32 %r118, [%r114+12288];
ld.shared.u32 %r119, [%r114+16384];
ld.shared.u32 %r120, [%r114+20480];
ld.shared.u32 %r121, [%r114+24576];
ld.shared.u32 %r122, [%r114+28672];
barrier.sync 0;
st.shared.u32 [%r113], %r1304;
st.shared.u32 [%r113+256], %r1368;
st.shared.u32 [%r113+512], %r1405;
st.shared.u32 [%r113+768], %r1442;
st.shared.u32 [%r113+1024], %r1479;
st.shared.u32 [%r113+1280], %r1516;
st.shared.u32 [%r113+1536], %r1553;
st.shared.u32 [%r113+1792], %r1590;
barrier.sync 0;
ld.shared.u32 %r1624, [%r114];
ld.shared.u32 %r1674, [%r114+4096];
ld.shared.u32 %r1636, [%r114+8192];
ld.shared.u32 %r1686, [%r114+12288];
ld.shared.u32 %r1625, [%r114+16384];
ld.shared.u32 %r1675, [%r114+20480];
ld.shared.u32 %r1637, [%r114+24576];
ld.shared.u32 %r1687, [%r114+28672];

	{add.f16x2 %r1620,%r115,%r119;
}

	
	{add.f16x2 %r1623,%r1624,%r1625;
}

	
	{sub.f16x2 %r1626,%r115,%r119;
}

	
	{sub.f16x2 %r1629,%r1624,%r1625;
}

	
	{add.f16x2 %r1632,%r117,%r121;
}

	
	{add.f16x2 %r1635,%r1636,%r1637;
}

	
	{sub.f16x2 %r1638,%r117,%r121;
}

	
	{sub.f16x2 %r1641,%r1636,%r1637;
}

	
	{xor.b32 %r1644,%r1638,0x80008000;
}

	
	{add.f16x2 %r1646,%r1620,%r1632;
}

	
	{add.f16x2 %r1649,%r1623,%r1635;
}

	
	{sub.f16x2 %r1652,%r1620,%r1632;
}

	
	{sub.f16x2 %r1655,%r1623,%r1635;
}

	
	{add.f16x2 %r1658,%r1626,%r1641;
}

	
	{add.f16x2 %r1661,%r1629,%r1644;
}

	
	{sub.f16x2 %r1664,%r1626,%r1641;
}

	
	{sub.f16x2 %r1667,%r1629,%r1644;
}

	
	{add.f16x2 %r1670,%r116,%r120;
}

	
	{add.f16x2 %r1673,%r1674,%r1675;
}

	
	{sub.f16x2 %r1676,%r116,%r120;
}

	
	{sub.f16x2 %r1679,%r1674,%r1675;
}

	
	{add.f16x2 %r1682,%r118,%r122;
}

	
	{add.f16x2 %r1685,%r1686,%r1687;
}

	
	{sub.f16x2 %r1688,%r118,%r122;
}

	
	{sub.f16x2 %r1691,%r1686,%r1687;
}

	
	{xor.b32 %r1694,%r1688,0x80008000;
}

	
	{add.f16x2 %r1696,%r1670,%r1682;
}

	
	{add.f16x2 %r1699,%r1673,%r1685;
}

	
	{sub.f16x2 %r1702,%r1670,%r1682;
}

	
	{sub.f16x2 %r1705,%r1673,%r1685;
}

	
	{add.f16x2 %r1708,%r1676,%r1691;
}

	
	{add.f16x2 %r1711,%r1679,%r1694;
}

	
	{sub.f16x2 %r1714,%r1676,%r1691;
}

	
	{sub.f16x2 %r1717,%r1679,%r1694;
}

	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f3;
cvt.rn.f16.f32 high, %f3;
mov.b32 %r1720, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1721, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1724, {low,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f13;
cvt.rn.f16.f32 high, %f13;
mov.b32 %r1725, {low,high};}


	
	{mul.f16x2 %r1734,%r1708,%r1720;
}

	
	{mul.f16x2 %r1737,%r1711,%r1721;
}

	
	{sub.f16x2 %r1740,%r1734,%r1737;
}

	
	{mul.f16x2 %r1743,%r1708,%r1721;
}

	
	{fma.rn.f16x2 %r1746,%r1711,%r1720,%r1743;
}

	
	{xor.b32 %r1750,%r1702,0x80008000;
}

	
	{mul.f16x2 %r1752,%r1714,%r1724;
}

	
	{mul.f16x2 %r1755,%r1717,%r1725;
}

	
	{sub.f16x2 %r1758,%r1752,%r1755;
}

	
	{mul.f16x2 %r1761,%r1714,%r1725;
}

	
	{fma.rn.f16x2 %r1764,%r1717,%r1724,%r1761;
}

	
	{add.f16x2 %r1768,%r1646,%r1696;
}

	
	{add.f16x2 %r1771,%r1649,%r1699;
}

	
	{sub.f16x2 %r1774,%r1646,%r1696;
}

	
	{sub.f16x2 %r1777,%r1649,%r1699;
}

	
	{add.f16x2 %r1780,%r1658,%r1740;
}

	
	{add.f16x2 %r1783,%r1661,%r1746;
}

	
	{sub.f16x2 %r1786,%r1658,%r1740;
}

	
	{sub.f16x2 %r1789,%r1661,%r1746;
}

	
	{add.f16x2 %r1792,%r1652,%r1705;
}

	
	{add.f16x2 %r1795,%r1655,%r1750;
}

	
	{sub.f16x2 %r1798,%r1652,%r1705;
}

	
	{sub.f16x2 %r1801,%r1655,%r1750;
}

	
	{add.f16x2 %r1804,%r1664,%r1758;
}

	
	{add.f16x2 %r1807,%r1667,%r1764;
}

	
	{sub.f16x2 %r1810,%r1664,%r1758;
}

	
	{sub.f16x2 %r1813,%r1667,%r1764;
}

	and.b32 %r125, %r5, 512;
bfe.u32 %r2078, %r5, 9, 1;
cvt.rn.f32.u32	%f195, %r2078;
mul.f32 %f196, %f195, 0f3EC90FDB;
cos.approx.f32 %f177, %f196;
sin.approx.f32 %f197, %f196;
neg.f32 %f178, %f197;

	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f177;
cvt.rn.f16.f32 high, %f178;
mov.b32 %r1816, {low,high};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1819, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1821, {high,high};}


	
	{mul.f16x2 %r1823,%r1783,%r1821;
}

	
	{xor.b32 %r1826,%r1823,0x80008000;
}

	
	{fma.rn.f16x2 %r1828,%r1780,%r1819,%r1826;
}

	
	{mul.f16x2 %r1832,%r1780,%r1821;
}

	
	{fma.rn.f16x2 %r1835,%r1783,%r1819,%r1832;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1839, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1841, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1843, {low,high};}


	
	{mul.f16x2 %r1844,%r1841,%r1843;
}

	
	{mul.f16x2 %r1847,%r1816,%r1839;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1850, {high,low};}


	
	{fma.rn.f16x2 %r1852,%r1844,%r1850,%r1847;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1856, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1858, {high,high};}


	
	{mul.f16x2 %r1860,%r1795,%r1858;
}

	
	{xor.b32 %r1863,%r1860,0x80008000;
}

	
	{fma.rn.f16x2 %r1865,%r1792,%r1856,%r1863;
}

	
	{mul.f16x2 %r1869,%r1792,%r1858;
}

	
	{fma.rn.f16x2 %r1872,%r1795,%r1856,%r1869;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1876, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1878, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1880, {low,high};}


	
	{mul.f16x2 %r1881,%r1878,%r1880;
}

	
	{mul.f16x2 %r1884,%r1852,%r1876;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1852;
mov.b32 %r1887, {high,low};}


	
	{fma.rn.f16x2 %r1889,%r1881,%r1887,%r1884;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1889;
mov.b32 %r1893, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1889;
mov.b32 %r1895, {high,high};}


	
	{mul.f16x2 %r1897,%r1807,%r1895;
}

	
	{xor.b32 %r1900,%r1897,0x80008000;
}

	
	{fma.rn.f16x2 %r1902,%r1804,%r1893,%r1900;
}

	
	{mul.f16x2 %r1906,%r1804,%r1895;
}

	
	{fma.rn.f16x2 %r1909,%r1807,%r1893,%r1906;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1913, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1915, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1917, {low,high};}


	
	{mul.f16x2 %r1918,%r1915,%r1917;
}

	
	{mul.f16x2 %r1921,%r1889,%r1913;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1889;
mov.b32 %r1924, {high,low};}


	
	{fma.rn.f16x2 %r1926,%r1918,%r1924,%r1921;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1926;
mov.b32 %r1930, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1926;
mov.b32 %r1932, {high,high};}


	
	{mul.f16x2 %r1934,%r1777,%r1932;
}

	
	{xor.b32 %r1937,%r1934,0x80008000;
}

	
	{fma.rn.f16x2 %r1939,%r1774,%r1930,%r1937;
}

	
	{mul.f16x2 %r1943,%r1774,%r1932;
}

	
	{fma.rn.f16x2 %r1946,%r1777,%r1930,%r1943;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1950, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1952, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1954, {low,high};}


	
	{mul.f16x2 %r1955,%r1952,%r1954;
}

	
	{mul.f16x2 %r1958,%r1926,%r1950;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1926;
mov.b32 %r1961, {high,low};}


	
	{fma.rn.f16x2 %r1963,%r1955,%r1961,%r1958;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1963;
mov.b32 %r1967, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1963;
mov.b32 %r1969, {high,high};}


	
	{mul.f16x2 %r1971,%r1789,%r1969;
}

	
	{xor.b32 %r1974,%r1971,0x80008000;
}

	
	{fma.rn.f16x2 %r1976,%r1786,%r1967,%r1974;
}

	
	{mul.f16x2 %r1980,%r1786,%r1969;
}

	
	{fma.rn.f16x2 %r1983,%r1789,%r1967,%r1980;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1987, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r1989, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r1991, {low,high};}


	
	{mul.f16x2 %r1992,%r1989,%r1991;
}

	
	{mul.f16x2 %r1995,%r1963,%r1987;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1963;
mov.b32 %r1998, {high,low};}


	
	{fma.rn.f16x2 %r2000,%r1992,%r1998,%r1995;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2000;
mov.b32 %r2004, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2000;
mov.b32 %r2006, {high,high};}


	
	{mul.f16x2 %r2008,%r1801,%r2006;
}

	
	{xor.b32 %r2011,%r2008,0x80008000;
}

	
	{fma.rn.f16x2 %r2013,%r1798,%r2004,%r2011;
}

	
	{mul.f16x2 %r2017,%r1798,%r2006;
}

	
	{fma.rn.f16x2 %r2020,%r1801,%r2004,%r2017;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r2024, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r1816;
mov.b32 %r2026, {high,high};}


	
	{.reg .f16 low,high;
cvt.rn.f16.f32 low, %f44;
cvt.rn.f16.f32 high, %f45;
mov.b32 %r2028, {low,high};}


	
	{mul.f16x2 %r2029,%r2026,%r2028;
}

	
	{mul.f16x2 %r2032,%r2000,%r2024;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2000;
mov.b32 %r2035, {high,low};}


	
	{fma.rn.f16x2 %r2037,%r2029,%r2035,%r2032;
}

	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2037;
mov.b32 %r2041, {low,low};}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r2037;
mov.b32 %r2043, {high,high};}


	
	{mul.f16x2 %r2045,%r1813,%r2043;
}

	
	{xor.b32 %r2048,%r2045,0x80008000;
}

	
	{fma.rn.f16x2 %r2050,%r1810,%r2041,%r2048;
}

	
	{mul.f16x2 %r2054,%r1810,%r2043;
}

	
	{fma.rn.f16x2 %r2057,%r1813,%r2041,%r2054;
}

	and.b32 %r2079, %r5, 511;
add.s32 %r140, %r56, %r2079;
barrier.sync 0;
shl.b32 %r2080, %r125, 3;
add.s32 %r2081, %r2080, %r140;
shl.b32 %r2082, %r2081, 2;
add.s32 %r141, %r215, %r2082;
st.shared.u32 [%r141], %r1768;
st.shared.u32 [%r141+2048], %r1828;
st.shared.u32 [%r141+4096], %r1865;
st.shared.u32 [%r141+6144], %r1902;
st.shared.u32 [%r141+8192], %r1939;
st.shared.u32 [%r141+10240], %r1976;
st.shared.u32 [%r141+12288], %r2013;
st.shared.u32 [%r141+14336], %r2050;
barrier.sync 0;
add.s32 %r2084, %r125, %r140;
shl.b32 %r2085, %r2084, 2;
add.s32 %r142, %r215, %r2085;
ld.shared.u32 %r143, [%r142];
ld.shared.u32 %r144, [%r142+4096];
ld.shared.u32 %r145, [%r142+8192];
ld.shared.u32 %r146, [%r142+12288];
ld.shared.u32 %r147, [%r142+16384];
ld.shared.u32 %r148, [%r142+20480];
ld.shared.u32 %r149, [%r142+24576];
ld.shared.u32 %r150, [%r142+28672];
barrier.sync 0;
st.shared.u32 [%r141], %r1771;
st.shared.u32 [%r141+2048], %r1835;
st.shared.u32 [%r141+4096], %r1872;
st.shared.u32 [%r141+6144], %r1909;
st.shared.u32 [%r141+8192], %r1946;
st.shared.u32 [%r141+10240], %r1983;
st.shared.u32 [%r141+12288], %r2020;
st.shared.u32 [%r141+14336], %r2057;
barrier.sync 0;
ld.shared.u32 %r2091, [%r142];
ld.shared.u32 %r2103, [%r142+4096];
ld.shared.u32 %r2115, [%r142+8192];
ld.shared.u32 %r2127, [%r142+12288];
ld.shared.u32 %r2092, [%r142+16384];
ld.shared.u32 %r2104, [%r142+20480];
ld.shared.u32 %r2116, [%r142+24576];
ld.shared.u32 %r2128, [%r142+28672];

	{add.f16x2 %r2087,%r143,%r147;
}

	
	{add.f16x2 %r2090,%r2091,%r2092;
}

	
	{sub.f16x2 %r2093,%r143,%r147;
}

	
	{sub.f16x2 %r2096,%r2091,%r2092;
}

	
	{add.f16x2 %r2099,%r144,%r148;
}

	
	{add.f16x2 %r2102,%r2103,%r2104;
}

	
	{add.f16x2 %r2111,%r145,%r149;
}

	
	{add.f16x2 %r2114,%r2115,%r2116;
}

	
	{add.f16x2 %r2123,%r146,%r150;
}

	
	{add.f16x2 %r2126,%r2127,%r2128;
}

	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2087;
mov.b32 {blow,bhigh}, %r2090;
mov.b32 %r2135, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2087;
mov.b32 {blow,bhigh}, %r2090;
mov.b32 %r2138, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2099;
mov.b32 {blow,bhigh}, %r2102;
mov.b32 %r2141, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2099;
mov.b32 {blow,bhigh}, %r2102;
mov.b32 %r2144, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2111;
mov.b32 {blow,bhigh}, %r2114;
mov.b32 %r2147, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2111;
mov.b32 {blow,bhigh}, %r2114;
mov.b32 %r2150, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2123;
mov.b32 {blow,bhigh}, %r2126;
mov.b32 %r2153, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2123;
mov.b32 {blow,bhigh}, %r2126;
mov.b32 %r2156, {ahigh,bhigh};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2093;
mov.b32 {blow,bhigh}, %r2096;
mov.b32 %r2159, {alow,blow};}


	
	{.reg .f16 alow,ahigh,blow,bhigh;
mov.b32 {alow,ahigh}, %r2093;
mov.b32 {blow,bhigh}, %r2096;
mov.b32 %r2162, {ahigh,bhigh};}


	mad.lo.s32 %r2183, %r2, 8194, %r5;
cvta.to.global.u64 %rd9, %rd5;
mul.wide.u32 %rd10, %r2183, 4;
add.s64 %rd3, %rd9, %rd10;
@%p2 bra BB12_11;
bra.uni BB12_5;

BB12_11:
st.global.u32 [%rd3], %r2135;
st.global.u32 [%rd3+4096], %r2141;
st.global.u32 [%rd3+8192], %r2147;
st.global.u32 [%rd3+12288], %r2153;
setp.gt.u32	%p9, %r34, 4096;
@%p9 bra BB12_13;

st.global.u32 [%rd3+16384], %r2159;

BB12_13:
st.global.u32 [%rd3+16388], %r2138;
st.global.u32 [%rd3+20484], %r2144;
st.global.u32 [%rd3+24580], %r2150;
st.global.u32 [%rd3+28676], %r2156;
@%p9 bra BB12_15;
bra.uni BB12_14;

BB12_5:
shl.b32 %r2185, %r3, 1;
add.s32 %r2186, %r2185, -1;
setp.lt.u32	%p5, %r2186, %r1;
st.global.u32 [%rd3], %r2135;
st.global.u32 [%rd3+4096], %r2141;
st.global.u32 [%rd3+8192], %r2147;
st.global.u32 [%rd3+12288], %r2153;
@%p5 bra BB12_8;
bra.uni BB12_6;

BB12_8:
setp.gt.u32	%p7, %r34, 4096;
@%p7 bra BB12_10;

st.global.u32 [%rd3+16384], %r2159;

BB12_10:
st.global.u32 [%rd3+16388], %r2138;
st.global.u32 [%rd3+20484], %r2144;
st.global.u32 [%rd3+24580], %r2150;
st.global.u32 [%rd3+28676], %r2156;
@%p7 bra BB12_15;

BB12_14:
st.global.u32 [%rd3+32772], %r2162;
bra.uni BB12_15;

BB12_6:
setp.gt.u32	%p6, %r34, 4096;
@%p6 bra BB12_15;

st.global.u32 [%rd3+16384], %r2159;

BB12_15:
ret;
}


